{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehr9omwbNMoT"
      },
      "source": [
        "<Body>   \n",
        "<img src=\"data:image/png;base64,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\" width=\"200\" align=\"right\" >   \n",
        "<h1> <b>Advanced Topics on Machine Learning </b> </h1>\n",
        "<p><b>CÃ¡tia Teixeira</b> (200808037) | <b>Henrique Bastos</b> (202204383) | <b>Ian Karkles</b> (202200596) | <b>Vitor Pereira</b> (202210497)\n",
        "<p>Master in Data Science and Engineering</p>\n",
        "</Body>\n",
        "Faculdade de Engenharia da Universidade do Porto\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QsduErGdNMoX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from torch.utils.data import SubsetRandomSampler, DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
        "import torch.autograd as autograd\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "from sklearn.metrics import PrecisionRecallDisplay, RocCurveDisplay, average_precision_score\n",
        "from imblearn.metrics import classification_report_imbalanced\n",
        "from pycaret.classification import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V9Ti8VoNMoY"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_APDTHmlNWxJ",
        "outputId": "f2506cac-15c2-427f-e3ee-d0a1d89d3d44"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uUdTphWMNkU1"
      },
      "outputs": [],
      "source": [
        "#df = pd.read_csv('/content/drive/MyDrive/creditcard.csv')\n",
        "df = pd.read_csv('/Users/henriqueribeiro/Downloads/creditcard.csv')\n",
        "\n",
        "#split df into train and test\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ph9o2Cf5NMoZ",
        "outputId": "84636853-29dd-4095-9604-6e4aab2fd8ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape:  (227845, 31)\n",
            "Test shape:  (56962, 31)\n"
          ]
        }
      ],
      "source": [
        "#print the characteristics of the dataframes\n",
        "print('Train shape: ', df_train.shape)\n",
        "print('Test shape: ', df_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fraud Dataset Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JceqksjuNMoa"
      },
      "outputs": [],
      "source": [
        "class FraudDataset(Dataset):\n",
        "\n",
        "  def __init__(self, dataframe, fraud = True):\n",
        "    df = dataframe\n",
        "    if fraud:\n",
        "      df = df[df['Class'] == 1]\n",
        "\n",
        "    x = df.iloc[:, 0:-1].values\n",
        "    y = df.iloc[:, -1].values\n",
        "    self.x_data=torch.tensor(x,dtype=torch.float32)\n",
        "    self.y_data=torch.tensor(y,dtype=torch.float32)\n",
        "\n",
        "    self.features = x.shape[1]\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    return self.x_data[idx], self.y_data[idx]\n",
        "\n",
        "creditData = FraudDataset(df_train, fraud= True)\n",
        "creditDataTest = FraudDataset(df_test, fraud= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SJTTtpQvNMoa"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(creditData, batch_size=32, drop_last=True)\n",
        "test_dataloader = DataLoader(creditDataTest, batch_size=32, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI9MIbrwNMoa"
      },
      "source": [
        "## Unconditional Vanilla GAN - Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucz3lOzUNMoa"
      },
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "s7ctWrc8NMob"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim, nr_features):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "        nn.Linear(latent_dim, 1024),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(1024, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(256, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(128, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(64, 32),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(32, 16),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(16, nr_features),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9X1MEgjONMob"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, nr_features):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "        nn.Linear(nr_features, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "\n",
        "        nn.Linear(256, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "\n",
        "        nn.Linear(128, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "\n",
        "        nn.Linear(64, 32),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "\n",
        "        nn.Linear(32, 16),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "\n",
        "        nn.Linear(16, 1),\n",
        "\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        return torch.sigmoid(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mgP_9IHZNMoc"
      },
      "outputs": [],
      "source": [
        "class GAN:\n",
        "    def __init__(self, train_df, latent_size=128):\n",
        "        self.lr = 0.00005\n",
        "        self.batch_size = 16\n",
        "        self.n_critic = 1\n",
        "        self.clip_value = 0.0001\n",
        "        self.cur_batch_size = 64\n",
        "        self.latent_dim = 128\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.dataset = FraudDataset(train_df, fraud= True)\n",
        "        self.dataloader = DataLoader(self.dataset, self.batch_size, shuffle=True)\n",
        "\n",
        "        self.G = Generator(latent_dim=latent_size, nr_features=self.dataset.features).to(self.device)\n",
        "        self.D = Discriminator(nr_features=self.dataset.features).to(self.device)\n",
        "        self.latent_size = latent_size\n",
        "\n",
        "        self.g_optimizer = torch.optim.Adam(self.G.parameters(), self.lr)\n",
        "        self.d_optimizer = torch.optim.Adam(self.D.parameters(), self.lr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def train_generator(self,real_data):\n",
        "        # Clear generator gradients\n",
        "        self.g_optimizer.zero_grad()\n",
        "\n",
        "        fake_targets = torch.ones(real_data.size(0)).to(self.device)\n",
        "        fake_targets = fake_targets - 0.1\n",
        "\n",
        "        # Random noise from a uniform distribution\n",
        "        latent_space_samples = torch.randn(real_data.size(0), self.latent_dim).to(self.device)\n",
        "        generated_data = self.G(latent_space_samples)  # Fake data generated by the generator\n",
        "        fake_preds = self.D(generated_data).reshape(-1)\n",
        "\n",
        "        g_loss = self.criterion(fake_preds, fake_targets)\n",
        "        g_loss.backward()\n",
        "        self.g_optimizer.step()\n",
        "\n",
        "        return g_loss, generated_data\n",
        "\n",
        "    def train_discriminator(self, real_data, real_labels):\n",
        "        self.d_optimizer.zero_grad()\n",
        "        real_labels = torch.ones(real_data.size(0)).to(self.device) * 0.9\n",
        "\n",
        "        real_preds = self.D(real_data).reshape(-1)\n",
        "        d_loss_real = self.criterion(real_preds, real_labels)\n",
        "\n",
        "        fake_labels = torch.zeros(self.cur_batch_size).to(self.device) * 0.1\n",
        "\n",
        "        # random noise from uniform distribution\n",
        "        latent_space_samples = torch.randn(self.cur_batch_size, self.latent_dim).to(self.device)\n",
        "\n",
        "\n",
        "        generated_data = self.G(latent_space_samples).detach()\n",
        "        fake_preds = self.D(generated_data).reshape(-1)\n",
        "        d_loss_fake = self.criterion(fake_preds, fake_labels)\n",
        "\n",
        "\n",
        "        loss = d_loss_real + d_loss_fake\n",
        "\n",
        "        # Compute gradients\n",
        "        loss.backward()\n",
        "        # Adjust the parameters using backprop\n",
        "        self.d_optimizer.step()\n",
        "\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def train(self,real_data, epochs=1000):\n",
        "        losses_gen = []\n",
        "        losses_dis = []\n",
        "        for epoch in range(0,epochs):\n",
        "            for i, (real_data, real_labels)  in enumerate(self.dataloader):\n",
        "                real_data, real_labels = real_data.to(self.device), real_labels.to(self.device)\n",
        "                # Train discriminator\n",
        "                d_error = self.train_discriminator(real_data, real_labels)\n",
        "                # Train generator every n_critic iterations\n",
        "                if i % self.n_critic == 0:\n",
        "                    g_error, _ = self.train_generator(real_data)\n",
        "                losses_gen.append(g_error)\n",
        "                losses_dis.append(d_error)\n",
        "\n",
        "            print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f} \\n'\n",
        "                .format(epoch+1, epochs, i+1, len(self.dataloader), d_error.item(), g_error.item()))\n",
        "\n",
        "        return losses_gen, losses_dis\n",
        "\n",
        "\n",
        "\n",
        "    def sample(self, count):\n",
        "        with torch.no_grad():\n",
        "            z = torch.Tensor(np.random.normal(0, 1, (count, self.latent_size))).to(self.device)\n",
        "            gen = self.G(z)\n",
        "            return gen.cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hIC8bCoFNMoc"
      },
      "outputs": [],
      "source": [
        "gan = GAN(df_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXkfNgcJNMoc",
        "outputId": "cbf5b325-e924-49fa-f050-e918d4cb728d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/1000], Step [25/25], d_loss: 7.4335, g_loss: 0.6058 \n",
            "\n",
            "Epoch [2/1000], Step [25/25], d_loss: 9.9778, g_loss: 0.6057 \n",
            "\n",
            "Epoch [3/1000], Step [25/25], d_loss: 9.8987, g_loss: 0.6081 \n",
            "\n",
            "Epoch [4/1000], Step [25/25], d_loss: 11.5117, g_loss: 0.6081 \n",
            "\n",
            "Epoch [5/1000], Step [25/25], d_loss: 9.4906, g_loss: 0.6121 \n",
            "\n",
            "Epoch [6/1000], Step [25/25], d_loss: 8.2178, g_loss: 0.6135 \n",
            "\n",
            "Epoch [7/1000], Step [25/25], d_loss: 10.8051, g_loss: 0.6108 \n",
            "\n",
            "Epoch [8/1000], Step [25/25], d_loss: 9.9007, g_loss: 0.6151 \n",
            "\n",
            "Epoch [9/1000], Step [25/25], d_loss: 8.8818, g_loss: 0.6140 \n",
            "\n",
            "Epoch [10/1000], Step [25/25], d_loss: 9.8975, g_loss: 0.6171 \n",
            "\n",
            "Epoch [11/1000], Step [25/25], d_loss: 14.2328, g_loss: 0.6199 \n",
            "\n",
            "Epoch [12/1000], Step [25/25], d_loss: 8.2788, g_loss: 0.6214 \n",
            "\n",
            "Epoch [13/1000], Step [25/25], d_loss: 10.7938, g_loss: 0.6198 \n",
            "\n",
            "Epoch [14/1000], Step [25/25], d_loss: 10.7912, g_loss: 0.6216 \n",
            "\n",
            "Epoch [15/1000], Step [25/25], d_loss: 8.9236, g_loss: 0.6243 \n",
            "\n",
            "Epoch [16/1000], Step [25/25], d_loss: 9.9398, g_loss: 0.6241 \n",
            "\n",
            "Epoch [17/1000], Step [25/25], d_loss: 11.7087, g_loss: 0.6277 \n",
            "\n",
            "Epoch [18/1000], Step [25/25], d_loss: 9.8685, g_loss: 0.6261 \n",
            "\n",
            "Epoch [19/1000], Step [25/25], d_loss: 13.3119, g_loss: 0.6288 \n",
            "\n",
            "Epoch [20/1000], Step [25/25], d_loss: 8.8769, g_loss: 0.6294 \n",
            "\n",
            "Epoch [21/1000], Step [25/25], d_loss: 10.7790, g_loss: 0.6294 \n",
            "\n",
            "Epoch [22/1000], Step [25/25], d_loss: 10.7782, g_loss: 0.6304 \n",
            "\n",
            "Epoch [23/1000], Step [25/25], d_loss: 8.9978, g_loss: 0.6312 \n",
            "\n",
            "Epoch [24/1000], Step [25/25], d_loss: 10.7739, g_loss: 0.6319 \n",
            "\n",
            "Epoch [25/1000], Step [25/25], d_loss: 10.7710, g_loss: 0.6369 \n",
            "\n",
            "Epoch [26/1000], Step [25/25], d_loss: 14.3696, g_loss: 0.6372 \n",
            "\n",
            "Epoch [27/1000], Step [25/25], d_loss: 9.8023, g_loss: 0.6374 \n",
            "\n",
            "Epoch [28/1000], Step [25/25], d_loss: 9.8565, g_loss: 0.6400 \n",
            "\n",
            "Epoch [29/1000], Step [25/25], d_loss: 9.9142, g_loss: 0.6404 \n",
            "\n",
            "Epoch [30/1000], Step [25/25], d_loss: 9.7998, g_loss: 0.6395 \n",
            "\n",
            "Epoch [31/1000], Step [25/25], d_loss: 10.2428, g_loss: 0.6436 \n",
            "\n",
            "Epoch [32/1000], Step [25/25], d_loss: 9.8398, g_loss: 0.6435 \n",
            "\n",
            "Epoch [33/1000], Step [25/25], d_loss: 10.7573, g_loss: 0.6451 \n",
            "\n",
            "Epoch [34/1000], Step [25/25], d_loss: 8.9609, g_loss: 0.6452 \n",
            "\n",
            "Epoch [35/1000], Step [25/25], d_loss: 10.5347, g_loss: 0.6452 \n",
            "\n",
            "Epoch [36/1000], Step [25/25], d_loss: 9.8351, g_loss: 0.6506 \n",
            "\n",
            "Epoch [37/1000], Step [25/25], d_loss: 10.7510, g_loss: 0.6488 \n",
            "\n",
            "Epoch [38/1000], Step [25/25], d_loss: 9.8462, g_loss: 0.6423 \n",
            "\n",
            "Epoch [39/1000], Step [25/25], d_loss: 8.0024, g_loss: 0.6467 \n",
            "\n",
            "Epoch [40/1000], Step [25/25], d_loss: 9.8578, g_loss: 0.6548 \n",
            "\n",
            "Epoch [41/1000], Step [25/25], d_loss: 9.0414, g_loss: 0.6551 \n",
            "\n",
            "Epoch [42/1000], Step [25/25], d_loss: 9.8953, g_loss: 0.6544 \n",
            "\n",
            "Epoch [43/1000], Step [25/25], d_loss: 8.0046, g_loss: 0.6609 \n",
            "\n",
            "Epoch [44/1000], Step [25/25], d_loss: 9.8104, g_loss: 0.6617 \n",
            "\n",
            "Epoch [45/1000], Step [25/25], d_loss: 8.7901, g_loss: 0.6584 \n",
            "\n",
            "Epoch [46/1000], Step [25/25], d_loss: 8.9017, g_loss: 0.6626 \n",
            "\n",
            "Epoch [47/1000], Step [25/25], d_loss: 10.7245, g_loss: 0.6637 \n",
            "\n",
            "Epoch [48/1000], Step [25/25], d_loss: 10.7217, g_loss: 0.6741 \n",
            "\n",
            "Epoch [49/1000], Step [25/25], d_loss: 10.7186, g_loss: 0.6712 \n",
            "\n",
            "Epoch [50/1000], Step [25/25], d_loss: 10.7122, g_loss: 0.6896 \n",
            "\n",
            "Epoch [51/1000], Step [25/25], d_loss: 8.9384, g_loss: 0.6897 \n",
            "\n",
            "Epoch [52/1000], Step [25/25], d_loss: 9.8450, g_loss: 0.6730 \n",
            "\n",
            "Epoch [53/1000], Step [25/25], d_loss: 10.6889, g_loss: 0.6922 \n",
            "\n",
            "Epoch [54/1000], Step [25/25], d_loss: 9.8255, g_loss: 0.7035 \n",
            "\n",
            "Epoch [55/1000], Step [25/25], d_loss: 8.7913, g_loss: 0.7047 \n",
            "\n",
            "Epoch [56/1000], Step [25/25], d_loss: 8.8962, g_loss: 0.7117 \n",
            "\n",
            "Epoch [57/1000], Step [25/25], d_loss: 10.6516, g_loss: 0.7270 \n",
            "\n",
            "Epoch [58/1000], Step [25/25], d_loss: 10.6328, g_loss: 0.7527 \n",
            "\n",
            "Epoch [59/1000], Step [25/25], d_loss: 10.6239, g_loss: 0.7559 \n",
            "\n",
            "Epoch [60/1000], Step [25/25], d_loss: 9.6937, g_loss: 0.7713 \n",
            "\n",
            "Epoch [61/1000], Step [25/25], d_loss: 9.6787, g_loss: 0.8280 \n",
            "\n",
            "Epoch [62/1000], Step [25/25], d_loss: 10.5634, g_loss: 0.8386 \n",
            "\n",
            "Epoch [63/1000], Step [25/25], d_loss: 10.5283, g_loss: 0.8063 \n",
            "\n",
            "Epoch [64/1000], Step [25/25], d_loss: 6.8801, g_loss: 0.8428 \n",
            "\n",
            "Epoch [65/1000], Step [25/25], d_loss: 6.9722, g_loss: 0.9084 \n",
            "\n",
            "Epoch [66/1000], Step [25/25], d_loss: 9.5407, g_loss: 0.9656 \n",
            "\n",
            "Epoch [67/1000], Step [25/25], d_loss: 10.4459, g_loss: 0.9178 \n",
            "\n",
            "Epoch [68/1000], Step [25/25], d_loss: 9.5560, g_loss: 1.0319 \n",
            "\n",
            "Epoch [69/1000], Step [25/25], d_loss: 13.6578, g_loss: 1.0786 \n",
            "\n",
            "Epoch [70/1000], Step [25/25], d_loss: 9.5238, g_loss: 1.0551 \n",
            "\n",
            "Epoch [71/1000], Step [25/25], d_loss: 7.6097, g_loss: 1.2214 \n",
            "\n",
            "Epoch [72/1000], Step [25/25], d_loss: 9.4701, g_loss: 1.4356 \n",
            "\n",
            "Epoch [73/1000], Step [25/25], d_loss: 11.8112, g_loss: 1.4501 \n",
            "\n",
            "Epoch [74/1000], Step [25/25], d_loss: 9.4318, g_loss: 1.3889 \n",
            "\n",
            "Epoch [75/1000], Step [25/25], d_loss: 10.2484, g_loss: 1.6217 \n",
            "\n",
            "Epoch [76/1000], Step [25/25], d_loss: 9.6746, g_loss: 2.0646 \n",
            "\n",
            "Epoch [77/1000], Step [25/25], d_loss: 8.4917, g_loss: 1.3675 \n",
            "\n",
            "Epoch [78/1000], Step [25/25], d_loss: 8.3746, g_loss: 1.7707 \n",
            "\n",
            "Epoch [79/1000], Step [25/25], d_loss: 9.2696, g_loss: 1.6404 \n",
            "\n",
            "Epoch [80/1000], Step [25/25], d_loss: 9.2555, g_loss: 2.0542 \n",
            "\n",
            "Epoch [81/1000], Step [25/25], d_loss: 8.3368, g_loss: 2.1111 \n",
            "\n",
            "Epoch [82/1000], Step [25/25], d_loss: 9.2664, g_loss: 2.1981 \n",
            "\n",
            "Epoch [83/1000], Step [25/25], d_loss: 10.1476, g_loss: 2.6073 \n",
            "\n",
            "Epoch [84/1000], Step [25/25], d_loss: 9.1645, g_loss: 2.1759 \n",
            "\n",
            "Epoch [85/1000], Step [25/25], d_loss: 8.3322, g_loss: 2.1003 \n",
            "\n",
            "Epoch [86/1000], Step [25/25], d_loss: 10.1347, g_loss: 2.4553 \n",
            "\n",
            "Epoch [87/1000], Step [25/25], d_loss: 8.2507, g_loss: 2.6834 \n",
            "\n",
            "Epoch [88/1000], Step [25/25], d_loss: 7.3551, g_loss: 3.1418 \n",
            "\n",
            "Epoch [89/1000], Step [25/25], d_loss: 9.0926, g_loss: 2.8860 \n",
            "\n",
            "Epoch [90/1000], Step [25/25], d_loss: 10.1047, g_loss: 3.3890 \n",
            "\n",
            "Epoch [91/1000], Step [25/25], d_loss: 12.7902, g_loss: 2.6675 \n",
            "\n",
            "Epoch [92/1000], Step [25/25], d_loss: 10.0824, g_loss: 2.8363 \n",
            "\n",
            "Epoch [93/1000], Step [25/25], d_loss: 10.0804, g_loss: 3.1371 \n",
            "\n",
            "Epoch [94/1000], Step [25/25], d_loss: 9.1719, g_loss: 3.0508 \n",
            "\n",
            "Epoch [95/1000], Step [25/25], d_loss: 6.4290, g_loss: 3.4320 \n",
            "\n",
            "Epoch [96/1000], Step [25/25], d_loss: 8.2052, g_loss: 3.0033 \n",
            "\n",
            "Epoch [97/1000], Step [25/25], d_loss: 10.0763, g_loss: 2.7724 \n",
            "\n",
            "Epoch [98/1000], Step [25/25], d_loss: 7.3091, g_loss: 3.3521 \n",
            "\n",
            "Epoch [99/1000], Step [25/25], d_loss: 10.0695, g_loss: 2.9381 \n",
            "\n",
            "Epoch [100/1000], Step [25/25], d_loss: 9.1849, g_loss: 2.6405 \n",
            "\n",
            "Epoch [101/1000], Step [25/25], d_loss: 9.2105, g_loss: 3.3976 \n",
            "\n",
            "Epoch [102/1000], Step [25/25], d_loss: 8.2764, g_loss: 3.4248 \n",
            "\n",
            "Epoch [103/1000], Step [25/25], d_loss: 8.3044, g_loss: 3.4756 \n",
            "\n",
            "Epoch [104/1000], Step [25/25], d_loss: 9.1112, g_loss: 3.9681 \n",
            "\n",
            "Epoch [105/1000], Step [25/25], d_loss: 9.1838, g_loss: 4.3845 \n",
            "\n",
            "Epoch [106/1000], Step [25/25], d_loss: 10.0487, g_loss: 4.8104 \n",
            "\n",
            "Epoch [107/1000], Step [25/25], d_loss: 7.7161, g_loss: 4.1387 \n",
            "\n",
            "Epoch [108/1000], Step [25/25], d_loss: 10.0675, g_loss: 3.7547 \n",
            "\n",
            "Epoch [109/1000], Step [25/25], d_loss: 10.0783, g_loss: 4.7277 \n",
            "\n",
            "Epoch [110/1000], Step [25/25], d_loss: 8.2080, g_loss: 3.8402 \n",
            "\n",
            "Epoch [111/1000], Step [25/25], d_loss: 8.1927, g_loss: 4.5241 \n",
            "\n",
            "Epoch [112/1000], Step [25/25], d_loss: 7.3807, g_loss: 4.8355 \n",
            "\n",
            "Epoch [113/1000], Step [25/25], d_loss: 7.4708, g_loss: 4.5963 \n",
            "\n",
            "Epoch [114/1000], Step [25/25], d_loss: 5.5786, g_loss: 4.1013 \n",
            "\n",
            "Epoch [115/1000], Step [25/25], d_loss: 7.2597, g_loss: 5.0851 \n",
            "\n",
            "Epoch [116/1000], Step [25/25], d_loss: 6.3443, g_loss: 3.8993 \n",
            "\n",
            "Epoch [117/1000], Step [25/25], d_loss: 9.1547, g_loss: 3.7041 \n",
            "\n",
            "Epoch [118/1000], Step [25/25], d_loss: 8.1759, g_loss: 4.6076 \n",
            "\n",
            "Epoch [119/1000], Step [25/25], d_loss: 7.3387, g_loss: 3.4253 \n",
            "\n",
            "Epoch [120/1000], Step [25/25], d_loss: 7.2430, g_loss: 3.3858 \n",
            "\n",
            "Epoch [121/1000], Step [25/25], d_loss: 7.2973, g_loss: 4.1343 \n",
            "\n",
            "Epoch [122/1000], Step [25/25], d_loss: 9.0719, g_loss: 4.4916 \n",
            "\n",
            "Epoch [123/1000], Step [25/25], d_loss: 10.4170, g_loss: 4.7750 \n",
            "\n",
            "Epoch [124/1000], Step [25/25], d_loss: 9.1535, g_loss: 5.3782 \n",
            "\n",
            "Epoch [125/1000], Step [25/25], d_loss: 6.4296, g_loss: 4.9266 \n",
            "\n",
            "Epoch [126/1000], Step [25/25], d_loss: 6.4154, g_loss: 4.4767 \n",
            "\n",
            "Epoch [127/1000], Step [25/25], d_loss: 7.3428, g_loss: 5.5111 \n",
            "\n",
            "Epoch [128/1000], Step [25/25], d_loss: 8.2427, g_loss: 4.4476 \n",
            "\n",
            "Epoch [129/1000], Step [25/25], d_loss: 7.3215, g_loss: 4.7100 \n",
            "\n",
            "Epoch [130/1000], Step [25/25], d_loss: 8.3441, g_loss: 5.1545 \n",
            "\n",
            "Epoch [131/1000], Step [25/25], d_loss: 6.4763, g_loss: 5.1903 \n",
            "\n",
            "Epoch [132/1000], Step [25/25], d_loss: 5.2766, g_loss: 4.1149 \n",
            "\n",
            "Epoch [133/1000], Step [25/25], d_loss: 5.4346, g_loss: 4.5247 \n",
            "\n",
            "Epoch [134/1000], Step [25/25], d_loss: 8.1889, g_loss: 5.7683 \n",
            "\n",
            "Epoch [135/1000], Step [25/25], d_loss: 6.3005, g_loss: 6.5170 \n",
            "\n",
            "Epoch [136/1000], Step [25/25], d_loss: 4.4576, g_loss: 6.2417 \n",
            "\n",
            "Epoch [137/1000], Step [25/25], d_loss: 6.4444, g_loss: 4.7166 \n",
            "\n",
            "Epoch [138/1000], Step [25/25], d_loss: 6.3189, g_loss: 4.9378 \n",
            "\n",
            "Epoch [139/1000], Step [25/25], d_loss: 7.2440, g_loss: 5.3849 \n",
            "\n",
            "Epoch [140/1000], Step [25/25], d_loss: 4.3966, g_loss: 3.6519 \n",
            "\n",
            "Epoch [141/1000], Step [25/25], d_loss: 7.1342, g_loss: 5.2357 \n",
            "\n",
            "Epoch [142/1000], Step [25/25], d_loss: 6.2553, g_loss: 5.5058 \n",
            "\n",
            "Epoch [143/1000], Step [25/25], d_loss: 5.3545, g_loss: 5.1883 \n",
            "\n",
            "Epoch [144/1000], Step [25/25], d_loss: 3.5945, g_loss: 5.0451 \n",
            "\n",
            "Epoch [145/1000], Step [25/25], d_loss: 3.5648, g_loss: 4.1134 \n",
            "\n",
            "Epoch [146/1000], Step [25/25], d_loss: 3.5894, g_loss: 5.8810 \n",
            "\n",
            "Epoch [147/1000], Step [25/25], d_loss: 7.2984, g_loss: 6.2197 \n",
            "\n",
            "Epoch [148/1000], Step [25/25], d_loss: 3.5727, g_loss: 5.1526 \n",
            "\n",
            "Epoch [149/1000], Step [25/25], d_loss: 2.6324, g_loss: 5.2044 \n",
            "\n",
            "Epoch [150/1000], Step [25/25], d_loss: 3.8964, g_loss: 6.2211 \n",
            "\n",
            "Epoch [151/1000], Step [25/25], d_loss: 3.4538, g_loss: 7.6121 \n",
            "\n",
            "Epoch [152/1000], Step [25/25], d_loss: 0.5659, g_loss: 4.9184 \n",
            "\n",
            "Epoch [153/1000], Step [25/25], d_loss: 2.4714, g_loss: 5.3547 \n",
            "\n",
            "Epoch [154/1000], Step [25/25], d_loss: 0.5962, g_loss: 6.8312 \n",
            "\n",
            "Epoch [155/1000], Step [25/25], d_loss: 1.8356, g_loss: 4.5487 \n",
            "\n",
            "Epoch [156/1000], Step [25/25], d_loss: 1.5120, g_loss: 5.2376 \n",
            "\n",
            "Epoch [157/1000], Step [25/25], d_loss: 2.3795, g_loss: 6.0190 \n",
            "\n",
            "Epoch [158/1000], Step [25/25], d_loss: 1.4202, g_loss: 4.1490 \n",
            "\n",
            "Epoch [159/1000], Step [25/25], d_loss: 2.4289, g_loss: 5.1329 \n",
            "\n",
            "Epoch [160/1000], Step [25/25], d_loss: 3.1924, g_loss: 4.2691 \n",
            "\n",
            "Epoch [161/1000], Step [25/25], d_loss: 0.9279, g_loss: 6.7091 \n",
            "\n",
            "Epoch [162/1000], Step [25/25], d_loss: 1.4803, g_loss: 5.7459 \n",
            "\n",
            "Epoch [163/1000], Step [25/25], d_loss: 0.5764, g_loss: 6.9556 \n",
            "\n",
            "Epoch [164/1000], Step [25/25], d_loss: 1.5843, g_loss: 4.9717 \n",
            "\n",
            "Epoch [165/1000], Step [25/25], d_loss: 1.8576, g_loss: 4.9646 \n",
            "\n",
            "Epoch [166/1000], Step [25/25], d_loss: 1.4801, g_loss: 6.0043 \n",
            "\n",
            "Epoch [167/1000], Step [25/25], d_loss: 0.5303, g_loss: 4.0820 \n",
            "\n",
            "Epoch [168/1000], Step [25/25], d_loss: 3.8054, g_loss: 7.6361 \n",
            "\n",
            "Epoch [169/1000], Step [25/25], d_loss: 1.4689, g_loss: 5.2560 \n",
            "\n",
            "Epoch [170/1000], Step [25/25], d_loss: 1.6175, g_loss: 5.5026 \n",
            "\n",
            "Epoch [171/1000], Step [25/25], d_loss: 0.7806, g_loss: 5.7511 \n",
            "\n",
            "Epoch [172/1000], Step [25/25], d_loss: 1.4501, g_loss: 6.7927 \n",
            "\n",
            "Epoch [173/1000], Step [25/25], d_loss: 0.6410, g_loss: 6.9126 \n",
            "\n",
            "Epoch [174/1000], Step [25/25], d_loss: 0.7578, g_loss: 6.0331 \n",
            "\n",
            "Epoch [175/1000], Step [25/25], d_loss: 1.6164, g_loss: 5.6594 \n",
            "\n",
            "Epoch [176/1000], Step [25/25], d_loss: 1.4585, g_loss: 4.5775 \n",
            "\n",
            "Epoch [177/1000], Step [25/25], d_loss: 0.4512, g_loss: 5.9318 \n",
            "\n",
            "Epoch [178/1000], Step [25/25], d_loss: 1.4253, g_loss: 6.9580 \n",
            "\n",
            "Epoch [179/1000], Step [25/25], d_loss: 2.4901, g_loss: 4.6383 \n",
            "\n",
            "Epoch [180/1000], Step [25/25], d_loss: 0.6512, g_loss: 6.8002 \n",
            "\n",
            "Epoch [181/1000], Step [25/25], d_loss: 2.4656, g_loss: 3.9333 \n",
            "\n",
            "Epoch [182/1000], Step [25/25], d_loss: 2.5236, g_loss: 4.7720 \n",
            "\n",
            "Epoch [183/1000], Step [25/25], d_loss: 1.4533, g_loss: 10.5317 \n",
            "\n",
            "Epoch [184/1000], Step [25/25], d_loss: 2.4473, g_loss: 7.4159 \n",
            "\n",
            "Epoch [185/1000], Step [25/25], d_loss: 1.7386, g_loss: 5.9419 \n",
            "\n",
            "Epoch [186/1000], Step [25/25], d_loss: 2.4203, g_loss: 4.5917 \n",
            "\n",
            "Epoch [187/1000], Step [25/25], d_loss: 1.8097, g_loss: 6.4916 \n",
            "\n",
            "Epoch [188/1000], Step [25/25], d_loss: 0.6954, g_loss: 5.1308 \n",
            "\n",
            "Epoch [189/1000], Step [25/25], d_loss: 0.5640, g_loss: 3.4595 \n",
            "\n",
            "Epoch [190/1000], Step [25/25], d_loss: 1.5607, g_loss: 5.1547 \n",
            "\n",
            "Epoch [191/1000], Step [25/25], d_loss: 2.4446, g_loss: 4.9709 \n",
            "\n",
            "Epoch [192/1000], Step [25/25], d_loss: 3.5547, g_loss: 4.6860 \n",
            "\n",
            "Epoch [193/1000], Step [25/25], d_loss: 1.5721, g_loss: 6.4108 \n",
            "\n",
            "Epoch [194/1000], Step [25/25], d_loss: 1.7143, g_loss: 5.5701 \n",
            "\n",
            "Epoch [195/1000], Step [25/25], d_loss: 0.7816, g_loss: 3.7792 \n",
            "\n",
            "Epoch [196/1000], Step [25/25], d_loss: 0.6762, g_loss: 5.2599 \n",
            "\n",
            "Epoch [197/1000], Step [25/25], d_loss: 0.6887, g_loss: 6.0932 \n",
            "\n",
            "Epoch [198/1000], Step [25/25], d_loss: 2.8730, g_loss: 1.8650 \n",
            "\n",
            "Epoch [199/1000], Step [25/25], d_loss: 1.8346, g_loss: 3.4224 \n",
            "\n",
            "Epoch [200/1000], Step [25/25], d_loss: 0.8104, g_loss: 3.8150 \n",
            "\n",
            "Epoch [201/1000], Step [25/25], d_loss: 2.7976, g_loss: 4.4998 \n",
            "\n",
            "Epoch [202/1000], Step [25/25], d_loss: 0.9272, g_loss: 6.0423 \n",
            "\n",
            "Epoch [203/1000], Step [25/25], d_loss: 0.9750, g_loss: 3.2560 \n",
            "\n",
            "Epoch [204/1000], Step [25/25], d_loss: 0.9492, g_loss: 2.7808 \n",
            "\n",
            "Epoch [205/1000], Step [25/25], d_loss: 2.2230, g_loss: 2.5283 \n",
            "\n",
            "Epoch [206/1000], Step [25/25], d_loss: 1.0600, g_loss: 3.6197 \n",
            "\n",
            "Epoch [207/1000], Step [25/25], d_loss: 0.9848, g_loss: 5.3760 \n",
            "\n",
            "Epoch [208/1000], Step [25/25], d_loss: 1.2502, g_loss: 3.2284 \n",
            "\n",
            "Epoch [209/1000], Step [25/25], d_loss: 2.0556, g_loss: 1.1984 \n",
            "\n",
            "Epoch [210/1000], Step [25/25], d_loss: 1.2260, g_loss: 1.8841 \n",
            "\n",
            "Epoch [211/1000], Step [25/25], d_loss: 1.0560, g_loss: 1.0880 \n",
            "\n",
            "Epoch [212/1000], Step [25/25], d_loss: 10.4274, g_loss: 1.5438 \n",
            "\n",
            "Epoch [213/1000], Step [25/25], d_loss: 2.0784, g_loss: 0.7474 \n",
            "\n",
            "Epoch [214/1000], Step [25/25], d_loss: 1.0904, g_loss: 2.0773 \n",
            "\n",
            "Epoch [215/1000], Step [25/25], d_loss: 1.1096, g_loss: 2.0337 \n",
            "\n",
            "Epoch [216/1000], Step [25/25], d_loss: 1.1592, g_loss: 2.5471 \n",
            "\n",
            "Epoch [217/1000], Step [25/25], d_loss: 1.2701, g_loss: 1.6039 \n",
            "\n",
            "Epoch [218/1000], Step [25/25], d_loss: 1.0798, g_loss: 0.6533 \n",
            "\n",
            "Epoch [219/1000], Step [25/25], d_loss: 1.1391, g_loss: 0.6506 \n",
            "\n",
            "Epoch [220/1000], Step [25/25], d_loss: 1.1735, g_loss: 2.1443 \n",
            "\n",
            "Epoch [221/1000], Step [25/25], d_loss: 1.2320, g_loss: 1.9021 \n",
            "\n",
            "Epoch [222/1000], Step [25/25], d_loss: 2.0642, g_loss: 0.6561 \n",
            "\n",
            "Epoch [223/1000], Step [25/25], d_loss: 1.2143, g_loss: 1.3752 \n",
            "\n",
            "Epoch [224/1000], Step [25/25], d_loss: 1.1762, g_loss: 3.0515 \n",
            "\n",
            "Epoch [225/1000], Step [25/25], d_loss: 2.1396, g_loss: 1.0029 \n",
            "\n",
            "Epoch [226/1000], Step [25/25], d_loss: 1.2376, g_loss: 2.5323 \n",
            "\n",
            "Epoch [227/1000], Step [25/25], d_loss: 1.1634, g_loss: 0.6595 \n",
            "\n",
            "Epoch [228/1000], Step [25/25], d_loss: 1.2081, g_loss: 0.6582 \n",
            "\n",
            "Epoch [229/1000], Step [25/25], d_loss: 1.1171, g_loss: 0.6580 \n",
            "\n",
            "Epoch [230/1000], Step [25/25], d_loss: 1.0956, g_loss: 2.1502 \n",
            "\n",
            "Epoch [231/1000], Step [25/25], d_loss: 1.1725, g_loss: 1.8723 \n",
            "\n",
            "Epoch [232/1000], Step [25/25], d_loss: 1.1520, g_loss: 0.6609 \n",
            "\n",
            "Epoch [233/1000], Step [25/25], d_loss: 1.2072, g_loss: 0.6533 \n",
            "\n",
            "Epoch [234/1000], Step [25/25], d_loss: 1.6074, g_loss: 0.6632 \n",
            "\n",
            "Epoch [235/1000], Step [25/25], d_loss: 1.3002, g_loss: 0.6590 \n",
            "\n",
            "Epoch [236/1000], Step [25/25], d_loss: 1.0515, g_loss: 0.6726 \n",
            "\n",
            "Epoch [237/1000], Step [25/25], d_loss: 1.2000, g_loss: 0.6608 \n",
            "\n",
            "Epoch [238/1000], Step [25/25], d_loss: 2.1225, g_loss: 0.6702 \n",
            "\n",
            "Epoch [239/1000], Step [25/25], d_loss: 1.3530, g_loss: 0.6704 \n",
            "\n",
            "Epoch [240/1000], Step [25/25], d_loss: 1.1703, g_loss: 0.6697 \n",
            "\n",
            "Epoch [241/1000], Step [25/25], d_loss: 1.1945, g_loss: 0.8326 \n",
            "\n",
            "Epoch [242/1000], Step [25/25], d_loss: 1.3962, g_loss: 0.6713 \n",
            "\n",
            "Epoch [243/1000], Step [25/25], d_loss: 1.1716, g_loss: 2.4986 \n",
            "\n",
            "Epoch [244/1000], Step [25/25], d_loss: 1.2317, g_loss: 2.6915 \n",
            "\n",
            "Epoch [245/1000], Step [25/25], d_loss: 1.1698, g_loss: 0.6726 \n",
            "\n",
            "Epoch [246/1000], Step [25/25], d_loss: 1.1251, g_loss: 2.4969 \n",
            "\n",
            "Epoch [247/1000], Step [25/25], d_loss: 1.1263, g_loss: 0.6727 \n",
            "\n",
            "Epoch [248/1000], Step [25/25], d_loss: 1.1572, g_loss: 0.6701 \n",
            "\n",
            "Epoch [249/1000], Step [25/25], d_loss: 1.2239, g_loss: 0.6770 \n",
            "\n",
            "Epoch [250/1000], Step [25/25], d_loss: 2.0901, g_loss: 2.0453 \n",
            "\n",
            "Epoch [251/1000], Step [25/25], d_loss: 1.0688, g_loss: 0.6756 \n",
            "\n",
            "Epoch [252/1000], Step [25/25], d_loss: 1.1514, g_loss: 0.6742 \n",
            "\n",
            "Epoch [253/1000], Step [25/25], d_loss: 1.0993, g_loss: 0.6642 \n",
            "\n",
            "Epoch [254/1000], Step [25/25], d_loss: 1.1115, g_loss: 0.6792 \n",
            "\n",
            "Epoch [255/1000], Step [25/25], d_loss: 1.2822, g_loss: 0.6992 \n",
            "\n",
            "Epoch [256/1000], Step [25/25], d_loss: 2.0593, g_loss: 2.1812 \n",
            "\n",
            "Epoch [257/1000], Step [25/25], d_loss: 1.0651, g_loss: 1.0866 \n",
            "\n",
            "Epoch [258/1000], Step [25/25], d_loss: 1.1715, g_loss: 0.6871 \n",
            "\n",
            "Epoch [259/1000], Step [25/25], d_loss: 1.2217, g_loss: 0.6779 \n",
            "\n",
            "Epoch [260/1000], Step [25/25], d_loss: 3.5364, g_loss: 0.6831 \n",
            "\n",
            "Epoch [261/1000], Step [25/25], d_loss: 2.0365, g_loss: 0.6890 \n",
            "\n",
            "Epoch [262/1000], Step [25/25], d_loss: 1.1284, g_loss: 0.6801 \n",
            "\n",
            "Epoch [263/1000], Step [25/25], d_loss: 1.0457, g_loss: 0.6828 \n",
            "\n",
            "Epoch [264/1000], Step [25/25], d_loss: 1.0768, g_loss: 0.6828 \n",
            "\n",
            "Epoch [265/1000], Step [25/25], d_loss: 1.1266, g_loss: 0.6763 \n",
            "\n",
            "Epoch [266/1000], Step [25/25], d_loss: 1.1417, g_loss: 0.6841 \n",
            "\n",
            "Epoch [267/1000], Step [25/25], d_loss: 1.1432, g_loss: 0.6840 \n",
            "\n",
            "Epoch [268/1000], Step [25/25], d_loss: 1.1143, g_loss: 1.8161 \n",
            "\n",
            "Epoch [269/1000], Step [25/25], d_loss: 1.1134, g_loss: 0.6888 \n",
            "\n",
            "Epoch [270/1000], Step [25/25], d_loss: 1.1077, g_loss: 1.5317 \n",
            "\n",
            "Epoch [271/1000], Step [25/25], d_loss: 1.1154, g_loss: 0.6863 \n",
            "\n",
            "Epoch [272/1000], Step [25/25], d_loss: 1.1470, g_loss: 0.6857 \n",
            "\n",
            "Epoch [273/1000], Step [25/25], d_loss: 1.0473, g_loss: 1.2857 \n",
            "\n",
            "Epoch [274/1000], Step [25/25], d_loss: 1.1872, g_loss: 0.6938 \n",
            "\n",
            "Epoch [275/1000], Step [25/25], d_loss: 2.1051, g_loss: 0.6885 \n",
            "\n",
            "Epoch [276/1000], Step [25/25], d_loss: 1.1036, g_loss: 0.6902 \n",
            "\n",
            "Epoch [277/1000], Step [25/25], d_loss: 2.0572, g_loss: 0.6920 \n",
            "\n",
            "Epoch [278/1000], Step [25/25], d_loss: 1.1497, g_loss: 4.1392 \n",
            "\n",
            "Epoch [279/1000], Step [25/25], d_loss: 1.1937, g_loss: 0.6924 \n",
            "\n",
            "Epoch [280/1000], Step [25/25], d_loss: 1.1048, g_loss: 0.6888 \n",
            "\n",
            "Epoch [281/1000], Step [25/25], d_loss: 1.1580, g_loss: 0.6943 \n",
            "\n",
            "Epoch [282/1000], Step [25/25], d_loss: 1.0633, g_loss: 0.6945 \n",
            "\n",
            "Epoch [283/1000], Step [25/25], d_loss: 1.1608, g_loss: 0.6916 \n",
            "\n",
            "Epoch [284/1000], Step [25/25], d_loss: 1.1327, g_loss: 0.6866 \n",
            "\n",
            "Epoch [285/1000], Step [25/25], d_loss: 1.1271, g_loss: 0.6937 \n",
            "\n",
            "Epoch [286/1000], Step [25/25], d_loss: 1.1097, g_loss: 2.9964 \n",
            "\n",
            "Epoch [287/1000], Step [25/25], d_loss: 2.0881, g_loss: 0.6973 \n",
            "\n",
            "Epoch [288/1000], Step [25/25], d_loss: 1.0671, g_loss: 0.6975 \n",
            "\n",
            "Epoch [289/1000], Step [25/25], d_loss: 1.1849, g_loss: 1.8305 \n",
            "\n",
            "Epoch [290/1000], Step [25/25], d_loss: 1.1390, g_loss: 0.6980 \n",
            "\n",
            "Epoch [291/1000], Step [25/25], d_loss: 1.0785, g_loss: 2.6313 \n",
            "\n",
            "Epoch [292/1000], Step [25/25], d_loss: 1.1189, g_loss: 0.6998 \n",
            "\n",
            "Epoch [293/1000], Step [25/25], d_loss: 1.1395, g_loss: 1.0161 \n",
            "\n",
            "Epoch [294/1000], Step [25/25], d_loss: 1.0291, g_loss: 0.6980 \n",
            "\n",
            "Epoch [295/1000], Step [25/25], d_loss: 1.1277, g_loss: 0.7018 \n",
            "\n",
            "Epoch [296/1000], Step [25/25], d_loss: 1.7450, g_loss: 0.7019 \n",
            "\n",
            "Epoch [297/1000], Step [25/25], d_loss: 1.1409, g_loss: 0.7022 \n",
            "\n",
            "Epoch [298/1000], Step [25/25], d_loss: 1.0519, g_loss: 0.7032 \n",
            "\n",
            "Epoch [299/1000], Step [25/25], d_loss: 1.0915, g_loss: 0.7036 \n",
            "\n",
            "Epoch [300/1000], Step [25/25], d_loss: 1.1334, g_loss: 0.7031 \n",
            "\n",
            "Epoch [301/1000], Step [25/25], d_loss: 1.0500, g_loss: 0.7050 \n",
            "\n",
            "Epoch [302/1000], Step [25/25], d_loss: 1.0425, g_loss: 2.1850 \n",
            "\n",
            "Epoch [303/1000], Step [25/25], d_loss: 1.1481, g_loss: 0.7044 \n",
            "\n",
            "Epoch [304/1000], Step [25/25], d_loss: 2.2587, g_loss: 0.7030 \n",
            "\n",
            "Epoch [305/1000], Step [25/25], d_loss: 1.1269, g_loss: 1.2662 \n",
            "\n",
            "Epoch [306/1000], Step [25/25], d_loss: 1.1668, g_loss: 0.7081 \n",
            "\n",
            "Epoch [307/1000], Step [25/25], d_loss: 1.5355, g_loss: 0.7067 \n",
            "\n",
            "Epoch [308/1000], Step [25/25], d_loss: 1.0614, g_loss: 2.2799 \n",
            "\n",
            "Epoch [309/1000], Step [25/25], d_loss: 1.0914, g_loss: 0.7080 \n",
            "\n",
            "Epoch [310/1000], Step [25/25], d_loss: 2.0488, g_loss: 0.7078 \n",
            "\n",
            "Epoch [311/1000], Step [25/25], d_loss: 1.1130, g_loss: 0.7626 \n",
            "\n",
            "Epoch [312/1000], Step [25/25], d_loss: 1.1329, g_loss: 0.7109 \n",
            "\n",
            "Epoch [313/1000], Step [25/25], d_loss: 1.0560, g_loss: 0.7099 \n",
            "\n",
            "Epoch [314/1000], Step [25/25], d_loss: 1.0783, g_loss: 1.5900 \n",
            "\n",
            "Epoch [315/1000], Step [25/25], d_loss: 1.1232, g_loss: 0.7094 \n",
            "\n",
            "Epoch [316/1000], Step [25/25], d_loss: 1.1125, g_loss: 2.6203 \n",
            "\n",
            "Epoch [317/1000], Step [25/25], d_loss: 1.0962, g_loss: 0.7106 \n",
            "\n",
            "Epoch [318/1000], Step [25/25], d_loss: 1.0824, g_loss: 2.2570 \n",
            "\n",
            "Epoch [319/1000], Step [25/25], d_loss: 1.0422, g_loss: 1.8142 \n",
            "\n",
            "Epoch [320/1000], Step [25/25], d_loss: 1.0384, g_loss: 0.7133 \n",
            "\n",
            "Epoch [321/1000], Step [25/25], d_loss: 1.0579, g_loss: 1.3991 \n",
            "\n",
            "Epoch [322/1000], Step [25/25], d_loss: 1.1550, g_loss: 1.8965 \n",
            "\n",
            "Epoch [323/1000], Step [25/25], d_loss: 1.0592, g_loss: 1.2495 \n",
            "\n",
            "Epoch [324/1000], Step [25/25], d_loss: 1.0199, g_loss: 0.7140 \n",
            "\n",
            "Epoch [325/1000], Step [25/25], d_loss: 1.1255, g_loss: 0.7154 \n",
            "\n",
            "Epoch [326/1000], Step [25/25], d_loss: 1.1679, g_loss: 0.7162 \n",
            "\n",
            "Epoch [327/1000], Step [25/25], d_loss: 1.0834, g_loss: 2.1997 \n",
            "\n",
            "Epoch [328/1000], Step [25/25], d_loss: 2.1003, g_loss: 0.7106 \n",
            "\n",
            "Epoch [329/1000], Step [25/25], d_loss: 1.1032, g_loss: 0.7164 \n",
            "\n",
            "Epoch [330/1000], Step [25/25], d_loss: 1.0853, g_loss: 0.7111 \n",
            "\n",
            "Epoch [331/1000], Step [25/25], d_loss: 1.0675, g_loss: 0.7159 \n",
            "\n",
            "Epoch [332/1000], Step [25/25], d_loss: 1.1172, g_loss: 1.4351 \n",
            "\n",
            "Epoch [333/1000], Step [25/25], d_loss: 1.1113, g_loss: 1.5215 \n",
            "\n",
            "Epoch [334/1000], Step [25/25], d_loss: 1.1001, g_loss: 0.9533 \n",
            "\n",
            "Epoch [335/1000], Step [25/25], d_loss: 1.0877, g_loss: 0.7177 \n",
            "\n",
            "Epoch [336/1000], Step [25/25], d_loss: 1.1400, g_loss: 1.9552 \n",
            "\n",
            "Epoch [337/1000], Step [25/25], d_loss: 1.1470, g_loss: 0.7026 \n",
            "\n",
            "Epoch [338/1000], Step [25/25], d_loss: 1.0975, g_loss: 0.7140 \n",
            "\n",
            "Epoch [339/1000], Step [25/25], d_loss: 1.1726, g_loss: 0.8419 \n",
            "\n",
            "Epoch [340/1000], Step [25/25], d_loss: 1.0755, g_loss: 1.1306 \n",
            "\n",
            "Epoch [341/1000], Step [25/25], d_loss: 1.1223, g_loss: 0.7153 \n",
            "\n",
            "Epoch [342/1000], Step [25/25], d_loss: 1.1060, g_loss: 0.7122 \n",
            "\n",
            "Epoch [343/1000], Step [25/25], d_loss: 1.0529, g_loss: 0.7225 \n",
            "\n",
            "Epoch [344/1000], Step [25/25], d_loss: 1.0879, g_loss: 0.7255 \n",
            "\n",
            "Epoch [345/1000], Step [25/25], d_loss: 1.1044, g_loss: 0.7084 \n",
            "\n",
            "Epoch [346/1000], Step [25/25], d_loss: 1.1401, g_loss: 0.7189 \n",
            "\n",
            "Epoch [347/1000], Step [25/25], d_loss: 1.1367, g_loss: 2.3891 \n",
            "\n",
            "Epoch [348/1000], Step [25/25], d_loss: 1.1313, g_loss: 0.7053 \n",
            "\n",
            "Epoch [349/1000], Step [25/25], d_loss: 1.1100, g_loss: 0.7082 \n",
            "\n",
            "Epoch [350/1000], Step [25/25], d_loss: 1.1855, g_loss: 0.8643 \n",
            "\n",
            "Epoch [351/1000], Step [25/25], d_loss: 1.1451, g_loss: 0.7110 \n",
            "\n",
            "Epoch [352/1000], Step [25/25], d_loss: 1.1620, g_loss: 0.7195 \n",
            "\n",
            "Epoch [353/1000], Step [25/25], d_loss: 1.1525, g_loss: 0.7146 \n",
            "\n",
            "Epoch [354/1000], Step [25/25], d_loss: 1.0993, g_loss: 0.7077 \n",
            "\n",
            "Epoch [355/1000], Step [25/25], d_loss: 1.1069, g_loss: 0.7157 \n",
            "\n",
            "Epoch [356/1000], Step [25/25], d_loss: 1.2089, g_loss: 0.7215 \n",
            "\n",
            "Epoch [357/1000], Step [25/25], d_loss: 1.1220, g_loss: 0.7205 \n",
            "\n",
            "Epoch [358/1000], Step [25/25], d_loss: 1.1774, g_loss: 1.1636 \n",
            "\n",
            "Epoch [359/1000], Step [25/25], d_loss: 1.0698, g_loss: 0.7234 \n",
            "\n",
            "Epoch [360/1000], Step [25/25], d_loss: 1.2097, g_loss: 0.7178 \n",
            "\n",
            "Epoch [361/1000], Step [25/25], d_loss: 1.1488, g_loss: 0.7021 \n",
            "\n",
            "Epoch [362/1000], Step [25/25], d_loss: 1.1193, g_loss: 0.7120 \n",
            "\n",
            "Epoch [363/1000], Step [25/25], d_loss: 1.0260, g_loss: 0.7176 \n",
            "\n",
            "Epoch [364/1000], Step [25/25], d_loss: 1.0212, g_loss: 0.7174 \n",
            "\n",
            "Epoch [365/1000], Step [25/25], d_loss: 1.1314, g_loss: 0.7114 \n",
            "\n",
            "Epoch [366/1000], Step [25/25], d_loss: 1.1248, g_loss: 0.7191 \n",
            "\n",
            "Epoch [367/1000], Step [25/25], d_loss: 1.1591, g_loss: 0.7205 \n",
            "\n",
            "Epoch [368/1000], Step [25/25], d_loss: 1.0900, g_loss: 0.7148 \n",
            "\n",
            "Epoch [369/1000], Step [25/25], d_loss: 1.1175, g_loss: 0.7445 \n",
            "\n",
            "Epoch [370/1000], Step [25/25], d_loss: 1.0445, g_loss: 0.7174 \n",
            "\n",
            "Epoch [371/1000], Step [25/25], d_loss: 1.1140, g_loss: 0.7112 \n",
            "\n",
            "Epoch [372/1000], Step [25/25], d_loss: 1.0903, g_loss: 0.7135 \n",
            "\n",
            "Epoch [373/1000], Step [25/25], d_loss: 1.0673, g_loss: 0.7170 \n",
            "\n",
            "Epoch [374/1000], Step [25/25], d_loss: 1.1012, g_loss: 0.7228 \n",
            "\n",
            "Epoch [375/1000], Step [25/25], d_loss: 2.1219, g_loss: 0.6863 \n",
            "\n",
            "Epoch [376/1000], Step [25/25], d_loss: 1.1159, g_loss: 0.7185 \n",
            "\n",
            "Epoch [377/1000], Step [25/25], d_loss: 1.0720, g_loss: 0.9916 \n",
            "\n",
            "Epoch [378/1000], Step [25/25], d_loss: 1.1460, g_loss: 0.7123 \n",
            "\n",
            "Epoch [379/1000], Step [25/25], d_loss: 1.1559, g_loss: 0.7181 \n",
            "\n",
            "Epoch [380/1000], Step [25/25], d_loss: 1.2937, g_loss: 0.6975 \n",
            "\n",
            "Epoch [381/1000], Step [25/25], d_loss: 1.1386, g_loss: 0.6943 \n",
            "\n",
            "Epoch [382/1000], Step [25/25], d_loss: 1.1818, g_loss: 0.7225 \n",
            "\n",
            "Epoch [383/1000], Step [25/25], d_loss: 1.1284, g_loss: 0.6951 \n",
            "\n",
            "Epoch [384/1000], Step [25/25], d_loss: 1.0889, g_loss: 0.7164 \n",
            "\n",
            "Epoch [385/1000], Step [25/25], d_loss: 1.0878, g_loss: 0.7214 \n",
            "\n",
            "Epoch [386/1000], Step [25/25], d_loss: 1.0692, g_loss: 0.6900 \n",
            "\n",
            "Epoch [387/1000], Step [25/25], d_loss: 1.1680, g_loss: 0.7383 \n",
            "\n",
            "Epoch [388/1000], Step [25/25], d_loss: 1.2228, g_loss: 0.7167 \n",
            "\n",
            "Epoch [389/1000], Step [25/25], d_loss: 1.2435, g_loss: 0.7138 \n",
            "\n",
            "Epoch [390/1000], Step [25/25], d_loss: 1.0739, g_loss: 0.7213 \n",
            "\n",
            "Epoch [391/1000], Step [25/25], d_loss: 1.1184, g_loss: 0.6915 \n",
            "\n",
            "Epoch [392/1000], Step [25/25], d_loss: 1.0635, g_loss: 0.7186 \n",
            "\n",
            "Epoch [393/1000], Step [25/25], d_loss: 1.1784, g_loss: 0.7066 \n",
            "\n",
            "Epoch [394/1000], Step [25/25], d_loss: 1.1051, g_loss: 0.7149 \n",
            "\n",
            "Epoch [395/1000], Step [25/25], d_loss: 1.1466, g_loss: 0.7039 \n",
            "\n",
            "Epoch [396/1000], Step [25/25], d_loss: 4.1585, g_loss: 0.7108 \n",
            "\n",
            "Epoch [397/1000], Step [25/25], d_loss: 2.0965, g_loss: 0.7061 \n",
            "\n",
            "Epoch [398/1000], Step [25/25], d_loss: 1.0905, g_loss: 0.7299 \n",
            "\n",
            "Epoch [399/1000], Step [25/25], d_loss: 1.2381, g_loss: 0.7141 \n",
            "\n",
            "Epoch [400/1000], Step [25/25], d_loss: 1.1056, g_loss: 0.6980 \n",
            "\n",
            "Epoch [401/1000], Step [25/25], d_loss: 1.1004, g_loss: 0.6732 \n",
            "\n",
            "Epoch [402/1000], Step [25/25], d_loss: 1.1803, g_loss: 2.8262 \n",
            "\n",
            "Epoch [403/1000], Step [25/25], d_loss: 1.2317, g_loss: 9.6065 \n",
            "\n",
            "Epoch [404/1000], Step [25/25], d_loss: 1.1995, g_loss: 0.7137 \n",
            "\n",
            "Epoch [405/1000], Step [25/25], d_loss: 1.1856, g_loss: 0.7084 \n",
            "\n",
            "Epoch [406/1000], Step [25/25], d_loss: 1.1276, g_loss: 0.7097 \n",
            "\n",
            "Epoch [407/1000], Step [25/25], d_loss: 1.1738, g_loss: 0.7243 \n",
            "\n",
            "Epoch [408/1000], Step [25/25], d_loss: 1.1612, g_loss: 0.7158 \n",
            "\n",
            "Epoch [409/1000], Step [25/25], d_loss: 1.1378, g_loss: 0.6848 \n",
            "\n",
            "Epoch [410/1000], Step [25/25], d_loss: 1.1752, g_loss: 0.6724 \n",
            "\n",
            "Epoch [411/1000], Step [25/25], d_loss: 1.1672, g_loss: 0.6899 \n",
            "\n",
            "Epoch [412/1000], Step [25/25], d_loss: 1.0969, g_loss: 0.6946 \n",
            "\n",
            "Epoch [413/1000], Step [25/25], d_loss: 1.1454, g_loss: 0.6885 \n",
            "\n",
            "Epoch [414/1000], Step [25/25], d_loss: 1.1786, g_loss: 0.6853 \n",
            "\n",
            "Epoch [415/1000], Step [25/25], d_loss: 1.1813, g_loss: 0.7128 \n",
            "\n",
            "Epoch [416/1000], Step [25/25], d_loss: 1.3387, g_loss: 0.6928 \n",
            "\n",
            "Epoch [417/1000], Step [25/25], d_loss: 1.2266, g_loss: 0.6852 \n",
            "\n",
            "Epoch [418/1000], Step [25/25], d_loss: 1.1527, g_loss: 0.7047 \n",
            "\n",
            "Epoch [419/1000], Step [25/25], d_loss: 1.2286, g_loss: 0.6568 \n",
            "\n",
            "Epoch [420/1000], Step [25/25], d_loss: 1.1774, g_loss: 0.6840 \n",
            "\n",
            "Epoch [421/1000], Step [25/25], d_loss: 1.1795, g_loss: 0.6755 \n",
            "\n",
            "Epoch [422/1000], Step [25/25], d_loss: 1.2305, g_loss: 0.6491 \n",
            "\n",
            "Epoch [423/1000], Step [25/25], d_loss: 1.2217, g_loss: 0.6820 \n",
            "\n",
            "Epoch [424/1000], Step [25/25], d_loss: 1.1511, g_loss: 0.7218 \n",
            "\n",
            "Epoch [425/1000], Step [25/25], d_loss: 1.2566, g_loss: 0.6870 \n",
            "\n",
            "Epoch [426/1000], Step [25/25], d_loss: 1.2328, g_loss: 0.6980 \n",
            "\n",
            "Epoch [427/1000], Step [25/25], d_loss: 1.1936, g_loss: 0.6847 \n",
            "\n",
            "Epoch [428/1000], Step [25/25], d_loss: 1.1821, g_loss: 0.6939 \n",
            "\n",
            "Epoch [429/1000], Step [25/25], d_loss: 1.2466, g_loss: 0.7013 \n",
            "\n",
            "Epoch [430/1000], Step [25/25], d_loss: 1.2585, g_loss: 0.6955 \n",
            "\n",
            "Epoch [431/1000], Step [25/25], d_loss: 1.2304, g_loss: 0.7064 \n",
            "\n",
            "Epoch [432/1000], Step [25/25], d_loss: 1.1971, g_loss: 0.6926 \n",
            "\n",
            "Epoch [433/1000], Step [25/25], d_loss: 1.2074, g_loss: 0.6715 \n",
            "\n",
            "Epoch [434/1000], Step [25/25], d_loss: 1.1972, g_loss: 0.6631 \n",
            "\n",
            "Epoch [435/1000], Step [25/25], d_loss: 1.3390, g_loss: 0.6467 \n",
            "\n",
            "Epoch [436/1000], Step [25/25], d_loss: 1.2407, g_loss: 0.6996 \n",
            "\n",
            "Epoch [437/1000], Step [25/25], d_loss: 1.2907, g_loss: 0.7046 \n",
            "\n",
            "Epoch [438/1000], Step [25/25], d_loss: 1.4309, g_loss: 0.6689 \n",
            "\n",
            "Epoch [439/1000], Step [25/25], d_loss: 1.5712, g_loss: 0.6665 \n",
            "\n",
            "Epoch [440/1000], Step [25/25], d_loss: 1.2627, g_loss: 0.7037 \n",
            "\n",
            "Epoch [441/1000], Step [25/25], d_loss: 1.2511, g_loss: 0.6666 \n",
            "\n",
            "Epoch [442/1000], Step [25/25], d_loss: 1.2706, g_loss: 0.6971 \n",
            "\n",
            "Epoch [443/1000], Step [25/25], d_loss: 1.2345, g_loss: 0.6862 \n",
            "\n",
            "Epoch [444/1000], Step [25/25], d_loss: 1.3265, g_loss: 0.6684 \n",
            "\n",
            "Epoch [445/1000], Step [25/25], d_loss: 1.2077, g_loss: 0.6466 \n",
            "\n",
            "Epoch [446/1000], Step [25/25], d_loss: 1.2980, g_loss: 0.6958 \n",
            "\n",
            "Epoch [447/1000], Step [25/25], d_loss: 1.3384, g_loss: 0.6702 \n",
            "\n",
            "Epoch [448/1000], Step [25/25], d_loss: 1.3085, g_loss: 0.6900 \n",
            "\n",
            "Epoch [449/1000], Step [25/25], d_loss: 1.2603, g_loss: 0.6654 \n",
            "\n",
            "Epoch [450/1000], Step [25/25], d_loss: 1.3078, g_loss: 0.6626 \n",
            "\n",
            "Epoch [451/1000], Step [25/25], d_loss: 1.3418, g_loss: 0.6987 \n",
            "\n",
            "Epoch [452/1000], Step [25/25], d_loss: 1.3992, g_loss: 0.6926 \n",
            "\n",
            "Epoch [453/1000], Step [25/25], d_loss: 1.3200, g_loss: 0.7099 \n",
            "\n",
            "Epoch [454/1000], Step [25/25], d_loss: 1.2707, g_loss: 0.6910 \n",
            "\n",
            "Epoch [455/1000], Step [25/25], d_loss: 1.2657, g_loss: 0.6876 \n",
            "\n",
            "Epoch [456/1000], Step [25/25], d_loss: 1.4361, g_loss: 0.6819 \n",
            "\n",
            "Epoch [457/1000], Step [25/25], d_loss: 1.3447, g_loss: 0.6786 \n",
            "\n",
            "Epoch [458/1000], Step [25/25], d_loss: 1.3436, g_loss: 0.7303 \n",
            "\n",
            "Epoch [459/1000], Step [25/25], d_loss: 1.4628, g_loss: 0.7083 \n",
            "\n",
            "Epoch [460/1000], Step [25/25], d_loss: 1.3811, g_loss: 0.6948 \n",
            "\n",
            "Epoch [461/1000], Step [25/25], d_loss: 1.3971, g_loss: 0.7113 \n",
            "\n",
            "Epoch [462/1000], Step [25/25], d_loss: 1.3457, g_loss: 0.6874 \n",
            "\n",
            "Epoch [463/1000], Step [25/25], d_loss: 1.3455, g_loss: 0.7241 \n",
            "\n",
            "Epoch [464/1000], Step [25/25], d_loss: 1.3008, g_loss: 0.7161 \n",
            "\n",
            "Epoch [465/1000], Step [25/25], d_loss: 1.3482, g_loss: 0.7195 \n",
            "\n",
            "Epoch [466/1000], Step [25/25], d_loss: 1.3432, g_loss: 0.7245 \n",
            "\n",
            "Epoch [467/1000], Step [25/25], d_loss: 1.3646, g_loss: 0.7067 \n",
            "\n",
            "Epoch [468/1000], Step [25/25], d_loss: 1.3917, g_loss: 0.7672 \n",
            "\n",
            "Epoch [469/1000], Step [25/25], d_loss: 1.3413, g_loss: 0.7485 \n",
            "\n",
            "Epoch [470/1000], Step [25/25], d_loss: 1.3552, g_loss: 0.7414 \n",
            "\n",
            "Epoch [471/1000], Step [25/25], d_loss: 1.6218, g_loss: 0.6859 \n",
            "\n",
            "Epoch [472/1000], Step [25/25], d_loss: 1.3694, g_loss: 0.7554 \n",
            "\n",
            "Epoch [473/1000], Step [25/25], d_loss: 1.3275, g_loss: 0.6833 \n",
            "\n",
            "Epoch [474/1000], Step [25/25], d_loss: 1.3920, g_loss: 0.7500 \n",
            "\n",
            "Epoch [475/1000], Step [25/25], d_loss: 1.3868, g_loss: 0.7166 \n",
            "\n",
            "Epoch [476/1000], Step [25/25], d_loss: 1.3609, g_loss: 0.7115 \n",
            "\n",
            "Epoch [477/1000], Step [25/25], d_loss: 1.3468, g_loss: 0.7657 \n",
            "\n",
            "Epoch [478/1000], Step [25/25], d_loss: 1.3685, g_loss: 0.7521 \n",
            "\n",
            "Epoch [479/1000], Step [25/25], d_loss: 1.3570, g_loss: 0.8180 \n",
            "\n",
            "Epoch [480/1000], Step [25/25], d_loss: 1.3585, g_loss: 0.7574 \n",
            "\n",
            "Epoch [481/1000], Step [25/25], d_loss: 1.3607, g_loss: 0.7498 \n",
            "\n",
            "Epoch [482/1000], Step [25/25], d_loss: 1.3494, g_loss: 0.7402 \n",
            "\n",
            "Epoch [483/1000], Step [25/25], d_loss: 1.3799, g_loss: 0.7244 \n",
            "\n",
            "Epoch [484/1000], Step [25/25], d_loss: 1.3479, g_loss: 0.7462 \n",
            "\n",
            "Epoch [485/1000], Step [25/25], d_loss: 1.3786, g_loss: 0.7878 \n",
            "\n",
            "Epoch [486/1000], Step [25/25], d_loss: 1.4007, g_loss: 0.7632 \n",
            "\n",
            "Epoch [487/1000], Step [25/25], d_loss: 1.4022, g_loss: 0.7534 \n",
            "\n",
            "Epoch [488/1000], Step [25/25], d_loss: 1.3737, g_loss: 0.7940 \n",
            "\n",
            "Epoch [489/1000], Step [25/25], d_loss: 1.3992, g_loss: 0.7892 \n",
            "\n",
            "Epoch [490/1000], Step [25/25], d_loss: 1.3681, g_loss: 0.7654 \n",
            "\n",
            "Epoch [491/1000], Step [25/25], d_loss: 1.3715, g_loss: 0.7895 \n",
            "\n",
            "Epoch [492/1000], Step [25/25], d_loss: 1.3547, g_loss: 0.7603 \n",
            "\n",
            "Epoch [493/1000], Step [25/25], d_loss: 1.3940, g_loss: 0.7831 \n",
            "\n",
            "Epoch [494/1000], Step [25/25], d_loss: 1.3774, g_loss: 0.7837 \n",
            "\n",
            "Epoch [495/1000], Step [25/25], d_loss: 1.4039, g_loss: 0.7498 \n",
            "\n",
            "Epoch [496/1000], Step [25/25], d_loss: 1.4161, g_loss: 0.7481 \n",
            "\n",
            "Epoch [497/1000], Step [25/25], d_loss: 1.3791, g_loss: 0.7841 \n",
            "\n",
            "Epoch [498/1000], Step [25/25], d_loss: 1.3756, g_loss: 0.8160 \n",
            "\n",
            "Epoch [499/1000], Step [25/25], d_loss: 1.3735, g_loss: 0.7840 \n",
            "\n",
            "Epoch [500/1000], Step [25/25], d_loss: 1.3728, g_loss: 0.7616 \n",
            "\n",
            "Epoch [501/1000], Step [25/25], d_loss: 1.3693, g_loss: 2.0536 \n",
            "\n",
            "Epoch [502/1000], Step [25/25], d_loss: 1.3598, g_loss: 0.7965 \n",
            "\n",
            "Epoch [503/1000], Step [25/25], d_loss: 1.3832, g_loss: 0.7744 \n",
            "\n",
            "Epoch [504/1000], Step [25/25], d_loss: 1.3776, g_loss: 0.8087 \n",
            "\n",
            "Epoch [505/1000], Step [25/25], d_loss: 1.3960, g_loss: 0.7453 \n",
            "\n",
            "Epoch [506/1000], Step [25/25], d_loss: 1.3826, g_loss: 1.2483 \n",
            "\n",
            "Epoch [507/1000], Step [25/25], d_loss: 1.3742, g_loss: 0.7493 \n",
            "\n",
            "Epoch [508/1000], Step [25/25], d_loss: 1.4053, g_loss: 0.7373 \n",
            "\n",
            "Epoch [509/1000], Step [25/25], d_loss: 1.3865, g_loss: 0.7961 \n",
            "\n",
            "Epoch [510/1000], Step [25/25], d_loss: 1.3697, g_loss: 0.7617 \n",
            "\n",
            "Epoch [511/1000], Step [25/25], d_loss: 1.4086, g_loss: 0.7844 \n",
            "\n",
            "Epoch [512/1000], Step [25/25], d_loss: 1.3642, g_loss: 0.7634 \n",
            "\n",
            "Epoch [513/1000], Step [25/25], d_loss: 1.3784, g_loss: 0.7586 \n",
            "\n",
            "Epoch [514/1000], Step [25/25], d_loss: 1.3647, g_loss: 0.7847 \n",
            "\n",
            "Epoch [515/1000], Step [25/25], d_loss: 1.3828, g_loss: 0.8098 \n",
            "\n",
            "Epoch [516/1000], Step [25/25], d_loss: 1.3741, g_loss: 0.7535 \n",
            "\n",
            "Epoch [517/1000], Step [25/25], d_loss: 1.3602, g_loss: 0.7909 \n",
            "\n",
            "Epoch [518/1000], Step [25/25], d_loss: 1.3598, g_loss: 0.7788 \n",
            "\n",
            "Epoch [519/1000], Step [25/25], d_loss: 1.4510, g_loss: 0.7909 \n",
            "\n",
            "Epoch [520/1000], Step [25/25], d_loss: 1.3687, g_loss: 0.8081 \n",
            "\n",
            "Epoch [521/1000], Step [25/25], d_loss: 1.3714, g_loss: 0.7666 \n",
            "\n",
            "Epoch [522/1000], Step [25/25], d_loss: 1.3816, g_loss: 0.7823 \n",
            "\n",
            "Epoch [523/1000], Step [25/25], d_loss: 1.3696, g_loss: 0.8005 \n",
            "\n",
            "Epoch [524/1000], Step [25/25], d_loss: 1.3568, g_loss: 0.7611 \n",
            "\n",
            "Epoch [525/1000], Step [25/25], d_loss: 1.3781, g_loss: 0.7938 \n",
            "\n",
            "Epoch [526/1000], Step [25/25], d_loss: 1.3825, g_loss: 0.8003 \n",
            "\n",
            "Epoch [527/1000], Step [25/25], d_loss: 1.3658, g_loss: 0.7633 \n",
            "\n",
            "Epoch [528/1000], Step [25/25], d_loss: 1.3726, g_loss: 0.7992 \n",
            "\n",
            "Epoch [529/1000], Step [25/25], d_loss: 1.3749, g_loss: 0.7875 \n",
            "\n",
            "Epoch [530/1000], Step [25/25], d_loss: 1.3796, g_loss: 0.7820 \n",
            "\n",
            "Epoch [531/1000], Step [25/25], d_loss: 1.3995, g_loss: 0.7855 \n",
            "\n",
            "Epoch [532/1000], Step [25/25], d_loss: 1.3826, g_loss: 0.7851 \n",
            "\n",
            "Epoch [533/1000], Step [25/25], d_loss: 1.3676, g_loss: 0.7483 \n",
            "\n",
            "Epoch [534/1000], Step [25/25], d_loss: 1.3671, g_loss: 0.8051 \n",
            "\n",
            "Epoch [535/1000], Step [25/25], d_loss: 1.3728, g_loss: 0.8160 \n",
            "\n",
            "Epoch [536/1000], Step [25/25], d_loss: 1.3761, g_loss: 0.7740 \n",
            "\n",
            "Epoch [537/1000], Step [25/25], d_loss: 1.3719, g_loss: 0.7750 \n",
            "\n",
            "Epoch [538/1000], Step [25/25], d_loss: 1.3783, g_loss: 0.7946 \n",
            "\n",
            "Epoch [539/1000], Step [25/25], d_loss: 1.3668, g_loss: 0.7966 \n",
            "\n",
            "Epoch [540/1000], Step [25/25], d_loss: 1.3719, g_loss: 0.7915 \n",
            "\n",
            "Epoch [541/1000], Step [25/25], d_loss: 1.3856, g_loss: 0.7889 \n",
            "\n",
            "Epoch [542/1000], Step [25/25], d_loss: 1.4115, g_loss: 0.7853 \n",
            "\n",
            "Epoch [543/1000], Step [25/25], d_loss: 1.3808, g_loss: 0.8013 \n",
            "\n",
            "Epoch [544/1000], Step [25/25], d_loss: 1.3680, g_loss: 0.7641 \n",
            "\n",
            "Epoch [545/1000], Step [25/25], d_loss: 1.3722, g_loss: 0.7934 \n",
            "\n",
            "Epoch [546/1000], Step [25/25], d_loss: 1.3731, g_loss: 0.7763 \n",
            "\n",
            "Epoch [547/1000], Step [25/25], d_loss: 1.3718, g_loss: 0.8018 \n",
            "\n",
            "Epoch [548/1000], Step [25/25], d_loss: 1.3649, g_loss: 0.7824 \n",
            "\n",
            "Epoch [549/1000], Step [25/25], d_loss: 1.3727, g_loss: 0.8159 \n",
            "\n",
            "Epoch [550/1000], Step [25/25], d_loss: 1.3821, g_loss: 0.7580 \n",
            "\n",
            "Epoch [551/1000], Step [25/25], d_loss: 1.3305, g_loss: 0.7820 \n",
            "\n",
            "Epoch [552/1000], Step [25/25], d_loss: 1.3748, g_loss: 0.8158 \n",
            "\n",
            "Epoch [553/1000], Step [25/25], d_loss: 1.3849, g_loss: 0.7995 \n",
            "\n",
            "Epoch [554/1000], Step [25/25], d_loss: 1.3588, g_loss: 0.7607 \n",
            "\n",
            "Epoch [555/1000], Step [25/25], d_loss: 1.3820, g_loss: 0.7777 \n",
            "\n",
            "Epoch [556/1000], Step [25/25], d_loss: 1.3767, g_loss: 0.7845 \n",
            "\n",
            "Epoch [557/1000], Step [25/25], d_loss: 1.3639, g_loss: 0.7779 \n",
            "\n",
            "Epoch [558/1000], Step [25/25], d_loss: 1.3818, g_loss: 0.7778 \n",
            "\n",
            "Epoch [559/1000], Step [25/25], d_loss: 1.3816, g_loss: 0.8631 \n",
            "\n",
            "Epoch [560/1000], Step [25/25], d_loss: 1.3768, g_loss: 0.7630 \n",
            "\n",
            "Epoch [561/1000], Step [25/25], d_loss: 1.3708, g_loss: 0.7710 \n",
            "\n",
            "Epoch [562/1000], Step [25/25], d_loss: 1.3778, g_loss: 0.7747 \n",
            "\n",
            "Epoch [563/1000], Step [25/25], d_loss: 1.3962, g_loss: 0.7784 \n",
            "\n",
            "Epoch [564/1000], Step [25/25], d_loss: 1.3806, g_loss: 0.7894 \n",
            "\n",
            "Epoch [565/1000], Step [25/25], d_loss: 1.3783, g_loss: 0.7769 \n",
            "\n",
            "Epoch [566/1000], Step [25/25], d_loss: 1.3553, g_loss: 0.7672 \n",
            "\n",
            "Epoch [567/1000], Step [25/25], d_loss: 1.3758, g_loss: 0.8012 \n",
            "\n",
            "Epoch [568/1000], Step [25/25], d_loss: 1.3738, g_loss: 0.7768 \n",
            "\n",
            "Epoch [569/1000], Step [25/25], d_loss: 1.3741, g_loss: 0.7817 \n",
            "\n",
            "Epoch [570/1000], Step [25/25], d_loss: 1.3796, g_loss: 0.7797 \n",
            "\n",
            "Epoch [571/1000], Step [25/25], d_loss: 1.3781, g_loss: 0.7874 \n",
            "\n",
            "Epoch [572/1000], Step [25/25], d_loss: 1.3676, g_loss: 0.7852 \n",
            "\n",
            "Epoch [573/1000], Step [25/25], d_loss: 1.3756, g_loss: 0.7945 \n",
            "\n",
            "Epoch [574/1000], Step [25/25], d_loss: 1.3787, g_loss: 0.7903 \n",
            "\n",
            "Epoch [575/1000], Step [25/25], d_loss: 1.3784, g_loss: 0.7871 \n",
            "\n",
            "Epoch [576/1000], Step [25/25], d_loss: 1.3701, g_loss: 0.7595 \n",
            "\n",
            "Epoch [577/1000], Step [25/25], d_loss: 1.3774, g_loss: 0.7875 \n",
            "\n",
            "Epoch [578/1000], Step [25/25], d_loss: 1.3666, g_loss: 0.7900 \n",
            "\n",
            "Epoch [579/1000], Step [25/25], d_loss: 1.3730, g_loss: 0.7771 \n",
            "\n",
            "Epoch [580/1000], Step [25/25], d_loss: 1.3826, g_loss: 0.7879 \n",
            "\n",
            "Epoch [581/1000], Step [25/25], d_loss: 1.3727, g_loss: 0.7834 \n",
            "\n",
            "Epoch [582/1000], Step [25/25], d_loss: 1.3634, g_loss: 0.8303 \n",
            "\n",
            "Epoch [583/1000], Step [25/25], d_loss: 1.3656, g_loss: 0.7679 \n",
            "\n",
            "Epoch [584/1000], Step [25/25], d_loss: 1.3797, g_loss: 0.7792 \n",
            "\n",
            "Epoch [585/1000], Step [25/25], d_loss: 1.3801, g_loss: 0.7920 \n",
            "\n",
            "Epoch [586/1000], Step [25/25], d_loss: 1.3799, g_loss: 0.7938 \n",
            "\n",
            "Epoch [587/1000], Step [25/25], d_loss: 1.3792, g_loss: 0.8074 \n",
            "\n",
            "Epoch [588/1000], Step [25/25], d_loss: 1.3605, g_loss: 0.7913 \n",
            "\n",
            "Epoch [589/1000], Step [25/25], d_loss: 1.3799, g_loss: 1.6184 \n",
            "\n",
            "Epoch [590/1000], Step [25/25], d_loss: 1.3680, g_loss: 0.8077 \n",
            "\n",
            "Epoch [591/1000], Step [25/25], d_loss: 1.3581, g_loss: 0.7886 \n",
            "\n",
            "Epoch [592/1000], Step [25/25], d_loss: 1.3593, g_loss: 0.7462 \n",
            "\n",
            "Epoch [593/1000], Step [25/25], d_loss: 1.3770, g_loss: 0.8030 \n",
            "\n",
            "Epoch [594/1000], Step [25/25], d_loss: 1.3783, g_loss: 0.7623 \n",
            "\n",
            "Epoch [595/1000], Step [25/25], d_loss: 1.3753, g_loss: 0.7928 \n",
            "\n",
            "Epoch [596/1000], Step [25/25], d_loss: 1.3792, g_loss: 0.8003 \n",
            "\n",
            "Epoch [597/1000], Step [25/25], d_loss: 1.3776, g_loss: 0.7808 \n",
            "\n",
            "Epoch [598/1000], Step [25/25], d_loss: 1.3801, g_loss: 0.7752 \n",
            "\n",
            "Epoch [599/1000], Step [25/25], d_loss: 1.3707, g_loss: 0.7739 \n",
            "\n",
            "Epoch [600/1000], Step [25/25], d_loss: 1.3814, g_loss: 0.7698 \n",
            "\n",
            "Epoch [601/1000], Step [25/25], d_loss: 1.4966, g_loss: 0.7728 \n",
            "\n",
            "Epoch [602/1000], Step [25/25], d_loss: 1.3754, g_loss: 0.7799 \n",
            "\n",
            "Epoch [603/1000], Step [25/25], d_loss: 1.6216, g_loss: 0.7311 \n",
            "\n",
            "Epoch [604/1000], Step [25/25], d_loss: 1.3640, g_loss: 0.7869 \n",
            "\n",
            "Epoch [605/1000], Step [25/25], d_loss: 1.3851, g_loss: 0.8149 \n",
            "\n",
            "Epoch [606/1000], Step [25/25], d_loss: 1.4048, g_loss: 0.7817 \n",
            "\n",
            "Epoch [607/1000], Step [25/25], d_loss: 1.3789, g_loss: 0.7743 \n",
            "\n",
            "Epoch [608/1000], Step [25/25], d_loss: 1.3647, g_loss: 0.7792 \n",
            "\n",
            "Epoch [609/1000], Step [25/25], d_loss: 1.3710, g_loss: 0.7570 \n",
            "\n",
            "Epoch [610/1000], Step [25/25], d_loss: 1.3831, g_loss: 0.7936 \n",
            "\n",
            "Epoch [611/1000], Step [25/25], d_loss: 1.3781, g_loss: 0.8159 \n",
            "\n",
            "Epoch [612/1000], Step [25/25], d_loss: 1.3745, g_loss: 0.7849 \n",
            "\n",
            "Epoch [613/1000], Step [25/25], d_loss: 1.3609, g_loss: 0.7860 \n",
            "\n",
            "Epoch [614/1000], Step [25/25], d_loss: 1.3815, g_loss: 0.8032 \n",
            "\n",
            "Epoch [615/1000], Step [25/25], d_loss: 1.4077, g_loss: 0.8093 \n",
            "\n",
            "Epoch [616/1000], Step [25/25], d_loss: 1.3619, g_loss: 0.7472 \n",
            "\n",
            "Epoch [617/1000], Step [25/25], d_loss: 1.3741, g_loss: 0.7778 \n",
            "\n",
            "Epoch [618/1000], Step [25/25], d_loss: 1.3699, g_loss: 0.7963 \n",
            "\n",
            "Epoch [619/1000], Step [25/25], d_loss: 1.3770, g_loss: 0.7846 \n",
            "\n",
            "Epoch [620/1000], Step [25/25], d_loss: 1.3745, g_loss: 0.7886 \n",
            "\n",
            "Epoch [621/1000], Step [25/25], d_loss: 1.3834, g_loss: 0.7709 \n",
            "\n",
            "Epoch [622/1000], Step [25/25], d_loss: 1.3665, g_loss: 0.7848 \n",
            "\n",
            "Epoch [623/1000], Step [25/25], d_loss: 1.3721, g_loss: 0.7677 \n",
            "\n",
            "Epoch [624/1000], Step [25/25], d_loss: 1.4417, g_loss: 0.7664 \n",
            "\n",
            "Epoch [625/1000], Step [25/25], d_loss: 1.3732, g_loss: 0.7650 \n",
            "\n",
            "Epoch [626/1000], Step [25/25], d_loss: 1.3788, g_loss: 0.8763 \n",
            "\n",
            "Epoch [627/1000], Step [25/25], d_loss: 1.3780, g_loss: 0.7758 \n",
            "\n",
            "Epoch [628/1000], Step [25/25], d_loss: 1.3748, g_loss: 0.7735 \n",
            "\n",
            "Epoch [629/1000], Step [25/25], d_loss: 1.3883, g_loss: 0.7710 \n",
            "\n",
            "Epoch [630/1000], Step [25/25], d_loss: 1.3777, g_loss: 0.7780 \n",
            "\n",
            "Epoch [631/1000], Step [25/25], d_loss: 1.3696, g_loss: 0.7685 \n",
            "\n",
            "Epoch [632/1000], Step [25/25], d_loss: 1.3578, g_loss: 0.8175 \n",
            "\n",
            "Epoch [633/1000], Step [25/25], d_loss: 1.3718, g_loss: 0.7967 \n",
            "\n",
            "Epoch [634/1000], Step [25/25], d_loss: 1.3702, g_loss: 0.7768 \n",
            "\n",
            "Epoch [635/1000], Step [25/25], d_loss: 4.4089, g_loss: 0.7626 \n",
            "\n",
            "Epoch [636/1000], Step [25/25], d_loss: 1.3857, g_loss: 0.7609 \n",
            "\n",
            "Epoch [637/1000], Step [25/25], d_loss: 1.4030, g_loss: 0.7755 \n",
            "\n",
            "Epoch [638/1000], Step [25/25], d_loss: 1.3708, g_loss: 0.7764 \n",
            "\n",
            "Epoch [639/1000], Step [25/25], d_loss: 1.3606, g_loss: 0.7725 \n",
            "\n",
            "Epoch [640/1000], Step [25/25], d_loss: 1.3741, g_loss: 0.7908 \n",
            "\n",
            "Epoch [641/1000], Step [25/25], d_loss: 1.3816, g_loss: 1.1510 \n",
            "\n",
            "Epoch [642/1000], Step [25/25], d_loss: 1.3755, g_loss: 0.7855 \n",
            "\n",
            "Epoch [643/1000], Step [25/25], d_loss: 1.3830, g_loss: 0.7746 \n",
            "\n",
            "Epoch [644/1000], Step [25/25], d_loss: 1.3527, g_loss: 0.7944 \n",
            "\n",
            "Epoch [645/1000], Step [25/25], d_loss: 1.3660, g_loss: 0.7631 \n",
            "\n",
            "Epoch [646/1000], Step [25/25], d_loss: 1.3771, g_loss: 0.7715 \n",
            "\n",
            "Epoch [647/1000], Step [25/25], d_loss: 1.3802, g_loss: 0.7690 \n",
            "\n",
            "Epoch [648/1000], Step [25/25], d_loss: 1.3771, g_loss: 0.7698 \n",
            "\n",
            "Epoch [649/1000], Step [25/25], d_loss: 1.3680, g_loss: 0.7689 \n",
            "\n",
            "Epoch [650/1000], Step [25/25], d_loss: 1.3811, g_loss: 0.7758 \n",
            "\n",
            "Epoch [651/1000], Step [25/25], d_loss: 1.3588, g_loss: 0.8058 \n",
            "\n",
            "Epoch [652/1000], Step [25/25], d_loss: 1.3895, g_loss: 0.7762 \n",
            "\n",
            "Epoch [653/1000], Step [25/25], d_loss: 1.3809, g_loss: 0.7784 \n",
            "\n",
            "Epoch [654/1000], Step [25/25], d_loss: 1.3781, g_loss: 0.7746 \n",
            "\n",
            "Epoch [655/1000], Step [25/25], d_loss: 1.3767, g_loss: 0.7838 \n",
            "\n",
            "Epoch [656/1000], Step [25/25], d_loss: 1.3645, g_loss: 0.7626 \n",
            "\n",
            "Epoch [657/1000], Step [25/25], d_loss: 1.3844, g_loss: 0.7915 \n",
            "\n",
            "Epoch [658/1000], Step [25/25], d_loss: 1.3797, g_loss: 0.7999 \n",
            "\n",
            "Epoch [659/1000], Step [25/25], d_loss: 1.3763, g_loss: 0.7993 \n",
            "\n",
            "Epoch [660/1000], Step [25/25], d_loss: 1.3738, g_loss: 9.6900 \n",
            "\n",
            "Epoch [661/1000], Step [25/25], d_loss: 1.3726, g_loss: 0.7699 \n",
            "\n",
            "Epoch [662/1000], Step [25/25], d_loss: 1.3752, g_loss: 0.7883 \n",
            "\n",
            "Epoch [663/1000], Step [25/25], d_loss: 1.3822, g_loss: 0.7495 \n",
            "\n",
            "Epoch [664/1000], Step [25/25], d_loss: 1.3726, g_loss: 0.7570 \n",
            "\n",
            "Epoch [665/1000], Step [25/25], d_loss: 1.3750, g_loss: 0.7887 \n",
            "\n",
            "Epoch [666/1000], Step [25/25], d_loss: 1.3788, g_loss: 0.7685 \n",
            "\n",
            "Epoch [667/1000], Step [25/25], d_loss: 1.3821, g_loss: 0.7720 \n",
            "\n",
            "Epoch [668/1000], Step [25/25], d_loss: 1.3619, g_loss: 0.7901 \n",
            "\n",
            "Epoch [669/1000], Step [25/25], d_loss: 1.3744, g_loss: 0.7711 \n",
            "\n",
            "Epoch [670/1000], Step [25/25], d_loss: 1.3769, g_loss: 0.7899 \n",
            "\n",
            "Epoch [671/1000], Step [25/25], d_loss: 1.3769, g_loss: 0.7959 \n",
            "\n",
            "Epoch [672/1000], Step [25/25], d_loss: 1.3755, g_loss: 0.7855 \n",
            "\n",
            "Epoch [673/1000], Step [25/25], d_loss: 1.3911, g_loss: 0.7785 \n",
            "\n",
            "Epoch [674/1000], Step [25/25], d_loss: 1.3693, g_loss: 0.7842 \n",
            "\n",
            "Epoch [675/1000], Step [25/25], d_loss: 1.3661, g_loss: 0.7812 \n",
            "\n",
            "Epoch [676/1000], Step [25/25], d_loss: 1.3723, g_loss: 0.7542 \n",
            "\n",
            "Epoch [677/1000], Step [25/25], d_loss: 1.3756, g_loss: 0.7966 \n",
            "\n",
            "Epoch [678/1000], Step [25/25], d_loss: 1.3755, g_loss: 0.7851 \n",
            "\n",
            "Epoch [679/1000], Step [25/25], d_loss: 1.3733, g_loss: 0.7828 \n",
            "\n",
            "Epoch [680/1000], Step [25/25], d_loss: 1.3711, g_loss: 0.7687 \n",
            "\n",
            "Epoch [681/1000], Step [25/25], d_loss: 1.3769, g_loss: 0.7852 \n",
            "\n",
            "Epoch [682/1000], Step [25/25], d_loss: 1.3682, g_loss: 0.7973 \n",
            "\n",
            "Epoch [683/1000], Step [25/25], d_loss: 1.3712, g_loss: 0.7809 \n",
            "\n",
            "Epoch [684/1000], Step [25/25], d_loss: 1.3830, g_loss: 0.8040 \n",
            "\n",
            "Epoch [685/1000], Step [25/25], d_loss: 1.3783, g_loss: 0.7902 \n",
            "\n",
            "Epoch [686/1000], Step [25/25], d_loss: 1.3717, g_loss: 0.7953 \n",
            "\n",
            "Epoch [687/1000], Step [25/25], d_loss: 1.3818, g_loss: 0.7658 \n",
            "\n",
            "Epoch [688/1000], Step [25/25], d_loss: 1.3749, g_loss: 0.7856 \n",
            "\n",
            "Epoch [689/1000], Step [25/25], d_loss: 1.3743, g_loss: 0.7793 \n",
            "\n",
            "Epoch [690/1000], Step [25/25], d_loss: 1.3755, g_loss: 0.7694 \n",
            "\n",
            "Epoch [691/1000], Step [25/25], d_loss: 1.3660, g_loss: 0.7746 \n",
            "\n",
            "Epoch [692/1000], Step [25/25], d_loss: 1.3815, g_loss: 0.7906 \n",
            "\n",
            "Epoch [693/1000], Step [25/25], d_loss: 1.3736, g_loss: 0.7698 \n",
            "\n",
            "Epoch [694/1000], Step [25/25], d_loss: 1.3829, g_loss: 0.7862 \n",
            "\n",
            "Epoch [695/1000], Step [25/25], d_loss: 1.3552, g_loss: 0.7770 \n",
            "\n",
            "Epoch [696/1000], Step [25/25], d_loss: 1.3753, g_loss: 0.7774 \n",
            "\n",
            "Epoch [697/1000], Step [25/25], d_loss: 1.3930, g_loss: 0.7535 \n",
            "\n",
            "Epoch [698/1000], Step [25/25], d_loss: 1.3792, g_loss: 0.7822 \n",
            "\n",
            "Epoch [699/1000], Step [25/25], d_loss: 1.3701, g_loss: 0.8157 \n",
            "\n",
            "Epoch [700/1000], Step [25/25], d_loss: 1.3781, g_loss: 0.7771 \n",
            "\n",
            "Epoch [701/1000], Step [25/25], d_loss: 1.3825, g_loss: 0.7687 \n",
            "\n",
            "Epoch [702/1000], Step [25/25], d_loss: 1.3651, g_loss: 0.7806 \n",
            "\n",
            "Epoch [703/1000], Step [25/25], d_loss: 1.3769, g_loss: 0.7731 \n",
            "\n",
            "Epoch [704/1000], Step [25/25], d_loss: 1.3849, g_loss: 0.7909 \n",
            "\n",
            "Epoch [705/1000], Step [25/25], d_loss: 1.3757, g_loss: 0.7773 \n",
            "\n",
            "Epoch [706/1000], Step [25/25], d_loss: 1.3767, g_loss: 0.7719 \n",
            "\n",
            "Epoch [707/1000], Step [25/25], d_loss: 1.3808, g_loss: 0.8034 \n",
            "\n",
            "Epoch [708/1000], Step [25/25], d_loss: 1.3769, g_loss: 9.7068 \n",
            "\n",
            "Epoch [709/1000], Step [25/25], d_loss: 1.3721, g_loss: 0.7785 \n",
            "\n",
            "Epoch [710/1000], Step [25/25], d_loss: 1.3748, g_loss: 0.7799 \n",
            "\n",
            "Epoch [711/1000], Step [25/25], d_loss: 1.3795, g_loss: 0.7910 \n",
            "\n",
            "Epoch [712/1000], Step [25/25], d_loss: 1.3740, g_loss: 0.7818 \n",
            "\n",
            "Epoch [713/1000], Step [25/25], d_loss: 1.3720, g_loss: 0.7733 \n",
            "\n",
            "Epoch [714/1000], Step [25/25], d_loss: 1.4250, g_loss: 0.7773 \n",
            "\n",
            "Epoch [715/1000], Step [25/25], d_loss: 1.3758, g_loss: 0.7735 \n",
            "\n",
            "Epoch [716/1000], Step [25/25], d_loss: 1.3745, g_loss: 0.7789 \n",
            "\n",
            "Epoch [717/1000], Step [25/25], d_loss: 1.3943, g_loss: 0.7700 \n",
            "\n",
            "Epoch [718/1000], Step [25/25], d_loss: 1.3811, g_loss: 0.8212 \n",
            "\n",
            "Epoch [719/1000], Step [25/25], d_loss: 1.3717, g_loss: 0.8524 \n",
            "\n",
            "Epoch [720/1000], Step [25/25], d_loss: 1.3760, g_loss: 0.7876 \n",
            "\n",
            "Epoch [721/1000], Step [25/25], d_loss: 1.3745, g_loss: 0.7764 \n",
            "\n",
            "Epoch [722/1000], Step [25/25], d_loss: 1.3781, g_loss: 0.7525 \n",
            "\n",
            "Epoch [723/1000], Step [25/25], d_loss: 1.3799, g_loss: 0.7851 \n",
            "\n",
            "Epoch [724/1000], Step [25/25], d_loss: 1.3734, g_loss: 0.7849 \n",
            "\n",
            "Epoch [725/1000], Step [25/25], d_loss: 1.3888, g_loss: 0.7607 \n",
            "\n",
            "Epoch [726/1000], Step [25/25], d_loss: 1.3765, g_loss: 0.7571 \n",
            "\n",
            "Epoch [727/1000], Step [25/25], d_loss: 1.3730, g_loss: 0.7906 \n",
            "\n",
            "Epoch [728/1000], Step [25/25], d_loss: 1.3712, g_loss: 0.7830 \n",
            "\n",
            "Epoch [729/1000], Step [25/25], d_loss: 1.4005, g_loss: 0.7862 \n",
            "\n",
            "Epoch [730/1000], Step [25/25], d_loss: 1.3700, g_loss: 0.7913 \n",
            "\n",
            "Epoch [731/1000], Step [25/25], d_loss: 1.3809, g_loss: 0.7836 \n",
            "\n",
            "Epoch [732/1000], Step [25/25], d_loss: 1.3587, g_loss: 0.7802 \n",
            "\n",
            "Epoch [733/1000], Step [25/25], d_loss: 1.3777, g_loss: 0.7916 \n",
            "\n",
            "Epoch [734/1000], Step [25/25], d_loss: 1.3777, g_loss: 0.7759 \n",
            "\n",
            "Epoch [735/1000], Step [25/25], d_loss: 1.3754, g_loss: 0.7794 \n",
            "\n",
            "Epoch [736/1000], Step [25/25], d_loss: 1.3806, g_loss: 0.7778 \n",
            "\n",
            "Epoch [737/1000], Step [25/25], d_loss: 1.3775, g_loss: 0.7766 \n",
            "\n",
            "Epoch [738/1000], Step [25/25], d_loss: 1.3820, g_loss: 0.7685 \n",
            "\n",
            "Epoch [739/1000], Step [25/25], d_loss: 1.3870, g_loss: 0.7877 \n",
            "\n",
            "Epoch [740/1000], Step [25/25], d_loss: 1.3775, g_loss: 0.7798 \n",
            "\n",
            "Epoch [741/1000], Step [25/25], d_loss: 1.3739, g_loss: 0.7820 \n",
            "\n",
            "Epoch [742/1000], Step [25/25], d_loss: 1.3775, g_loss: 0.7863 \n",
            "\n",
            "Epoch [743/1000], Step [25/25], d_loss: 1.3734, g_loss: 0.7741 \n",
            "\n",
            "Epoch [744/1000], Step [25/25], d_loss: 1.4298, g_loss: 2.0555 \n",
            "\n",
            "Epoch [745/1000], Step [25/25], d_loss: 1.3883, g_loss: 0.7604 \n",
            "\n",
            "Epoch [746/1000], Step [25/25], d_loss: 1.3788, g_loss: 0.7751 \n",
            "\n",
            "Epoch [747/1000], Step [25/25], d_loss: 1.3768, g_loss: 0.7650 \n",
            "\n",
            "Epoch [748/1000], Step [25/25], d_loss: 1.3722, g_loss: 2.5566 \n",
            "\n",
            "Epoch [749/1000], Step [25/25], d_loss: 1.3752, g_loss: 0.7807 \n",
            "\n",
            "Epoch [750/1000], Step [25/25], d_loss: 1.3767, g_loss: 0.7832 \n",
            "\n",
            "Epoch [751/1000], Step [25/25], d_loss: 1.3717, g_loss: 0.7751 \n",
            "\n",
            "Epoch [752/1000], Step [25/25], d_loss: 1.3776, g_loss: 0.8016 \n",
            "\n",
            "Epoch [753/1000], Step [25/25], d_loss: 1.3694, g_loss: 0.7835 \n",
            "\n",
            "Epoch [754/1000], Step [25/25], d_loss: 1.3781, g_loss: 0.7686 \n",
            "\n",
            "Epoch [755/1000], Step [25/25], d_loss: 1.3837, g_loss: 0.8061 \n",
            "\n",
            "Epoch [756/1000], Step [25/25], d_loss: 1.3794, g_loss: 0.7856 \n",
            "\n",
            "Epoch [757/1000], Step [25/25], d_loss: 1.3582, g_loss: 0.7689 \n",
            "\n",
            "Epoch [758/1000], Step [25/25], d_loss: 1.3646, g_loss: 0.7743 \n",
            "\n",
            "Epoch [759/1000], Step [25/25], d_loss: 1.3675, g_loss: 0.7805 \n",
            "\n",
            "Epoch [760/1000], Step [25/25], d_loss: 1.3652, g_loss: 0.8034 \n",
            "\n",
            "Epoch [761/1000], Step [25/25], d_loss: 1.3818, g_loss: 0.7788 \n",
            "\n",
            "Epoch [762/1000], Step [25/25], d_loss: 1.3752, g_loss: 0.7889 \n",
            "\n",
            "Epoch [763/1000], Step [25/25], d_loss: 1.3649, g_loss: 0.7902 \n",
            "\n",
            "Epoch [764/1000], Step [25/25], d_loss: 1.3863, g_loss: 0.7678 \n",
            "\n",
            "Epoch [765/1000], Step [25/25], d_loss: 1.3735, g_loss: 0.7847 \n",
            "\n",
            "Epoch [766/1000], Step [25/25], d_loss: 1.3712, g_loss: 0.8571 \n",
            "\n",
            "Epoch [767/1000], Step [25/25], d_loss: 1.3762, g_loss: 0.7780 \n",
            "\n",
            "Epoch [768/1000], Step [25/25], d_loss: 1.3754, g_loss: 0.7822 \n",
            "\n",
            "Epoch [769/1000], Step [25/25], d_loss: 1.3650, g_loss: 0.7908 \n",
            "\n",
            "Epoch [770/1000], Step [25/25], d_loss: 1.3687, g_loss: 0.7845 \n",
            "\n",
            "Epoch [771/1000], Step [25/25], d_loss: 1.3727, g_loss: 0.7594 \n",
            "\n",
            "Epoch [772/1000], Step [25/25], d_loss: 1.3779, g_loss: 0.7734 \n",
            "\n",
            "Epoch [773/1000], Step [25/25], d_loss: 1.3765, g_loss: 0.7850 \n",
            "\n",
            "Epoch [774/1000], Step [25/25], d_loss: 1.3676, g_loss: 0.7877 \n",
            "\n",
            "Epoch [775/1000], Step [25/25], d_loss: 1.3761, g_loss: 0.7786 \n",
            "\n",
            "Epoch [776/1000], Step [25/25], d_loss: 1.3677, g_loss: 0.7583 \n",
            "\n",
            "Epoch [777/1000], Step [25/25], d_loss: 1.3836, g_loss: 0.7728 \n",
            "\n",
            "Epoch [778/1000], Step [25/25], d_loss: 1.3811, g_loss: 0.7693 \n",
            "\n",
            "Epoch [779/1000], Step [25/25], d_loss: 1.3784, g_loss: 0.7663 \n",
            "\n",
            "Epoch [780/1000], Step [25/25], d_loss: 1.3814, g_loss: 0.7817 \n",
            "\n",
            "Epoch [781/1000], Step [25/25], d_loss: 1.3733, g_loss: 0.7798 \n",
            "\n",
            "Epoch [782/1000], Step [25/25], d_loss: 1.3821, g_loss: 0.7900 \n",
            "\n",
            "Epoch [783/1000], Step [25/25], d_loss: 1.3821, g_loss: 0.7536 \n",
            "\n",
            "Epoch [784/1000], Step [25/25], d_loss: 1.3752, g_loss: 0.7620 \n",
            "\n",
            "Epoch [785/1000], Step [25/25], d_loss: 1.3824, g_loss: 0.7726 \n",
            "\n",
            "Epoch [786/1000], Step [25/25], d_loss: 1.3821, g_loss: 0.7962 \n",
            "\n",
            "Epoch [787/1000], Step [25/25], d_loss: 1.3751, g_loss: 0.7926 \n",
            "\n",
            "Epoch [788/1000], Step [25/25], d_loss: 1.3824, g_loss: 0.7917 \n",
            "\n",
            "Epoch [789/1000], Step [25/25], d_loss: 1.3730, g_loss: 0.7680 \n",
            "\n",
            "Epoch [790/1000], Step [25/25], d_loss: 1.3639, g_loss: 0.7773 \n",
            "\n",
            "Epoch [791/1000], Step [25/25], d_loss: 1.3657, g_loss: 0.7910 \n",
            "\n",
            "Epoch [792/1000], Step [25/25], d_loss: 1.3761, g_loss: 0.7830 \n",
            "\n",
            "Epoch [793/1000], Step [25/25], d_loss: 1.3814, g_loss: 0.8940 \n",
            "\n",
            "Epoch [794/1000], Step [25/25], d_loss: 1.3751, g_loss: 0.7767 \n",
            "\n",
            "Epoch [795/1000], Step [25/25], d_loss: 1.3770, g_loss: 0.7833 \n",
            "\n",
            "Epoch [796/1000], Step [25/25], d_loss: 1.3751, g_loss: 0.7755 \n",
            "\n",
            "Epoch [797/1000], Step [25/25], d_loss: 1.3786, g_loss: 0.7783 \n",
            "\n",
            "Epoch [798/1000], Step [25/25], d_loss: 1.3763, g_loss: 0.7831 \n",
            "\n",
            "Epoch [799/1000], Step [25/25], d_loss: 1.3657, g_loss: 0.7637 \n",
            "\n",
            "Epoch [800/1000], Step [25/25], d_loss: 1.3789, g_loss: 0.7806 \n",
            "\n",
            "Epoch [801/1000], Step [25/25], d_loss: 1.3715, g_loss: 0.7769 \n",
            "\n",
            "Epoch [802/1000], Step [25/25], d_loss: 1.3684, g_loss: 0.7881 \n",
            "\n",
            "Epoch [803/1000], Step [25/25], d_loss: 1.3704, g_loss: 0.7810 \n",
            "\n",
            "Epoch [804/1000], Step [25/25], d_loss: 1.3798, g_loss: 0.7678 \n",
            "\n",
            "Epoch [805/1000], Step [25/25], d_loss: 1.3850, g_loss: 0.7840 \n",
            "\n",
            "Epoch [806/1000], Step [25/25], d_loss: 1.3758, g_loss: 0.7996 \n",
            "\n",
            "Epoch [807/1000], Step [25/25], d_loss: 1.3761, g_loss: 0.7751 \n",
            "\n",
            "Epoch [808/1000], Step [25/25], d_loss: 1.3737, g_loss: 0.7609 \n",
            "\n",
            "Epoch [809/1000], Step [25/25], d_loss: 1.3780, g_loss: 0.7714 \n",
            "\n",
            "Epoch [810/1000], Step [25/25], d_loss: 1.3754, g_loss: 0.7826 \n",
            "\n",
            "Epoch [811/1000], Step [25/25], d_loss: 1.3801, g_loss: 0.7856 \n",
            "\n",
            "Epoch [812/1000], Step [25/25], d_loss: 1.3743, g_loss: 0.7756 \n",
            "\n",
            "Epoch [813/1000], Step [25/25], d_loss: 1.3772, g_loss: 0.7838 \n",
            "\n",
            "Epoch [814/1000], Step [25/25], d_loss: 1.3777, g_loss: 0.7836 \n",
            "\n",
            "Epoch [815/1000], Step [25/25], d_loss: 1.3652, g_loss: 0.7849 \n",
            "\n",
            "Epoch [816/1000], Step [25/25], d_loss: 1.3603, g_loss: 0.7765 \n",
            "\n",
            "Epoch [817/1000], Step [25/25], d_loss: 1.3810, g_loss: 2.6379 \n",
            "\n",
            "Epoch [818/1000], Step [25/25], d_loss: 1.3822, g_loss: 0.7811 \n",
            "\n",
            "Epoch [819/1000], Step [25/25], d_loss: 1.3774, g_loss: 0.7908 \n",
            "\n",
            "Epoch [820/1000], Step [25/25], d_loss: 1.3756, g_loss: 0.7647 \n",
            "\n",
            "Epoch [821/1000], Step [25/25], d_loss: 1.3722, g_loss: 0.7766 \n",
            "\n",
            "Epoch [822/1000], Step [25/25], d_loss: 1.3783, g_loss: 0.7900 \n",
            "\n",
            "Epoch [823/1000], Step [25/25], d_loss: 1.3799, g_loss: 9.6971 \n",
            "\n",
            "Epoch [824/1000], Step [25/25], d_loss: 1.3799, g_loss: 0.7842 \n",
            "\n",
            "Epoch [825/1000], Step [25/25], d_loss: 1.3774, g_loss: 0.7773 \n",
            "\n",
            "Epoch [826/1000], Step [25/25], d_loss: 1.3761, g_loss: 0.7744 \n",
            "\n",
            "Epoch [827/1000], Step [25/25], d_loss: 1.3786, g_loss: 0.7924 \n",
            "\n",
            "Epoch [828/1000], Step [25/25], d_loss: 1.3608, g_loss: 0.7589 \n",
            "\n",
            "Epoch [829/1000], Step [25/25], d_loss: 1.3798, g_loss: 0.7480 \n",
            "\n",
            "Epoch [830/1000], Step [25/25], d_loss: 1.3771, g_loss: 0.7812 \n",
            "\n",
            "Epoch [831/1000], Step [25/25], d_loss: 1.3673, g_loss: 0.7809 \n",
            "\n",
            "Epoch [832/1000], Step [25/25], d_loss: 1.3769, g_loss: 0.7924 \n",
            "\n",
            "Epoch [833/1000], Step [25/25], d_loss: 1.3764, g_loss: 0.7790 \n",
            "\n",
            "Epoch [834/1000], Step [25/25], d_loss: 1.3793, g_loss: 0.7979 \n",
            "\n",
            "Epoch [835/1000], Step [25/25], d_loss: 1.3791, g_loss: 0.7891 \n",
            "\n",
            "Epoch [836/1000], Step [25/25], d_loss: 1.3723, g_loss: 0.7779 \n",
            "\n",
            "Epoch [837/1000], Step [25/25], d_loss: 1.3687, g_loss: 0.7900 \n",
            "\n",
            "Epoch [838/1000], Step [25/25], d_loss: 1.3620, g_loss: 0.7843 \n",
            "\n",
            "Epoch [839/1000], Step [25/25], d_loss: 1.3788, g_loss: 0.7832 \n",
            "\n",
            "Epoch [840/1000], Step [25/25], d_loss: 1.3621, g_loss: 0.7552 \n",
            "\n",
            "Epoch [841/1000], Step [25/25], d_loss: 1.3956, g_loss: 0.7482 \n",
            "\n",
            "Epoch [842/1000], Step [25/25], d_loss: 1.3770, g_loss: 0.7749 \n",
            "\n",
            "Epoch [843/1000], Step [25/25], d_loss: 1.3838, g_loss: 0.7640 \n",
            "\n",
            "Epoch [844/1000], Step [25/25], d_loss: 1.3958, g_loss: 0.7799 \n",
            "\n",
            "Epoch [845/1000], Step [25/25], d_loss: 1.3744, g_loss: 0.7860 \n",
            "\n",
            "Epoch [846/1000], Step [25/25], d_loss: 1.3810, g_loss: 0.7892 \n",
            "\n",
            "Epoch [847/1000], Step [25/25], d_loss: 1.3708, g_loss: 0.7890 \n",
            "\n",
            "Epoch [848/1000], Step [25/25], d_loss: 1.3807, g_loss: 0.7668 \n",
            "\n",
            "Epoch [849/1000], Step [25/25], d_loss: 1.3788, g_loss: 0.7802 \n",
            "\n",
            "Epoch [850/1000], Step [25/25], d_loss: 1.3781, g_loss: 0.7940 \n",
            "\n",
            "Epoch [851/1000], Step [25/25], d_loss: 1.3765, g_loss: 0.7740 \n",
            "\n",
            "Epoch [852/1000], Step [25/25], d_loss: 1.3784, g_loss: 0.7671 \n",
            "\n",
            "Epoch [853/1000], Step [25/25], d_loss: 1.3751, g_loss: 0.7744 \n",
            "\n",
            "Epoch [854/1000], Step [25/25], d_loss: 1.3692, g_loss: 0.7706 \n",
            "\n",
            "Epoch [855/1000], Step [25/25], d_loss: 1.3769, g_loss: 0.7770 \n",
            "\n",
            "Epoch [856/1000], Step [25/25], d_loss: 1.3695, g_loss: 0.7904 \n",
            "\n",
            "Epoch [857/1000], Step [25/25], d_loss: 1.3808, g_loss: 0.7713 \n",
            "\n",
            "Epoch [858/1000], Step [25/25], d_loss: 1.3828, g_loss: 0.7835 \n",
            "\n",
            "Epoch [859/1000], Step [25/25], d_loss: 1.3686, g_loss: 0.7643 \n",
            "\n",
            "Epoch [860/1000], Step [25/25], d_loss: 1.3762, g_loss: 0.7744 \n",
            "\n",
            "Epoch [861/1000], Step [25/25], d_loss: 1.3768, g_loss: 0.7851 \n",
            "\n",
            "Epoch [862/1000], Step [25/25], d_loss: 1.3803, g_loss: 0.7727 \n",
            "\n",
            "Epoch [863/1000], Step [25/25], d_loss: 1.3776, g_loss: 0.7686 \n",
            "\n",
            "Epoch [864/1000], Step [25/25], d_loss: 1.3759, g_loss: 0.7691 \n",
            "\n",
            "Epoch [865/1000], Step [25/25], d_loss: 1.3801, g_loss: 0.7839 \n",
            "\n",
            "Epoch [866/1000], Step [25/25], d_loss: 1.3645, g_loss: 0.7873 \n",
            "\n",
            "Epoch [867/1000], Step [25/25], d_loss: 1.3741, g_loss: 0.7939 \n",
            "\n",
            "Epoch [868/1000], Step [25/25], d_loss: 1.3757, g_loss: 0.7725 \n",
            "\n",
            "Epoch [869/1000], Step [25/25], d_loss: 1.3785, g_loss: 0.7593 \n",
            "\n",
            "Epoch [870/1000], Step [25/25], d_loss: 1.3788, g_loss: 0.7722 \n",
            "\n",
            "Epoch [871/1000], Step [25/25], d_loss: 1.3749, g_loss: 0.7769 \n",
            "\n",
            "Epoch [872/1000], Step [25/25], d_loss: 1.3694, g_loss: 0.7751 \n",
            "\n",
            "Epoch [873/1000], Step [25/25], d_loss: 1.3769, g_loss: 0.7846 \n",
            "\n",
            "Epoch [874/1000], Step [25/25], d_loss: 1.3808, g_loss: 0.7803 \n",
            "\n",
            "Epoch [875/1000], Step [25/25], d_loss: 1.3660, g_loss: 0.7720 \n",
            "\n",
            "Epoch [876/1000], Step [25/25], d_loss: 1.3924, g_loss: 0.7534 \n",
            "\n",
            "Epoch [877/1000], Step [25/25], d_loss: 1.3856, g_loss: 0.7595 \n",
            "\n",
            "Epoch [878/1000], Step [25/25], d_loss: 1.3788, g_loss: 0.7883 \n",
            "\n",
            "Epoch [879/1000], Step [25/25], d_loss: 1.3681, g_loss: 0.7836 \n",
            "\n",
            "Epoch [880/1000], Step [25/25], d_loss: 1.3817, g_loss: 0.7821 \n",
            "\n",
            "Epoch [881/1000], Step [25/25], d_loss: 1.3693, g_loss: 0.7868 \n",
            "\n",
            "Epoch [882/1000], Step [25/25], d_loss: 1.3691, g_loss: 0.8069 \n",
            "\n",
            "Epoch [883/1000], Step [25/25], d_loss: 1.3759, g_loss: 0.7924 \n",
            "\n",
            "Epoch [884/1000], Step [25/25], d_loss: 1.3777, g_loss: 0.7784 \n",
            "\n",
            "Epoch [885/1000], Step [25/25], d_loss: 1.3682, g_loss: 0.7823 \n",
            "\n",
            "Epoch [886/1000], Step [25/25], d_loss: 1.3780, g_loss: 0.7783 \n",
            "\n",
            "Epoch [887/1000], Step [25/25], d_loss: 1.3725, g_loss: 0.7902 \n",
            "\n",
            "Epoch [888/1000], Step [25/25], d_loss: 1.3768, g_loss: 0.7830 \n",
            "\n",
            "Epoch [889/1000], Step [25/25], d_loss: 1.3747, g_loss: 0.7702 \n",
            "\n",
            "Epoch [890/1000], Step [25/25], d_loss: 1.3578, g_loss: 0.7638 \n",
            "\n",
            "Epoch [891/1000], Step [25/25], d_loss: 1.3808, g_loss: 0.7820 \n",
            "\n",
            "Epoch [892/1000], Step [25/25], d_loss: 1.3617, g_loss: 0.7820 \n",
            "\n",
            "Epoch [893/1000], Step [25/25], d_loss: 1.3779, g_loss: 0.7911 \n",
            "\n",
            "Epoch [894/1000], Step [25/25], d_loss: 1.3802, g_loss: 0.7900 \n",
            "\n",
            "Epoch [895/1000], Step [25/25], d_loss: 1.3790, g_loss: 0.7750 \n",
            "\n",
            "Epoch [896/1000], Step [25/25], d_loss: 1.3674, g_loss: 0.7787 \n",
            "\n",
            "Epoch [897/1000], Step [25/25], d_loss: 1.3763, g_loss: 0.7718 \n",
            "\n",
            "Epoch [898/1000], Step [25/25], d_loss: 1.3674, g_loss: 0.7789 \n",
            "\n",
            "Epoch [899/1000], Step [25/25], d_loss: 1.3788, g_loss: 0.7789 \n",
            "\n",
            "Epoch [900/1000], Step [25/25], d_loss: 1.3751, g_loss: 0.7785 \n",
            "\n",
            "Epoch [901/1000], Step [25/25], d_loss: 1.3753, g_loss: 0.7764 \n",
            "\n",
            "Epoch [902/1000], Step [25/25], d_loss: 1.3787, g_loss: 0.7793 \n",
            "\n",
            "Epoch [903/1000], Step [25/25], d_loss: 1.3787, g_loss: 0.7709 \n",
            "\n",
            "Epoch [904/1000], Step [25/25], d_loss: 1.3781, g_loss: 0.7835 \n",
            "\n",
            "Epoch [905/1000], Step [25/25], d_loss: 1.3776, g_loss: 0.7723 \n",
            "\n",
            "Epoch [906/1000], Step [25/25], d_loss: 1.3721, g_loss: 0.7736 \n",
            "\n",
            "Epoch [907/1000], Step [25/25], d_loss: 1.3808, g_loss: 0.7763 \n",
            "\n",
            "Epoch [908/1000], Step [25/25], d_loss: 1.3781, g_loss: 0.7753 \n",
            "\n",
            "Epoch [909/1000], Step [25/25], d_loss: 1.3801, g_loss: 0.7801 \n",
            "\n",
            "Epoch [910/1000], Step [25/25], d_loss: 1.3715, g_loss: 0.7778 \n",
            "\n",
            "Epoch [911/1000], Step [25/25], d_loss: 1.3792, g_loss: 0.7797 \n",
            "\n",
            "Epoch [912/1000], Step [25/25], d_loss: 1.3609, g_loss: 0.7737 \n",
            "\n",
            "Epoch [913/1000], Step [25/25], d_loss: 1.3759, g_loss: 0.7731 \n",
            "\n",
            "Epoch [914/1000], Step [25/25], d_loss: 1.3740, g_loss: 0.7823 \n",
            "\n",
            "Epoch [915/1000], Step [25/25], d_loss: 1.3756, g_loss: 0.7746 \n",
            "\n",
            "Epoch [916/1000], Step [25/25], d_loss: 1.3722, g_loss: 0.7761 \n",
            "\n",
            "Epoch [917/1000], Step [25/25], d_loss: 1.3723, g_loss: 0.7757 \n",
            "\n",
            "Epoch [918/1000], Step [25/25], d_loss: 1.3647, g_loss: 0.7737 \n",
            "\n",
            "Epoch [919/1000], Step [25/25], d_loss: 1.3775, g_loss: 0.7762 \n",
            "\n",
            "Epoch [920/1000], Step [25/25], d_loss: 1.3838, g_loss: 0.7880 \n",
            "\n",
            "Epoch [921/1000], Step [25/25], d_loss: 1.3734, g_loss: 0.7945 \n",
            "\n",
            "Epoch [922/1000], Step [25/25], d_loss: 1.3703, g_loss: 0.7869 \n",
            "\n",
            "Epoch [923/1000], Step [25/25], d_loss: 1.3775, g_loss: 0.7780 \n",
            "\n",
            "Epoch [924/1000], Step [25/25], d_loss: 1.3802, g_loss: 0.7696 \n",
            "\n",
            "Epoch [925/1000], Step [25/25], d_loss: 1.3784, g_loss: 0.7714 \n",
            "\n",
            "Epoch [926/1000], Step [25/25], d_loss: 1.3792, g_loss: 0.7859 \n",
            "\n",
            "Epoch [927/1000], Step [25/25], d_loss: 1.3885, g_loss: 0.7930 \n",
            "\n",
            "Epoch [928/1000], Step [25/25], d_loss: 1.3780, g_loss: 0.7732 \n",
            "\n",
            "Epoch [929/1000], Step [25/25], d_loss: 1.3765, g_loss: 0.7727 \n",
            "\n",
            "Epoch [930/1000], Step [25/25], d_loss: 1.3759, g_loss: 0.7753 \n",
            "\n",
            "Epoch [931/1000], Step [25/25], d_loss: 1.3771, g_loss: 0.7777 \n",
            "\n",
            "Epoch [932/1000], Step [25/25], d_loss: 1.3751, g_loss: 0.7821 \n",
            "\n",
            "Epoch [933/1000], Step [25/25], d_loss: 1.3835, g_loss: 0.8539 \n",
            "\n",
            "Epoch [934/1000], Step [25/25], d_loss: 1.3789, g_loss: 0.7735 \n",
            "\n",
            "Epoch [935/1000], Step [25/25], d_loss: 1.3756, g_loss: 0.7773 \n",
            "\n",
            "Epoch [936/1000], Step [25/25], d_loss: 1.3771, g_loss: 0.7685 \n",
            "\n",
            "Epoch [937/1000], Step [25/25], d_loss: 1.3806, g_loss: 0.7811 \n",
            "\n",
            "Epoch [938/1000], Step [25/25], d_loss: 1.3729, g_loss: 0.7773 \n",
            "\n",
            "Epoch [939/1000], Step [25/25], d_loss: 1.3765, g_loss: 0.7907 \n",
            "\n",
            "Epoch [940/1000], Step [25/25], d_loss: 1.3693, g_loss: 0.7751 \n",
            "\n",
            "Epoch [941/1000], Step [25/25], d_loss: 1.3781, g_loss: 0.7741 \n",
            "\n",
            "Epoch [942/1000], Step [25/25], d_loss: 1.3750, g_loss: 0.7677 \n",
            "\n",
            "Epoch [943/1000], Step [25/25], d_loss: 1.3595, g_loss: 0.7684 \n",
            "\n",
            "Epoch [944/1000], Step [25/25], d_loss: 1.3736, g_loss: 0.7736 \n",
            "\n",
            "Epoch [945/1000], Step [25/25], d_loss: 1.3744, g_loss: 0.7746 \n",
            "\n",
            "Epoch [946/1000], Step [25/25], d_loss: 1.3937, g_loss: 0.7774 \n",
            "\n",
            "Epoch [947/1000], Step [25/25], d_loss: 1.3849, g_loss: 9.7100 \n",
            "\n",
            "Epoch [948/1000], Step [25/25], d_loss: 1.3793, g_loss: 0.7932 \n",
            "\n",
            "Epoch [949/1000], Step [25/25], d_loss: 1.3752, g_loss: 0.8056 \n",
            "\n",
            "Epoch [950/1000], Step [25/25], d_loss: 1.3717, g_loss: 1.4920 \n",
            "\n",
            "Epoch [951/1000], Step [25/25], d_loss: 1.3769, g_loss: 0.7808 \n",
            "\n",
            "Epoch [952/1000], Step [25/25], d_loss: 1.3797, g_loss: 0.7770 \n",
            "\n",
            "Epoch [953/1000], Step [25/25], d_loss: 1.3781, g_loss: 0.7727 \n",
            "\n",
            "Epoch [954/1000], Step [25/25], d_loss: 1.3799, g_loss: 0.7738 \n",
            "\n",
            "Epoch [955/1000], Step [25/25], d_loss: 1.3752, g_loss: 0.7740 \n",
            "\n",
            "Epoch [956/1000], Step [25/25], d_loss: 1.3750, g_loss: 0.7748 \n",
            "\n",
            "Epoch [957/1000], Step [25/25], d_loss: 1.3813, g_loss: 0.7642 \n",
            "\n",
            "Epoch [958/1000], Step [25/25], d_loss: 1.3810, g_loss: 0.7689 \n",
            "\n",
            "Epoch [959/1000], Step [25/25], d_loss: 1.3843, g_loss: 0.7771 \n",
            "\n",
            "Epoch [960/1000], Step [25/25], d_loss: 1.3604, g_loss: 0.7823 \n",
            "\n",
            "Epoch [961/1000], Step [25/25], d_loss: 1.3713, g_loss: 0.7863 \n",
            "\n",
            "Epoch [962/1000], Step [25/25], d_loss: 1.3685, g_loss: 0.7832 \n",
            "\n",
            "Epoch [963/1000], Step [25/25], d_loss: 1.3769, g_loss: 0.7769 \n",
            "\n",
            "Epoch [964/1000], Step [25/25], d_loss: 1.3665, g_loss: 0.7848 \n",
            "\n",
            "Epoch [965/1000], Step [25/25], d_loss: 1.3708, g_loss: 0.7859 \n",
            "\n",
            "Epoch [966/1000], Step [25/25], d_loss: 1.3940, g_loss: 0.7694 \n",
            "\n",
            "Epoch [967/1000], Step [25/25], d_loss: 1.3728, g_loss: 0.7749 \n",
            "\n",
            "Epoch [968/1000], Step [25/25], d_loss: 1.3787, g_loss: 0.7779 \n",
            "\n",
            "Epoch [969/1000], Step [25/25], d_loss: 1.3782, g_loss: 0.7702 \n",
            "\n",
            "Epoch [970/1000], Step [25/25], d_loss: 1.3840, g_loss: 0.7737 \n",
            "\n",
            "Epoch [971/1000], Step [25/25], d_loss: 1.3766, g_loss: 0.7775 \n",
            "\n",
            "Epoch [972/1000], Step [25/25], d_loss: 1.3767, g_loss: 0.7762 \n",
            "\n",
            "Epoch [973/1000], Step [25/25], d_loss: 1.3785, g_loss: 0.7696 \n",
            "\n",
            "Epoch [974/1000], Step [25/25], d_loss: 1.3784, g_loss: 0.7699 \n",
            "\n",
            "Epoch [975/1000], Step [25/25], d_loss: 1.3768, g_loss: 0.7657 \n",
            "\n",
            "Epoch [976/1000], Step [25/25], d_loss: 1.3814, g_loss: 0.7834 \n",
            "\n",
            "Epoch [977/1000], Step [25/25], d_loss: 1.3675, g_loss: 0.7813 \n",
            "\n",
            "Epoch [978/1000], Step [25/25], d_loss: 1.3715, g_loss: 0.7707 \n",
            "\n",
            "Epoch [979/1000], Step [25/25], d_loss: 1.3764, g_loss: 0.7755 \n",
            "\n",
            "Epoch [980/1000], Step [25/25], d_loss: 1.3772, g_loss: 0.7772 \n",
            "\n",
            "Epoch [981/1000], Step [25/25], d_loss: 1.3729, g_loss: 0.7703 \n",
            "\n",
            "Epoch [982/1000], Step [25/25], d_loss: 1.3778, g_loss: 0.7763 \n",
            "\n",
            "Epoch [983/1000], Step [25/25], d_loss: 1.3750, g_loss: 0.7709 \n",
            "\n",
            "Epoch [984/1000], Step [25/25], d_loss: 1.3769, g_loss: 0.7770 \n",
            "\n",
            "Epoch [985/1000], Step [25/25], d_loss: 1.3766, g_loss: 0.7785 \n",
            "\n",
            "Epoch [986/1000], Step [25/25], d_loss: 1.3734, g_loss: 0.7865 \n",
            "\n",
            "Epoch [987/1000], Step [25/25], d_loss: 1.3830, g_loss: 0.7894 \n",
            "\n",
            "Epoch [988/1000], Step [25/25], d_loss: 1.3752, g_loss: 0.7890 \n",
            "\n",
            "Epoch [989/1000], Step [25/25], d_loss: 1.3664, g_loss: 0.7773 \n",
            "\n",
            "Epoch [990/1000], Step [25/25], d_loss: 1.3659, g_loss: 0.7685 \n",
            "\n",
            "Epoch [991/1000], Step [25/25], d_loss: 1.3649, g_loss: 0.7703 \n",
            "\n",
            "Epoch [992/1000], Step [25/25], d_loss: 1.3596, g_loss: 0.7764 \n",
            "\n",
            "Epoch [993/1000], Step [25/25], d_loss: 1.3735, g_loss: 0.7816 \n",
            "\n",
            "Epoch [994/1000], Step [25/25], d_loss: 1.3776, g_loss: 0.7689 \n",
            "\n",
            "Epoch [995/1000], Step [25/25], d_loss: 1.3618, g_loss: 0.7824 \n",
            "\n",
            "Epoch [996/1000], Step [25/25], d_loss: 1.3789, g_loss: 0.7757 \n",
            "\n",
            "Epoch [997/1000], Step [25/25], d_loss: 1.3765, g_loss: 0.7748 \n",
            "\n",
            "Epoch [998/1000], Step [25/25], d_loss: 1.3777, g_loss: 0.7762 \n",
            "\n",
            "Epoch [999/1000], Step [25/25], d_loss: 1.3661, g_loss: 0.7734 \n",
            "\n",
            "Epoch [1000/1000], Step [25/25], d_loss: 1.3707, g_loss: 0.7711 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "losses_gen, losses_dis = gan.train(df_train, epochs=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "KTG2JvN9NMod",
        "outputId": "51e207ae-d08c-4fde-d054-507e81a24c2b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0cAAAHUCAYAAADx11NHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAAChCUlEQVR4nOzdd3hT1RsH8G860kFbSheUAi1Q6GLvvWTJcCAqMhR/KA4QxcUQRUVBQUD2UBSQqWwQUdl7b2ihm5buvdJm3d8ftWnTpm2SJk1avp/n4aG58703J8l57zn3XJEgCAKIiIiIiIiecBamDoCIiIiIiMgcMDkiIiIiIiICkyMiIiIiIiIATI6IiIiIiIgAMDkiIiIiIiICwOSIiIiIiIgIAJMjIiIiIiIiAEyOiIiIiIiIADA5IqInDJ97TZqwXBAREcDkiIg0CA8Px7x58zBkyBC0bdsWHTt2xJgxY7Bt2zbI5XJTh6eXrKwsfPrpp7h69aqpQ9HLpUuX4Ofnh0uXLpW7zMyZM+Hn56f65+/vj3bt2mHkyJFYuXIl8vPz1ZafMGECJkyYYOzQMWDAAMycObNK24iNjYWfnx/27NljoKiKHTt2DDNmzDD4dovMnDkTAwYMMNr2q6pkmfHz80NgYCC6du2K//3vfzhx4oTB9qNNGTaEPXv2lDkmTf+qQp9jqa7jJ6KqsTJ1AERkXg4fPoxZs2ahefPmeP3119G0aVPk5+fj1KlTmD9/Ps6cOYPVq1dDJBKZOlSdBAcHY//+/XjhhRdMHYpRubu7Y+XKlQAApVKJ7OxsXL16FevWrcPZs2exadMm2NjYAADmzp1bLTGtXLkSDg4OVdqGh4cHdu7ciSZNmhgoqmIbN240+DZrmtGjR+PFF18EAMhkMiQnJ2P37t14++238dlnn+HVV1+t8j6CgoKwc+dO+Pr6VnlbFenXrx927typen3y5EmsWbMGK1euhLu7u0H2oc+xVNfxE1HVMDkiIpXw8HDMmjULvXv3xo8//ggrq+KviL59+6Jr166YNm0a/vrrLwwbNsyEkVJ5xGIx2rVrpzatb9++aNu2LaZMmYJffvkF77zzDgBUWyUtMDCwytvQdFxkOA0aNChzfocNG4b33nsPCxcuxIABA9CoUaMq7cPBwaFa3kMXFxe4uLioXkdERAAAAgICqnwMRfQ5luo6fiKqGnarIyKVn3/+GRYWFvjqq6/UEqMiQ4YMwXPPPac2TalUYv369Rg0aBBatWqFIUOG4LffflNbZsKECfjss8+wfv169OvXD61bt8aYMWNw+/ZtteUePnyIt956Cx06dECHDh0wZcoUxMTEqOYXdUvZsWMH+vfvjw4dOuDcuXMAgD/++AOjRo1Cu3bt0KZNGzz77LP466+/VOsVXfl+9dVX1bqSHT58GKNGjUL79u3Rs2dPfPHFF8jMzFTNX7FiBQYNGoSVK1eiS5cu6NWrl9r8kkJCQjB16lR069YNQUFB6N27N7755hu17mx+fn7YunUrPvvsM3Tp0gXt27fH+++/j5SUFLVt7dixA0OGDEGbNm0wfvx4xMXFadyntgYOHIh27dphx44dqmmlu9WdO3cOL730Etq3b4/OnTvjnXfeQXh4uNp29u3bh+effx5t27ZFv379sHjxYkil0grPVcludUXd444cOYJ3330X7dq1Q48ePbB69Wrk5ORg9uzZ6NixI3r06IFFixap7gUq3a1uz549CAwMxK1bt/Dyyy+jdevW6N+/PzZs2KAWb2xsLD799FP06tULQUFB6N69Oz799FOkp6erzsHly5dx+fJltS5PSUlJmDVrFvr27Ys2bdpg9OjROHbsmNq2/fz8sHLlSowaNQpt2rRRtdjp686dO5g0aRK6du2KDh064O2330ZoaKjaMps2bcLQoUPRunVr9O7dG19++SVycnJ0eg91MX36dMhkMuzatQtA+V3DSpelAQMGYP78+XjttdfQpk0bfPbZZ2XWLSovJ0+exMiRI1XfH/v27VPbdnh4ON5880106NABPXr0wNKlSzFr1qwqdwktKlO//vorhg4dirZt22L37t0AgKNHj2Ls2LFo3749WrVqhaFDh2Lr1q2qdfU5FnM7fiLSjMkREakcO3YM3bp1g6ura7nLfP/992qtRl9++SWWL1+OZ555BmvXrsXQoUMxf/58rFq1Sm29v//+G8eOHcOcOXOwZMkSpKSk4L333oNCoQAAREZGYsyYMUhNTcX333+Pb7/9FjExMXjllVeQmpqqtq2VK1dixowZ+OKLL9C+fXts3boVX3zxBQYOHIh169bhhx9+gFgsxscff4yEhAQEBQXhiy++AAB88cUXqu5kq1evxocffoh27dph+fLlmDJlCv7++29MmDBBLaGJi4vDqVOnVJWSunXrljkvSUlJGDduHCQSCb777jv89NNPGD58OH777Tds3rxZbdmlS5dCqVRiyZIl+PTTT3HixAnMnz9fNX/Lli2YO3cu+vbti9WrV6Nt27b4/PPPK3zvtNGzZ08kJCTg8ePHZebFxMTg3XffRatWrbBmzRp8++23iIyMxOTJk6FUKgEAW7duxYwZMxAUFISVK1di8uTJ+O233/DNN9/odK4AYM6cOWjZsiXWrFmD7t27Y9myZRg9ejRsbW2xcuVKDB48GD///DOOHDlS7vEolUp88MEHGDZsGNavX48OHTpg4cKFOHPmDABAIpHg1VdfRXh4OObOnYsNGzbg1VdfxZ9//omlS5cCKOxaGBgYiMDAQOzcuRNBQUFISUnB6NGjcfXqVUyfPh0rVqyAl5cXpkyZggMHDqjFsHbtWowcORLLly/HkCFDdHtDSrh48SJeeeUVAMD8+fPxzTffID4+HmPGjFElN4cOHcKiRYswbtw4bNiwAVOmTMH+/fsxb948ANq9h7pq1qwZGjZsiGvXrum87tatW9G6dWusXr0ao0eP1rhMcnIyvv76a7z66qtYv349GjVqhBkzZqiOOS0tDePHj0d8fDwWLFiAOXPm4MiRIzh06JBex6PJihUr8Oabb2LhwoXo2bMnTp48iSlTpiAoKAirV6/GihUr0LhxY3z99de4detWudup7FjM9fiJSB271RERACAzMxOZmZnw8fEpM6/0IAwikQiWlpaIjIzE77//jg8//BCTJ08GAPTq1QsikQjr1q3D2LFjUa9ePdU2NmzYoLr3JDc3FzNmzEBwcDBatWqFlStXws7ODhs3blQt0717dwwcOBA///yz2g3zY8eOxdChQ1WvY2JiMGnSJLz77ruqaV5eXhg1ahSuXbuG4cOHq7qQ+fr6wtfXF5mZmVizZg1eeuklVeIEAC1btsS4ceOwe/dujBs3ThX7jBkz0KlTp3LP38OHDxEQEIBly5ap4u/RowfOnTuHS5cuqc5P0T4WLFigen379m1VEiAIAlavXo1hw4Zh9uzZqnOak5Oj1uqjDzc3NwBASkoKvLy81Obdvn0b+fn5eOutt1C/fn0AhV2tjh07hry8PNjb22PVqlUYOHCgWjIkkUjw559/QiaTAdDuXAFA79698cEHHwAAWrRogUOHDsHV1VX1XnTr1g0HDx7E9evX8fTTT2vchiAIePfdd1X3ynTs2BH//vsvTp48id69eyMqKgoNGjTA999/j8aNG6u2e+vWLVy+fBlAYXkoer+KujytWbMGaWlp+Pvvv1XnqW/fvpg4cSIWLlyIESNGwMKi8Npip06d8Prrr1d4rNpYvHgxvL29sX79elhaWgIofN8HDRqE5cuXY9myZbh8+TIaNWqEcePGwcLCAl26dIG9vb2qJbOy91Df+77c3NzKtGxqo2HDhvj4449VrzUNRCCRSPDtt9+ie/fuAAAfHx/0798fp06dQvPmzfHbb78hNzcX+/btUx1T27Ztq5SIlvb000+r3Yt46NAhPP/88/jss89U09q3b4+uXbvi0qVLaNu2rcbtVHYs+qxTHcdPROqYHBERAJR7ZTk6OhqDBw9Wm+bl5YXjx4/j4sWLEAQBAwYMUEugBgwYgDVr1uDatWsYOHAgAPVKKADVD71EIgFQeOW8S5cusLW1VW3LwcEBnTp1wvnz59X2HxAQoPa6qMtWVlYWIiIiEB0draqIFXX5Ku3mzZuQSqUYMWKE2vROnTrBy8sLly9fViVHmvZZWq9evdCrVy/IZDKEhYUhOjoaDx8+RFpaGpydndWWLX3fQYMGDVTnISIiAqmpqejfv7/aMk8//XSVk6OiLmqaBtNo27YtbGxsMHr0aAwdOhR9+vRB165d0aZNGwCFXXtSU1MxaNAgtfUmTZqESZMmqU2r7FwBhZXNIkVJW9G+imKsW7cusrOztd6OWCyGi4sL8vLyVHFs27YNSqUSUVFRiI6ORlhYGCIiIiocdfHy5cto3759mQTymWeewaxZsxAREaFKtrU51srk5eXhzp07mDp1qioxAgAnJydVRRkoTOx27tyJUaNGYeDAgejbty9Gjhypej8rew/1JQiCXgOwaHtuSn4eGjRoAACq9/DixYto37696vsCKPz+Kfm+V1XpON944w0AhRdwIiMj8ejRI9y5cwdA+d8nRSo6Fn3WqY7jJyJ1TI6ICABQr1492Nvbl+ly5enpqbrfAABWrVqFhw8fAgAyMjIAAMOHD9e4zcTERNXfdnZ2avOKrrwXJWUZGRk4fPgwDh8+XGY7JW+uBgB7e3u1148ePcIXX3yBCxcuwNraGs2aNYO/vz+A8p9fU3S1vahiXpKbm1uZSnmdOnU0bqdIUTe5rVu3Ii8vD56enmjTpo1qZLiSNJ2LojiL4ipqcStiiFG2it6PkhWtIo0aNcKWLVuwfv167Nq1C5s3b4aTkxPGjh2LDz74QPVeV9Tlskhl5wqAxlaM0u+rNmxtbdVelzyXAPDrr79i7dq1yMjIgJubG1q1agU7O7sKk67MzExVS1NJRWUlKyurSjGXlp2dDUEQKi2Lw4YNg1KpxLZt21Tdvby8vPDxxx9j2LBhlb6H+o4wmZCQgJYtW+q8nrbnpuTnoeh7oeg9TEtLQ1BQUJl19G3N0qR0nGlpaZg7dy6OHj0KkUgEb29vVUtoZc/DquhY9FmnOo6fiNQxOSIilQEDBuDEiRPIyclRVV7FYjFat26tWqZkK4iTkxOAwpvENVWIGzZsqPW+HR0d0aNHD41dlDQNDlFEqVRi8uTJsLa2xq5duxAQEAArKyuEhYVh//795a5XdC9MSkoKmjVrpjYvOTlZY+W4IuvXr8fGjRvx1VdfYfDgwXB0dASAcu+1KE9RUlT6Pqui5KQqzp8/D29vb43JEQDVoAJSqRTXrl3Dzp07sXbtWvj7+6taStLS0tTWSU9Px/37983ySvbBgwfx3Xff4ZNPPsGoUaNUSfb777+vagnQpG7dukhOTi4zvWha6cS1qhwdHSESiTRWdpOTk9U+cyNGjMCIESOQnZ2Ns2fP4qeffsInn3yCjh07on79+hW+h+V1T6xIWFgYkpOTVa2oRQlW6Zbm3NxcrZJiXTVo0EDjeSn9+TCkjz/+GBEREdi4cSPat28PsVgMiUSC33//3Wj7LI8pjp/oSccBGYhIZfLkyZDL5ZgzZ47G7iP5+flqo8cVXU1NT09H69atVf/S0tKwbNkynSr0Xbp0QVhYGAICAlTbadWqFTZu3Ih///233PXS09MRGRmJ0aNHo3Xr1qpE6vTp0wCKK3EluysBhV2QxGJxmRubr169iri4OHTo0EHr2AHg2rVr8PX1xQsvvKBKjBITE/Hw4UOdbob38fGBp6dnmYEIqvowzpMnT+LOnTuqm/5L27hxI/r37w+pVAqxWIzu3burbvSPi4tDs2bNUK9evTJx7N+/H5MnT1bdc2ROrl27BicnJ7zxxhuqxCg3NxfXrl1Te0+KrtYX6dy5M27cuFGmFfXAgQNwd3eHt7e3QeO0t7dHq1at8Ndff6kGKAEKW5ROnjyJjh07AgA++OADTJkyBUBhQvX000/j3XffhVwuR1JSUqXvoT6WL18OW1tbPP/88wCKW/wSEhJUy2RmZlZpRLyKdO7cGTdv3lRLVpOSknDz5k2j7A8oLDeDBw9G165dIRaLAZT9Pqkupjh+oicdW46ISMXPzw+LFi3CrFmzMGrUKIwePRp+fn6Qy+W4ceMGdu3ahZSUFFWffD8/PzzzzDP4/PPP8fjxY7Rq1QqRkZFYunQpGjVqpHFwh/K8++67GDNmDN566y288sorsLGxwc6dO3H06FEsX7683PVcXV3h5eWFrVu3okGDBnBycsKZM2dUI8QV3ctTlLCcPHkSdevWhb+/PyZPnoxVq1bB2toa/fv3R2xsLJYtWwZfX19VZVBbbdq0werVq7F+/Xq0a9cO0dHRWLduHaRSqSoGbYhEInz88cf46KOPMGfOHAwdOhQ3b97E9u3btVpfKpWqKk6CICArKwtXr17F5s2b0bVrV4wfP17jet26dcMPP/yAKVOmYPz48bC0tMSOHTsgFovRv39/WFpa4r333sPXX38NV1dXDBgwAJGRkVi+fDnGjRtX7qh0ptSmTRts374d3333Hfr374+kpCRs2LABKSkpavE6OTnhxo0buHDhAgIDA/H666/jwIEDmDhxIqZOnQpnZ2fs27cPFy9exPz588skU9rIycnR+LDZhg0bYvDgwfjoo48wadIkTJ48GWPHjoVMJsP69eshlUpVCVG3bt0wd+5cfP/99+jTpw+ysrKwcuVK+Pj4wN/fH9bW1hW+hxVJSEhQlRu5XI7ExETs3bsXZ8+exddff626F8bPzw+enp5YtWoVHBwcVIOvlO4qaiivvvoqtm7dikmTJqnOw+rVqyGTyYz2IOo2bdrg4MGDCAoKQoMGDXD9+nWsX78eIpFIp8+yIZji+ImedEyOiEjNkCFD0KpVK2zfvh27du3C48ePIQgCGjdujGHDhmHMmDFqSc+CBQuwbt067NixAwkJCXB1dcWwYcPwwQcflGmtqYi/vz+2bt2KpUuX4tNPP4UgCGjZsiVWrVqFp556qsJ1V69ejW+//RYzZ86EWCyGr68v1qxZg/nz5+Pq1auYMGECWrRogREjRmDr1q04c+YMDh06hPfeew9ubm7YsmULdu7cCWdnZwwdOhQffPCBzveSvPXWW0hPT8fmzZuxatUqeHp64tlnn1VVHrOyslTdECtTNBra6tWrsX//frRs2RJff/01Pvzww0rXTU5Oxssvv6x6bW9vj6ZNm2LatGmYMGECrK2tNa7n7++PtWvXYtWqVfjwww+hUCjQqlUr/PLLL6puh+PGjYO9vT02bNiAnTt3okGDBnjzzTfx5ptvanVc1e35559HbGwsdu/ejW3btqF+/fro27cvxo4di88//xzh4eFo3rw5xo0bh7t37+LNN9/EggULMHLkSGzfvh2LFy/GN998A5lMBn9/f6xevbrSsliezMxMtREKi3Tv3h2DBw9G9+7d8euvv2L58uX48MMPIRaL0alTJ3z//fdo0aIFAGDMmDGQyWTYsWMHtm3bBltbW3Tv3h2ffPIJrK2ttXoPy7Nr1y7VvYUWFhZwdnZG27Zt8euvv6pGUgMKW2CXL1+O+fPn48MPP4Sbmxtee+01REREIDIyUq9zUxEnJyds3rwZ3377LT799FPUqVMHY8eOhZ2dnUHu99Lku+++w7x581Stbj4+Pvjqq69w4MABXL161Sj7LI8pjp/oSScSKrtTkIiIiMgEbt26hYyMDPTt21c1TS6Xo1+/fhg+fDhmzZplwuiM70k/fiJTYMsRERERmaW4uDhMnz4dU6ZMQZcuXSCRSLBz505kZ2fjpZdeMnV4RvekHz+RKbDliIiIiMzW9u3bsW3bNsTExMDa2hpt27bF+++/rzaKZm32pB8/UXVjckRERERERAQO5U1ERERERASAyREREREREREAJkdEREREREQAavFodTdu3IAgCOU+04OIiIiIiJ4MRQ9Pbt++fYXL1dqWI0EQYC5jTQiCAKlUajbxkPljmSFdscyQrlhmSFcsM6Qrcyoz2uYGtbblqKjFyByGuszLy0NwcDB8fX35RGvSCssM6YplhnTFMkO6YpkhXZlTmblz545Wy9XaliMiIiIiIiJdMDkiIiIiIiICkyMiIiIiIiIATI6IiIiIiIgAMDkiIiIiIiICwOSIiIiIiIgIAJMjIiIiIiIiAEyOiIiIiIiIADA5IiIiIiIiAsDkiIiIiIiICACTIyIiIiIiIgBMjoiIiIiITCYvLw8//vgjhg4dijZt2qBr166YNm0aQkNDTR2aRjk5Odi3b5+pwzAaJkdERERERCaQm5uLV155BX/++Sc++eQT/PXXX9iwYQPq1KmDMWPGICYmxtQhlrFx40bs3r3b1GEYjZWpAyAiIiIiehKtWrUKqampOHz4MJycnAAAXl5eWLBgAeLj47Fx40Z8/vnnJo5SnSAIpg7BqJgcVYN78afwMP8K6ufWhb29r6nDISIiIqrVMiVShCRlVus+/T3qoq6dWOvllUol9u7dizfeeEOVGJW0cOFC1fSrV69i/vz5CAsLg7e3N6ZOnYohQ4YAAGbOnIm6desiMTERJ06cgLOzM6ZPn47nnnsOACCVSrFw4UIcPHgQANC7d2/MmTMHzs7OiI2NxVNPPYVp06Zh48aNGDlyJD7//HOsW7cOv//+O5KSkuDs7IwxY8Zg6tSp2LNnD1auXAkA8PPzw4MHD1BQUIDly5fj0KFDyMzMRLdu3TB37lx4enoiLi4OY8eOxTvvvIOtW7di5MiR+OKLL6pymo2OyVE1uBd/CgBwKmwLxrl/adpgiIiIiGqxTIkUzb7diwyJtFr362wnRsRnz2udID169AhpaWno1KmTxvkeHh4AgOTkZLz11luYPn06evfujZs3b2LmzJlwdXVVrbt161a8//77+Oijj7B582bMnTsXTz31FBwdHbFkyRLcvXsXP/30E2xsbLB06VK8//772LRpk2pf169fx+7du6FUKrFv3z5s2rQJS5YsQePGjXHmzBl8+eWX6N+/P4YNG4bQ0FDcuHEDK1asAADMnTsX169fx/fffw9nZ2f88MMPePfdd9W63t26dUu1fXPH5KgayRT5pg6BiIiIiMxAeno6AKBu3bqqaefPn8eUKVNUrxs2bIhBgwahR48eGD9+PADA29sbwcHB2LRpkyo58vPzw5tvvgkAeP/997F582aEhoYiICAAW7Zswe7du+Hn5wegsEWqa9euePDgAerUqQMAeO2119CkSRMAQEJCAhYsWIDu3bsDAF555RWsWrUKoaGhCAoKgr29PaytreHu7o7MzEzs378fP/30E7p16wYA+OGHH9CvXz+cO3cOnp6eAICxY8eqtm/umBwRERERUa1R978WHHPvVlfUZS4rK0s1rX379qqR4P755x9s374dEREROHHiBNq3b69aTiaToWnTpqrXPj4+qr8dHBwAAHK5HDExMZDJZBgzZozavpVKJaKiohAUFASg8D6nIt26dcOtW7ewePFihIeHIzg4GMnJyRpbfaKioqBUKtG2bVvVNGdnZzRt2hTh4eGq5Khhw4ZanxdTY3JERERERLVKXTsxunq7mzqMCnl7e8PZ2Rk3btxAmzZtAAB2dnbw9vYGALi6ugIoTHJGjhyJt99+W219K6viary1tXWZ7QuCAIVCAQDYtm0b7O3t1ea7uroiIyMDAGBjY6Oa/scff2D+/Pl48cUXMXjwYMyYMQOvvvqqxmMouV5JCoVCLZkqbzlzxKG8iYiIiIiqmZWVFV544QVs2rQJOTk5ZeYnJiYCAJo2bYro6Gh4e3ur/h07dkw1wEJFGjduDEtLS2RkZKjWdXBwwIIFC5Camqpxne3bt2PKlCmYPXs2nnvuOdSrVw+pqamqUepEIpHa9q2srHDz5k3VtPT0dERHR6u1bNUkTI6IiIiIiEzgvffeg7u7O8aMGYMjR44gJiYGt2/fxueff47ly5ejY8eOGDt2LO7evYulS5ciKioKBw8exJIlS7Tqqubg4IAXX3wRX375JS5duoSwsDB8+umniI6ORqNGjTSuU69ePVy4cAGRkZG4e/cupk+fDplMBqm0cIALOzs7JCUlITY2FnXq1MGLL76IefPm4dKlSwgJCcEnn3yCBg0aoGfPngY9V9WF3eqIiIiIiEzAzs4Ov/32GzZt2oTVq1cjOjoaYrEYbdq0wYoVKzBw4EAAwNq1a/HDDz9gw4YNqF+/PmbOnIlnnnlGq33MnDkT33//PaZNmwaZTIbOnTtj/fr1sLS01Lj87NmzMXv2bDz77LNwdXXF008/DTs7OwQHBwMABg0ahB07dmD48OE4fvw4ZsyYodq+VCpFjx49sHHjRojF2t9/ZU5EQi19ktOdO3cAAK1btzZxJMDGszNVf0/s9Z0JI6GaIi8vD8HBwQgICCjTR5hIE5YZ0hXLDOmKZYZ0ZU5lRtvcgN3qiIiIiIiIwOSIiIiIiIgIAJMjIiIiIiIiAEyOiIiIiIiIADA5IiIiIiIiAsDkiIiIiIiICACTIyIiIiIiIgBMjowuT5pt6hCIiIiIiEgLTI6MTK4oMHUIRERERESkBSZHRERERETVbMCAAfDz84Ofnx/8/f3Rvn17jBkzBmfOnFEt4+fnh0uXLhl0vytWrMCECRP0WnfAgAHYs2ePQeK4cOECwsPDDbItQzJpchQdHY1Jkyahffv26NevH37++WfVvG+++UZVYIr+bdmyxYTREhEREREZzuzZs3H27FmcOnUKO3fuRIcOHfDWW2/h/PnzAICzZ8+iffv2Bt3n//73P6xYsUKvdXft2oVhw4YZJI6JEyciJSXFINsyJCtT7VipVGLy5Mlo3bo19u7di+joaHz44YeoX78+Ro4cifDwcHz00Ud4/vnnVes4ODiYKlwiIiIiIoNydHSEu7s7AKB+/fr49NNPkZycjAULFuDgwYOqeYZUp04dvdd1cXExYCTmyWTJUUpKCgICAvDll1/CwcEBPj4+6N69O65du6ZKjiZNmmSUQkFEREREtZdUno9MSVK17rOunQfEVrZV3s7LL7+McePGITo6GoMHD8bmzZvRtWtXXLhwAd999x0iIiLg4eGBN998E2PGjAEApKamYt68eTh9+jTs7OzwwgsvYPr06Xj8+DGeeuopTJs2DRs3bsTIkSNRr149XL58Gb/99hv27NmDvXv3okePHvjll18gFovxySefwNbWFt9//z2ys7Px8ssv45NPPgFQ2K1u6tSpGDVqFCZMmIAePXrg6tWruHLlCjw9PTFnzhz07t0bABAWFoZvvvkGN27cgFKpROvWrTFv3jw0b94cAwYMAAC8+uqrmDp1Kt577z3cuHEDCxcuRHBwMFxcXPDmm2/ilVdeAQDMnDkTAHD//n0kJydj+/bt8PHxqfK51sRkyZGHhwd+/PFHAIAgCLh+/TquXLmCuXPnIicnB4mJiUY7aCIiIiKqnaTyfOy68h2kivxq3a/Y0hajO8+scoLUvHlzAIXJRRGFQoEPPvgAEydOxMiRI3H9+nXMmDEDnTp1gq+vL6ZMmQJLS0ts2bIFubm5mD59Ojw8PNCvXz8AwPXr17F7924olUocPHhQbX83btxA48aNsWvXLmzduhVffvklAgMDsWbNGty9exefffYZhg8fjsDAwDKxrl27FnPnzsXcuXOxePFifP755zh+/DgA4O2330aXLl3w0ksvoX79+li4cCEWLVqEtWvXYteuXejevTtWrFiBnj17Ijw8HK+99homTpyIb7/9Frdu3cJXX30FNzc3DBo0CACwf/9+rFq1Cm5ubkbNEUyWHJU0YMAAxMXFoX///hgyZAju3r0LkUiEtWvX4vTp03B2dsbrr7+u1sVOG4IgIC8vz0hRa0eSL1F7bep4qGaQSCRq/xNVhmWGdMUyQ7qqKWVGqsiHYIL9CgDyJHmQWyq1Wl6pVEIqlZapG1paWgIA0tPTAQAFBQVISkpCRkYGHB0d4eLigoEDB2Lt2rVwcHDAzZs3cePGDRw6dAheXl4AgFmzZkEikSA/vzBBHDNmDNzc3AAAMpkMCoUCeXl5kEqlEAQBH330Eezs7PDMM89g06ZNmDx5Mpo0aYImTZpg8eLFCA4Oho+Pj1rMCoUCvXr1wtChQwEAr7/+Ol5++WU8evQIjo6OeOGFFzBy5EhVg8fw4cOxadMm5OXlwda2MIG0tbWFSCTC1q1b4efnh7fffhsA0KBBA4SEhGDdunXo2bMn5HI5goKC0K1bNwD61acFQYBIJKp0ObNIjpYvX46UlBR8+eWXWLBgAYKCgiASidCsWTOMHz8eV65cweeffw4HBwdV9qgNmUyG4OBgI0ZeuQKl+nOOTB0P1SxRUVGmDoFqGJYZ0hXLDOmqJpSZFtZDUWCVVa37tBE5IfxhpNbLy2QyxMXFlakb5uTkAADS0tIAFA5gVqdOHQwcOBDz5s3DmjVrVIOZOTg44OLFi3BwcEBWVhaysgqPuUGDBgCKW58kEolqP8nJycjLy0NwcDDi4uLg5OSkek+Tk5MBQDUfACwsLBAdHY3g4GC1mPPy8uDl5aW2XQAICQmBu7s72rRpgy1btiAyMhJxcXGIjIxE3bp11Y636Nju3r2rti0AcHV1RXh4OIKDg5GZmYk6depUuR4tFosrXcYskqPWrVsDKMyMP/74Y1y/fh39+/eHs7MzAMDf3x9RUVHYvn27TsmRtbU1fH19jRGy1rLzU/HwfvHrgIAA0wVDNYZEIkFUVBR8fHxgZ2dn6nCoBmCZIV2xzJCuWGYMy9raGg0bNixTN7x69SoAoH///li8eDG8vb0REBCARYsWITw8HCdOnMCJEycwd+5cLF26FD4+PrC0tNRYx6xbty4AIDAwEA0bNgQAuLu7Izo6GgEBAQgNDYWNjY1q3aLlW7ZsqVq+ZJwl/7a3t4enp2eZdX19feHs7Izx48fDyckJgYGBeP755xEXF4fNmzerxVl0bC4uLnBxcVGbFx8fD6Cw7ly07arUo0t2U6yISQdkuHnzJgYOHKia5uvrC5lMhpycnDKjYTRr1gwXL17UaR8ikQj29vYGiVdfcpF6s5+p46Gaxc7OjmWGdMIyQ7pimSFdscwYhoWFBcRicZlzeejQIQQFBaFFixYAABsbG+Tm5mL16tWYNWsWWrdujWnTpmHSpEk4d+4cxo8fj8zMTGRmZsLT0xMAsHnzZly8eBGzZ88GUNh9rWg/1tbWsLS0hL29PcRiMSwsLFTzSnZ3K5pWMs6Sf1taWsLa2lrjunfu3EFycjJ+//13hIaGIiAgACtXrixTN7exsYG9vT18fX1x5coVtXn3799H06ZNYW9vDyurwpSlKuVOmy51gAmfcxQbG4upU6ciMTFRNe3u3btwcXHBb7/9hokTJ6otHxISgmbNmlVzlERERERExpGdnY3k5GQkJSXhwYMH+Pbbb3H48GHV6GxF6tati3///Rfz58/Ho0ePcOXKFYSEhCAwMBAtWrRAt27d8Nlnn+HBgwe4dOkS1q9fj549e5roqABnZ2fk5eXhxIkTSE5Oxp49e7B161ZIpVLVMvb29ggNDUV2djbGjh2L4OBgLFmyBJGRkdi7dy+2bduGcePGVXvsJkuOWrdujaCgIMyePRthYWE4deoUFi1ahLfffhv9+/fHlStXsGHDBjx69Ajbtm3Dvn378L///c9U4RIRERERGdT8+fPRq1cv9OnTB6+//joiIyOxceNGdOnSRW05sViM1atXIyQkBM888ww++OADjB49Gi+++CIAYNGiRbCzs8PLL7+Mjz76CC+//DLGjh1rikMCALRv3x5TpkzBggULMHPmTBw8eBBffPEFUlNTVQ0jEyZMwMKFC7FixQo0bNgQ69atw5kzZzBy5EisWbMGM2fOxAsvvFDtsYsEQTDFgB4AgMTERMybNw8XLlyAnZ0dxo8fj7feegsikQhHjx7F8uXLERUVBS8vL0yfPh2DBw/Wett37twBUHw/k6lkSVKw59oPqtcTe31nwmiopii6EbKoTy9RZVhmSFcsM6QrlhnSlTmVGW1zA5MOyFC/fn2sXLlS47yBAweq3Y9ERERERERkTCbrVkdERERERGROmBwRERERERGByREREREREREAJkdEREREREQAmBwREREREREBYHJEREREREQEgMkRERERERERACZHREREREREAJgcERERERERAWByREREREREBIDJkdEJEEwdAhERERERaYHJEREREREREZgcERERERERAWByREREREREBIDJkdGJIDJ1CEREREREpAUmR0RERERERGByREREREREBIDJEREREREREQAmR0RERERERACYHBEREREREQFgckRERERERASAyREREREREREAJkdEREREREQAmBwREREREREBYHJEREREREQEgMkRERERERERACZHREREREREAJgcERERERERAWByREREREREBIDJEREREREREQAmR0RERERERACYHBEREREREQFgckRERERERASAyREREREREREAJkdGJ0AwdQhERERERKQFJkdEREREREQwcXIUHR2NSZMmoX379ujXrx9+/vln1byYmBhMnDgR7dq1w7Bhw3D27FkTRkpERERERLWdyZIjpVKJyZMno169eti7dy+++uorrFmzBgcPHoQgCJgyZQrc3Nywe/duPPvss5g6dSri4uJMFS4REREREdVyVqbacUpKCgICAvDll1/CwcEBPj4+6N69O65duwY3NzfExMRgx44dsLe3R/PmzXHhwgXs3r0b7733nqlCJiIiIiKiWsxkLUceHh748ccf4eDgAEEQcO3aNVy5cgVdunTBrVu3EBgYCHt7e9XyHTt2xM2bN00VLhERERER1XImazkqacCAAYiLi0P//v0xZMgQzJ8/Hx4eHmrLuLq6IiEhQaftCoKAvLw8Q4aqs/z8fLXXpo6HagaJRKL2P1FlWGZIVywzpCuWGdKVOZUZQRAgEokqXc4skqPly5cjJSUFX375JRYsWACJRAKxWKy2jFgshlQq1Wm7MpkMwcHBhgxVZwXKbLXXpo6HapaoqChTh0A1DMsM6YplhnTFMkO6MpcyUzq/0MQskqPWrVsDAAoKCvDxxx/jhRdeKJNhSqVS2Nra6rRda2tr+Pr6GixOfWTnp+Lh/eLXAQEBpguGagyJRIKoqCj4+PjAzs7O1OFQDcAyQ7pimSFdscyQrsypzISFhWm1nEkHZLh58yYGDhyomubr6wuZTAZ3d3dERESUWb50V7vKiEQitfuWTEEuUu9GZ+p4qGaxs7NjmSGdsMyQrlhmSFcsM6Qrcygz2nSpA0w4IENsbCymTp2KxMRE1bS7d+/CxcUFHTt2xL1799Tu17l27Rratm1rilCJiIiIiOgJYLLkqHXr1ggKCsLs2bMRFhaGU6dOYdGiRXj77bfRpUsXeHp6YtasWQgNDcX69etx+/ZtjB492lThEhERERFRLWey5MjS0hKrV6+GnZ0dXn75ZXz22WeYMGECXn31VdW85ORkjBo1CgcOHMCqVavQsGFDU4VLRERERES1nEkHZKhfvz5WrlypcZ63tze2bNlSzREREREREdGTymQtR0REREREROaEyRERERERERGYHBEREREREQFgckRERERERASAyREREREREREAJkdEREREREQAmBwREREREREBYHJEREREREQEgMkRERERERERACZHREREREREAJgcERERERERAWByZHQCBFOHQEREREREWmByREREREREBCZHREREREREAJgcERERERERAWByREREREREBIDJkfFxPAYiIiIiohqByZGRpeclmDoEIiIiIiLSApMjI1Mo5aYOgYiIiIiItMDkiIiIiIiICEyOiIiIiIiIADA5IiIiIiIiAsDkyOhEEJk6BCIiIiIi0gKTIyIiIiIiIjA5IiIiIiIiAsDkiIiIiIiICACTIyIiIiIiIgBMjoyP4zEQEREREdUITI6IiIiIiIjA5IiIiIiIiAgAkyMiIiIiIiIATI6qAW86IiIiIiKqCZgcERERERERgckRERERERERACZHREREREREAEycHCUmJmLatGno0qULevfujQULFqCgoAAA8M0338DPz0/t35YtW0wZLhERERER1WJWptqxIAiYNm0anJycsHXrVmRmZmL27NmwsLDAjBkzEB4ejo8++gjPP/+8ah0HBwdThUtERERERLWcyVqOIiIicPPmTSxYsAAtWrRAp06dMG3aNBw6dAgAEB4ejsDAQLi7u6v+2dnZmSpcIiIiIiKq5UyWHLm7u+Pnn3+Gm5ub2vScnBzk5OQgMTERPj4+pgmOiIiIiIieOCbrVufk5ITevXurXiuVSmzZsgXdunVDeHg4RCIR1q5di9OnT8PZ2Rmvv/66Whc7bQiCgLy8PEOHrhPpf/dQFTF1PFQzSCQStf+JKsMyQ7pimSFdscyQrsypzAiCAJGo8uePmiw5Km3RokW4f/8+du3ahXv37kEkEqFZs2YYP348rly5gs8//xwODg4YNGiQ1tuUyWQIDg42YtSVy5DHqb02dTxUs0RFRZk6BKphWGZIVywzpCuWGdKVuZQZsVhc6TJmkRwtWrQImzZtwtKlS9GyZUu0aNEC/fv3h7OzMwDA398fUVFR2L59u07JkbW1NXx9fY0UtXYepSkRE1X8OiAgwGSxUM0hkUgQFRUFHx8f3mtHWmGZIV2xzJCuWGZIV+ZUZsLCwrRazuTJ0bx587B9+3YsWrQIQ4YMAQCIRCJVYlSkWbNmuHjxok7bFolEsLe3N1SoerHJVc9QTR0P1Sx2dnYsM6QTlhnSFcsM6YplhnRlDmVGmy51gImfc7Ry5Urs2LEDS5YswfDhw1XTly1bhokTJ6otGxISgmbNmlVzhERERERE9KQwWXIUHh6O1atX480330THjh2RnJys+te/f39cuXIFGzZswKNHj7Bt2zbs27cP//vf/0wVLhERERER1XIm61Z37NgxKBQKrFmzBmvWrFGb9+DBAyxbtgzLly/HsmXL4OXlhcWLF6N9+/YmirYqtGvCIyIiIiIi0zJZcjR58mRMnjy53PkDBw7EwIEDqzEiIiIiIiJ6kpn0niMiIiIiIiJzweSIiIiIiIgITI6MTstRA4mIiIiIyMSYHBkdsyMiIiIiopqAyRERERERERGYHBEREREREQFgckRERERERASAyREREREREREAJkdEREREREQAmBwREREREREBYHJEREREREQEgMkRERERERERACZHRifiQ2CJiIiIiGoEJkdERERERERgcmR0AgRTh0BERERERFpgckRERERERAQmR0bHe46IiIiIiGoGJkdERERERERgckRERERERASAyREREREREREAJkdEREREREQAmBwREREREREBYHJEREREREQEgMkRERERERERACZHREREREREAJgcGZ+ID4ElIiIiIqoJmBwRERERERGByREREREREREAJkdEREREREQAmBwZHe84IiIiIiKqGZgcGR3TIyIiIiKimoDJEREREREREZgcERERERERAWByREREREREBIDJkdHxjiMiIiIioprBpMlRYmIipk2bhi5duqB3795YsGABCgoKAAAxMTGYOHEi2rVrh2HDhuHs2bOmDJWIiIiIiGo5kyVHgiBg2rRpkEgk2Lp1K5YuXYoTJ07gxx9/hCAImDJlCtzc3LB79248++yzmDp1KuLi4kwVLhERERER1XJWptpxREQEbt68iXPnzsHNzQ0AMG3aNHz//ffo06cPYmJisGPHDtjb26N58+a4cOECdu/ejffee89UIRMRERERUS2md8vR9evXkZaWBgDYt28f3nrrLaxbtw6CIGi1vru7O37++WdVYlQkJycHt27dQmBgIOzt7VXTO3bsiJs3b+obLhERERERUYX0ajnasWMHvvrqK/zyyy+oV68eZs2ahe7du2Pjxo2QyWSYOnVqpdtwcnJC7969Va+VSiW2bNmCbt26ITk5GR4eHmrLu7q6IiEhQac4BUFAXl6eTusYWtE9VEVMHQ/VDBKJRO1/osqwzJCuWGZIVywzpCtzKjOCIEAkqnyoNL2So02bNmHOnDno3r07lixZghYtWuCXX37BmTNnMHfuXK2So9IWLVqE+/fvY9euXdi4cSPEYrHafLFYDKlUqtM2ZTIZgoODdY7FkLIU6vdJmToeqlmioqJMHQLVMCwzpCuWGdIVywzpylzKTOn8QhO9kqPY2FgMGDAAAHDu3Dn06dMHANC8eXOkpKTovL1FixZh06ZNWLp0KVq2bAkbGxtkZGSoLSOVSmFra6vTdq2treHr66tzPIYUl2GB6Iji1wEBAaYLhmoMiUSCqKgo+Pj4wM7OztThUA3AMkO6YpkhXbHMkK7MqcyEhYVptZxeyZGrqyuSkpJgZWWF4OBgfPzxxwCAkJCQMvcQVWbevHnYvn07Fi1ahCFDhgAA6tevX+YAUlJSynS1q4xIJFK7b8kUbCQ2aq9NHQ/VLHZ2diwzpBOWGdIVywzpimWGdGUOZUabLnWAnsnR8OHD8fHHH8POzg4NGjRAly5dcPjwYcybNw+jR4/WejsrV67Ejh07sGTJEgwdOlQ1vW3btli/fj3y8/NVrUXXrl1Dx44d9QnXtLR8I4iIiIiIyLT0So4++ugjNGjQADExMRg3bhwsLS2RmpqKMWPGaD3Udnh4OFavXo3JkyejY8eOSE5OVs3r0qULPD09MWvWLLz77rs4ceIEbt++jQULFugTLhERERERUaX0So4sLCwwYcIEtWmlX1fm2LFjUCgUWLNmDdasWaM278GDB1i9ejU+++wzjBo1Ct7e3li1ahUaNmyoT7hERERERESV0is5kkql+OWXX/D000/D29sbn332GQ4fPowOHTrghx9+QL169SrdxuTJkzF58uRy53t7e2PLli36hEdERERERKQzvR4C+8MPP+DXX39FTk4OTp8+jb179+Ktt95Cbm4uFi5caOgYiYiIiIiIjE6v5OjIkSNYsmQJgoKCcOzYMXTp0gVvv/025syZg5MnTxo4xJqOAzIQEREREdUEeiVHGRkZaN68OYDC5xz17NkTAODs7Iz8/HzDRUdERERERFRN9LrnqEmTJrhz5w5SU1MRGxuL3r17AwCOHj2KRo0aGTTA2kYpKGAhsjR1GEREREREVIpeydEbb7yBDz/8EBYWFujWrRv8/f2xatUqrFq1CvPnzzd0jLVKfEY4vOq1NHUYRERERERUil7J0XPPPQd/f3/ExsaiT58+AIDWrVtjw4YN6N69u0EDrOlK33GkFJQmiYOIiIiIiCqmV3IEAP7+/mjUqBFCQkJgbW2NDh06wMHBwZCxERERERERVRu9kiOlUonvv/8e27Ztg1wuhyAIEIvFePnllzF79myIRByhjYiIiIiIaha9kqN169Zh9+7d+OSTT9ClSxcolUpcuXIFq1atQv369fHGG28YOk4iIiIiIiKj0is5+uOPPzB37lyMHDlSNS0wMBAuLi5YsWIFkyM1pVvRBJNEQUREREREFdPrOUepqalo27Ztmelt27ZFfHx8lYOqVdjDkIiIiIioRtArOfLx8cH58+fLTD937hy8vLyqHBQREREREVF106tb3euvv44vvvgCMTEx6NChAwDg2rVr2Lp1Kz799FODBkhERERERFQd9H7OUUZGBn7++Wds2LABAODm5obp06dj3LhxBg2wphME3mNERERERFQT6P2co4kTJ2LixIlIS0uDIAhwdXXFlStX8NRTT+HYsWOGjLGGK50c8SYkIiIiIiJzpHdyVMTFxUX1d35+PuLi4qq6yVqGo9UREREREdUEeg3IQLpgMkREREREVBMwOTIy3nNERERERFQzMDkiIiIiIiKCDvccrVy5stJloqOjqxRM7cSWIyIiIiKimkDr5GjPnj1aLefp6al3MLURu9UREREREdUMWidHx48fN2YctZbAliMiIiIiohqB9xwZmbWljalDICIiIiIiLTA5MjJXBy9Th0BERERERFpgckRERERERAQmR0RERERERACYHFU/jl5HRERERGSWmBwRERERERGByVE1YEsREREREVFNwOSIiIiIiIgITI6IiIiIiIgAMDkyutLjL6TnJZomECIiIiIiqhCTo2oWmx5itG2HxF/AX7fXIkuSYrR9EBERERHVVkyOapGL4fuRmBWF48GbTR0KEREREVGNw+TI6Kp/tLqMvKRq3ycRERERUU3H5Ki6cWRvIiIiIiKzZBbJkVQqxYgRI3Dp0iXVtG+++QZ+fn5q/7Zs2WLCKI1HoZRDKShMHQYRERER0RPNytQBFBQU4KOPPkJoaKja9PDwcHz00Ud4/vnnVdMcHByqO7wqs7G2r3B+gVyC7Re/AgAMa/MOPJy8AQCxaQ8QmngZ7b2HwNnew+hxEhERERE96UzachQWFoaXXnoJjx49KjMvPDwcgYGBcHd3V/2zs7MzQZRVYyGyVHstlOpXd//xWdXfh2+vUf199P6viE69hyN31hs3QCIiIiIiAmDilqPLly+ja9eumD59Otq1a6eanpOTg8TERPj4+FRp+4IgIC8vr2pBGphSqVCLqUCarza/dLz5shy9jiEs/iYa1m2pX5BkchKJRO1/osqwzJCuWGZIVywzpCtzKjOCIEAkElW6nEmTo7Fjx2qcHh4eDpFIhLVr1+L06dNwdnbG66+/rtbFThsymQzBwcGGCNVgJJJ8tZhSZalq8zXFq88xnA3fgRY2Q2Br4aR7kGQ2oqKiTB0C1TAsM6QrlhnSFcsM6cpcyoxYLK50GZPfc6RJREQERCIRmjVrhvHjx+PKlSv4/PPP4eDggEGDBmm9HWtra/j6+hoxUu3cuV78d54yFQEBAarXssePkZxYPL9oXsl1Si6v7X4AwMXTHt4u2q1L5kUikSAqKgo+Pj41sjspVT+WGdIVywzpimWGdGVOZSYsLEyr5cwyOXruuefQv39/ODs7AwD8/f0RFRWF7du365QciUQi2NtXPCCCKUiRAyc7V1iILGFlba02T2xjBStL9aw2UxqP+nWbwkKk2y1iYrHYLI+ftGdnZ8f3kHTCMkO6YpkhXbHMkK7Mocxo06UOMJOhvEsTiUSqxKhIs2bNkJiYqHmFGmbf9SU4FbK98EWp5x5djjhUZvm/7/6EW4+OAgCy89Nw6OYq3PzvdVXEpj3AqQfbkSVJrXxhIiIiIqJaziyTo2XLlmHixIlq00JCQtCsWTPTBGQE0al3NU5/mHhZ4/RbMccBAGcf/oGUnBiDJEdH7/+KyORbOHr/1ypvi4iIiIiopjPL5Kh///64cuUKNmzYgEePHmHbtm3Yt28f/ve//5k6NJMKT7qBxKxIg283S5Ji8G0SEREREdU0ZpkctWnTBsuWLcP+/fsxYsQI/Pbbb1i8eDHat29v6tAMTqGUab3smYc7jRgJEREREdGTzWwGZHjw4IHa64EDB2LgwIEmiqb6BMefN3UIREREREQEM205IiIiIiIiqm5MjoiIiIiIiMDkqJYTEJceimP3NyEtJ87UwRARERERmTUmR7XcP/c2ICYtGAdvrazSdpSC0kARERERERGZJyZHTwihCslNdOo9bLvwJW7HnDRYPERERERE5obJkQmdefi7qUPQyong3yBXSnE9+oipQyEiIiIiMhomRyYUnnTd1CEQEREREdF/mBwRERERERGByREREREREREAJkc1miAIFc4/F7q7miIhIiIiIqr5mBzVYkpBYeoQDEKprB3HQURERETmjclRjVZxy5G5EwRlpa1fwXHnseXCXDxMuFJNURERERHRk4rJERndpYiDOHZ/M+RKmWqaXCHDvutLceDGMiiU8grWPQClIMf5MHYRJCIiIiLjYnJUg50M2V5py4uppecmIDjuHGLS7uPe4zOq6aGJV5ApSUZ6XgKiUm6bMELTkimkpg6BiIiIiP7D5KgGi069g4y8RINsKy0nDvfjzhm8sl4gz1P9nS1JVf0tVxbvp6KWo9rsfNgebL/4FR6nPzR1KEREREQEJkc13q2YYwbZzoGby3E54iCuRh42yPYEQclWkUo8TLgMpaDAv/d+MXUoRERERAQmRzVeVModg27vQcJFAIXJzc1HR/UaCEEQBBy+vRY7L3+DrBKtReUur+fAEgWyPFyJOMSWFyIiIiIyCCtTB0BV98/dDYjLCDXoNsOSruPmo6N6rZsnzUJy9iMAwKWI/arp+iZB5bkQvg9RKbdxL+4sJvb6zqDbJiIiIqInD1uOagFDJ0YAkJQVXe6+LobvR540u4K1i5MghRGfURRt4FYzIiIiInqyseWIdPLP3Q0AgIy8RAxtPVmndQtkeZUvRERERERkImw5MkPRqfdMHQJEEFU4PyEzQsvtFItNDykxbHfF2yciIiIiqm5MjszQieDfTB2C0ZwM2WbqEIiIiIiINGK3uifIvcdn0NyjAwDA1rpOhcvmy3IMtFcRUNlADBpmXwo/gPjMcAPFQERERERUOSZHT5ArkX/iSuSfEMECg4JeL3e55OwYPEq7b5idapEblaZQyhEcf94w+yciIiIi0hK71T2BBCjx7/1fy51/5uHOKu6hxP1E2iRGpW4/EgRtsynet0REREREhsPk6AlVUQKifXKiBW3ylzK7027/ApS6RkNEREREVC4mR08okciYrS7FyU1lo94BwKO0e3iYcEWnpKzi5ywREREREemOydETSmRGb/3j9Ic4H7YbsWnBWq9z69FRI0ZERLVFToEM47acwffH7po6FCIiqgHMp4ZM1cq4LUf6bTsuIwyAdp3q5EqZXvsgoifLvH9uY8eNKMw+fAOJ2RJTh0NERGaOydETSmGA5EIi1X+4b03pk6DrsHZERJW4FZeu+junQG7CSIiIqCZgckR6yy1Ir3whjihHRERERDUEkyMqIzs/1ajbvx93TmO3OIOOkkdEREREpCM+BJb0pimVCU28ijsxJytc73LEQT22TERERERkXEyOSG/3486ioXMLJGZGok3j/nCyc8O50F1qy4hEIh1yndqZFAmCgD13HsG9ji36NK9v6nCIiIiIqBzsVkd6i0y+hXOhuxCWdA1/3VlX5e1VlhpdjTyMAlnefwvXnETq7wdxeGnTafRf/Q8SsgwzWlaWJAUP4i9BpigwyPaIiIiIyEySI6lUihEjRuDSpUuqaTExMZg4cSLatWuHYcOG4ezZsyaMkCojkWZDodQ0EpQOSUwlCc/dx6dxIXyfTnGZgz23H6n+Dk7KNMw2r/2AC+F7cTF8v0G2R0RERERmkBwVFBTgww8/RGhoqGqaIAiYMmUK3NzcsHv3bjz77LOYOnUq4uLiTBgpVea383M0TNVjtLoKcqSolNu6b68WC0+6buoQiIiIiGoNk95zFBYWho8++qjMKGUXL15ETEwMduzYAXt7ezRv3hwXLlzA7t278d5775koWtKHLs9TKnrOkVJQaLEsEREREZFhmTQ5unz5Mrp27Yrp06ejXbt2qum3bt1CYGAg7O3tVdM6duyImzdvVn+QVG1CE68gNPFKpcvtuPQNxFZ21RARERERET1JTJocjR07VuP05ORkeHh4qE1zdXVFQkKCTtsXBAF5eXl6x0fmKV+Wg3xZjto0c36f5fLie7EK8vM1xlp6mkQiUfu/IuZ87FR9dCkzTxKlsrglWpIvQV6epQmjMS8sM6QrlhnSlTmVGUEQCkdRroRZDuUtkUggFovVponFYkilUp22I5PJEBwcbMjQyEyZ8/uckpau+vtBRBTcC8o+ZLe8+KOioirdvjkfO1U/bcrMkyQnJ1f1d3hYOAocxRUs/WRimSFdscyQrsylzJTOLzQxy+TIxsYGGRkZatOkUilsbW112o61tTV8fX0NGJl+7vCeeaMLCAgoMy044SzS8+LRyXskxJa6lR1DOrXroervo4lyvDmwMNaS5aJ0/BKJBFFRUfDx8YGdXdkuhBWtS0+mysrMk8rhcgqAwgSpuW9zNHNxMG1AZqSmlJn99x5DJAKeCfQyWQx5Ujm+PnYPHRrWw0ttm5gsDlOrSpmRKZRYdSEU/u5OGOrnaaQIn1zbb0bj0qNUfD24NZxsrU0djoo5fc+EhYVptZxZJkf169cvcwApKSllutpVRiQSqd23RLVX6fe5QJ6HO3HHAQAOds7o1vxZU4QFAJDIi7v1FCgEjWWyvHJqZ2dXaRlmGacicqWAuccfoImrEz4d0MrU4ZgFC4vibnR2tpV/np5E2nzPmMrVmFSM33Gx8O/pw9G+kYtJ4vj6+HWsOl9YLxnTqQVsrZ/s7pn6lJklJ+/j87/vAgAy54+Bg435VOBrOoVSicm7rwIArKyssXp0VxNHVJY5fM9o06UOMIOhvDVp27Yt7t27h/z8fNW0a9euoW3btiaMisxZWk4cMvKS8M/dDXiYcBlyRfEoeem58SaMjKj67AtPx5qL4Zj15w3cS8gwdThEVXY2IlH194WoZJPFcSy0+HekQF75iKpU1sF7Maq/M/O1H8mWKqcsMYTvqXDd7s+nsswyOerSpQs8PT0xa9YshIaGYv369bh9+zZGjx5t6tDITB24uRz7ri9BXEYozoftgUif5ysR1XCh6cUXlOIyOVAHERGRrswyObK0tMTq1auRnJyMUaNG4cCBA1i1ahUaNmxo6tCoBuIzkYiIiIhIG2Zzz9GDBw/UXnt7e2PLli0mioZqPLWGI/NPj0KTs9DC3cnUYRARERE90cyy5YjoSeP/3X5Th0BERET0xGNyRLXSxbB9qr9l8gLTBUJERLWK+fdFMH+CwLNoLDy1VcfkiGqlR2n3VX+n5yXgftw5k8XCLyoyBRY7IsPhED9Vp+0wyqQ7nlnDYnJET4TLEQdNHUKl9t+NqXwhogrxJ5KIiKgqmBwRmYlRv540dQhERERETzQmR0RERERERGByRGR0hupmnZQVjcsRh5AnzTLMBomISCu8h8+weC+u8fDUVp3ZPOeIqDaRyhW4EJ2Crk3c1KcrlFpvQyZI1Eb0OXx7DQAgJYf3JlHlePcRERGR7pgcmakRbafCTuyIOjZ1sfHszAqXfbXHt8jKT8W1qCOIKTFKG1Wvb/69jZDETPz0cndM2X0Zm66EY2RQI7VlzkQkITQ5C75ujhVuKyb9HkLyD0ES/Qj9A8eqzUvKijZ47FT78OohkeHwYoNhceA64+GprTp2qzNTbo6NUMemrlbLWlhYwtneA08Fvlrpsi91no0uTUdUNTwqJSotB3OP3ML2G1EYsPofbLoSDgA4eC+2TPcB/+/248u/b1W4vQuRuwEA0Wm3jRIvEREREZXF5OgJY2/jxGcNVJGmh9el50lVf19+lKo2T6lh+W/+vWP4wIiIiIioSpgc0RPjUviBKm9j/JYz8Jm3B5Gp2QCATIkUXZb+ibFbzlR520RERERkWkyOzFBnHbq92Vo7qL0eGDgR9Z2awrNuc9U0Z/v6sLGqg94tXzZYjDVRcPz5Kq2fnS/D9htRiM3Mwzu7LgEA5v17G9di0/Aw2TAjyD21+h/EZ+WpTdPUUkVERFRT8WfNeHhqq44DMpiZ13rOh0ikXc7aon4ndPIZpjatkYs/Grn440b0P4jPLLzvpU2j/mjq3pbd6QBk5CXB2d5Dr3VLdo/792E85AolNl4ON1RoAICT4Yl4Z9clPNeyeNqBe7F4tlVjg+6HiIj0wwtW+mENhGoKthyZUOemw8tM0zYxAoD6Tk1hY22v1bLqiVHx3/6e3dSWG9zqDa33XxPlFmSUO2/j5XD0XP4XbsSmaZx/LDRB7bXNp1uRLpFqXLYq7idkqr3+5l/dBmVQKOVIyoqGUlAYMiyqAUp+yll/IzIcXlwkenIwOaoBnO3rAwA86/qqTdc2kRIqaGR1tHVVe93Q2Rd+DbqVs3TNl5ITC5lCc0Izaed5XIxOQb/Vf2uc/+KmU8YMzWDOPPwdh2+vwYWwfaYOhYiIiKhGYXJkQpYW1ujnP67S5Ya2nozeLV9Cv4BxaO7eXsutl3+VSz2pKrtcIxc/tddujrWnS9eN6H/w138PUy1PToFc7XVitgQfH7hqzLDUVJTMaiMqpbClKTTxiiHCIXpipeUV4IN9V/Dn/VhTh0Imxq50RE8O3nNkIk62bvD16AgrS2uM6ToHVyIPw6uen8Zlba3roLlHBwBAHRtn1XRRBQlQyYEaxFZ2avOae7THzUf/QhAEtKjfCVciD1UY65BWbyIhIwwNnJtj64W5avOaubdHRPKNCtc3N2m58aq/4zLCcCXiEFIKAstdvteKI4hIzTFoDB51CsqdV3pf12PTEJuRa9D9E5UUmpwF73p1ILayNHUoZmXq7svYeTMKK86EQLF4gqnDISKqFBP5qmNyZCLPdfwQFv+14NhaO6B3y5e0Ws/Psytux54AAHi5aE6mAKBlg854lHoXVpY2aFSvpdo8a0sbjO40AwIAa0txpfu0thSjsavm5KF3yxfhUscTwfHnK7yfx9woBSUepd7DyZCtAABLJAAIUs3Pk8qx+OR9dPV2M3hiBAA+9fJ1Wr71ooNY+rTBw6BaTNtbJLZei8Cr286hX/P6OPbuYOMGVcPsvxtj6hCIaiVW38mcMTkyEQsdBl4oqY6NM17sPAsWIkvYlGoRKsnSwgpDWr9Z7nyrCpMi7W88FYks0KpRH7Rq1AfxGeH4++5PWq9rSpvPza5w/nt7LmPjFcOORKevtzvHYF+wfiPs0ZNFn3vGX912DkDhSIlERFTzMNk0LN5zVAPVsakLO7FD5QtWM0/n5pUvVMJTgRNhJ3Y0UjRVs/FKOHxd8uDrklf5wnqoZyvTetnOjbIwo0+kUeKoiFJQVvs+qWpK9qaoKT0rsiQpSMiMYFcQ0oogCNh28StsPDsTEml2te2Xo9UZFs+m8bCsVh2TIxOwsrA2dQhmQWxpa7YVovoOBZjVNxKz+kbC07H8+4P09VJr3a7SO9lU77Dc8Rnh2H7xK1yN/Kta90tPFrlCij3XfsCRO+vxOP2hqcOhGiA86TqkcgkA4NSD7SaOhohqIyZH1axF/U4Y0W6qqcOokGudhqq/2zUZqDbPzrq4pad9k0Ea1vXSej/mfHUjyCNH49/G4iCWV75QNfr77k+QKQpw93HNGL6caqacEvcphsSfN10g5TDjr6gnVlTKHdXfCZkRJoyEiGor3nNUzXq2GG3qEMrwdg1CdOo9BDTsCQCwt3HCsDbvQCrPLzOs9wudPkFo4hU42rqVmQcATwW9hocJl+FVzw9/3lpV4X7r2Tcw25aj6rZs+AN88KcfsqWm/0gqlOaVqBERUe3CX37jYb2q6thyROjj9wqGt30XnZsOV03zcPLWmPxYWYoR0LCnxnkAYC92QrsmA+FeybOROjcdAWsrG/h5dqla8LVID+8MU4cAAMiSpJo6BDKRO/HpaLXwABYcvVP5wiaiVAqQK3g/XG0Tl5mHr/++hZDETFOHUmNcfpSCYw/jK1/QTLAllmoKJkcESwsruDs20XsEvfI81+HDcue51PEEALRpPEDt2U0EvNLa1D92vOpUU1W18vHMhhMITszEnL9uGiQeQ5MrlOi45E80/3YvMiRStXklr5YKgqDV1VOFUj3JSsnJR1a+tJylDUMQBCiVxvuMCYKApGyJXusqlQJyC7QfLMaQhv90HF/9cxtBCw/ovG7p99HYzOEb8nFmHrov+wuD1x3Fxehkg2xTKlfofC6VSgGhyVlsraiAjBdzahwmR2Q0zvbqw0+3aTxA9be9jROAwsEpXuw8E41dAsqsX9/Jx6jxmauBvmkG3V52fhquRBxSe/itockUSpNVqp5UuQUyhKeUP1qXUKIKJ1MocSc+vdIKzKN0zQ8bzsqX4ujDeEjlZQcGyZPKIVcokZQtgVIpqC2TmC1RlQtBEFAgVyAlJx+7bkUjTyqHVFG8bIFcide2ncOOG5Gq5X+7GoHDwY9Vy/xv53ncjk9HbGYeFp+8p5q+/sJDeH21CwfuxkChVKLvyr/RZtHBcsvk6rMPYPnRbxB/slV1DhOyJGgybzdazN8Hiaywa6lEVhyfISp/2fkyWH28BdafbMGy08FaryeVKzB47b8Y+fPxSiuv7+6+BM8vd2HrNe3ux7kQl4Oeq45i/90YDFjzDxp+tQsPksq23qw9/xCWH/2GlvP3lZkXl5mH1gsPYPLvF9SmnwhLQJ8VRzD59wto/NUu/Ha1/Jhux6drFW96nvoAOT+cuId6n+3Er5fDtFpfX6WvOyRkSdBz+V/49OA1o+63PJcfpaj+3nP7UZW3l5KTD+95e9Bh8Z86JUiT/7gA/+/244cT93XaX6clf8Llsx3otfyIVhckfroYikZf7cKh+7GqaR/su4KW8/fhXkIGBq/9F8//csKoFx70cSo8Ea5zdmLanstq03ffjob/gn3Yd6fq711F8mUK9FlxBHVnb4flR7/B7tOtem9rzbkHaDF/L04/AY99YHJE1aZdk6cQ0LAnOjcdjrp27mrzngp8DQMDJ6pNe7rN25jY67tqjLCY5gvwAuys9B81ztpCiQnt4jCgmeZua7ZWVb+6pKkyeOTOetyLO4sDN5ZVefuayBVKtF54AI2/3o1EPa9Yk24EQUCrRQfRcsE+nAxLqHT5FzaeRLsfDmHxSe0rMM/9cgI/nipcfsDqfzFk3VHYzdimtr/4rDw0+moXbD7dCs8vd8H6ky3w+WYPknPyEZ6SjUZf7UbA9wcgUygxaO2/aPjlLjT+ejde3nwab/1xEYuOFyc4t+PTseVaBMZtOQsAOBz8GBO3n8PIn48jOq1wUJSt14qHtC+ZuLyz6xISs/Px/K8ncSQkDueiknE/MRPLz4RoPLb39l4use5FAMCSU/dRIFciJbcAR0Liyqxz43HVL1r8WCIh+nD/Va3X23glHMdCE3A4+DH2VfJg2vUXQgEUP7+qMu+ffITbCZkY9etJnIlIQk6BHO/sulRmuSm7C6eFp2YjKk19kJr3913B/cRMbLgUppa8DFzzL85FJWPDpTDEZUkwcbt2MVXkQqlWkhmHriNXKscbOy+Us4ZhlK5yv7/vCi5Gp2DxyfvIqQUXhr47fhdJOfm4m5CBE2HaV35/vVz4PMCZf17XaX+peQXIzJfhQnQyvvm38m68b/9xEfFZEjy74YRq2oozIQhPzUabRQdxLDQBB+7F4q+QxxVspfoNXPMvcqVyrDr3QG36S5tOIzQlGy9sNO6gR2vPP8C5qGTkFBRe8JEqlHr/Tk/dcxkRqTnov/ofQ4ZolpgcUbWxEFmia7ORCPLqrXG+YBadFcr3v46PsWx4CFp56PdsjWEtU9CvaTrGtdVcmX3Gv+pdI2YcUv+BOhWyHbklRgQzhlPhiQhNyUZmvgzfavEjR1WXL1eoWnne33tF4zKiEin+n/cLKwyly0dFDt6LxUcHrkEQBLXE4Kk1/6r+/urv28jMV68YJmbnY94/tzH3yE0oBQGPM/Pw5/1YnAhLRIZECul/XUy2XY/E9htRqvVSctVbBP4ukaDc1+E+lLwSSVO2FpXW0vGXJ1da9YFKMvXsslfy3JTuTmgMBRpaCEsqmZgChS1HReRmduXeWCJSi38HFLXgmPNLvKfVfTwly09VVcfnQxdKE3c3LP29Cqi/16QZk6Nq0K/FBNiK6qKbzyhTh2LezLzPcs8mmbC0AKb31K8ZvGk947eqrDmv/qyYyJRbRt9nyS//J6ViRIWk5fSlzy9VuWaxIENxELNiV5qZ/3RSNWNxqDomR9XAw7EpWtgORhOXVqYOpdp5OHobdHslR9QzJn65ENV82lQaeSN5WZWdElOes2Yu7LoLlNf12zCM1YtDVG6HdSLzYvqHqlCtNrTNW0jLeQwXh4aVLlvyC7KZezuNyzRx7YorkX8aJjgyOHPvGlkblTzn5jJSrinzDWOdA3M5JuZyZAzV8VB2/j4Yjy4XLPguVI4tR2RUFiILuDk2hoXIstJlS35x2lrX0biMUgBypaYvtp6OZfvx1ha6/oBVx48qqdPmCqyhKiIV/eZq+86ziJAmOQWyGtNyxyJMNQXLatWZvpZJVEQoeQXctB/vyvb+WV/thsktqWZUAYiI9KdtsvN3SBzcPv8dk3+/aOSIiIh0w+SIzFM5l5rzZHKEJGtuVapOdtbm81C3h8lZJt1/TbnyS4aj63te1SLCElb7DPvpGGQKJX4x8vOJiJ40/L6sOrNOjv7991/4+fmp/Zs2bZqpwyIjsRM7qv52snMDAKTmFgAie9X0WX/ewOabld+/pI92nsZNMozVFtZpiYHvwWKyQyVo6p437KfjTIpJI3azJaKazqwHZAgLC0P//v0xb9481TQbGxsTRkTG5OHkjVZefVAgz0OL+p0AAEEL9yNT4o1hLZMRlmaPu4kRAKzw7cmm+KxfZMUb1NF73WIwaW9QmenmXgc0xPNXqoKVIfNRXe/EPw/iEJuh/bNJqlpEWMKIaj5Td5cn0pZZJ0fh4eFo2bIl3N3dTR0KVZNOTYepvU7OKQBggX3B9dWmR6TbY9n5JihQWODT3lFq8/566IqnW6bqtX8fZwmiMuzUphmq7m/MHOuDfVfww8iORtwDkTpdHm5oqgsMZn5do8Z60s+ruV0wM/QocNV9fOZ2PonMPjnq0aOHqcOgaqRQKnHwXiwC6teFn0fdCpe9nehYZtqfD9yw5359pEmsMa5tgs77/7x/BOKyxXCsYQ8aXHEmBDtvRJk6DP7IGdDmq+E4HZ6IRSM7op696VrM5/1zR+d1atKQvdpGasojKnmBpjriqEnv35PK0C32bNN5crBLdOXMNjkSBAGRkZE4e/Ys1q1bB4VCgaFDh2LatGkQi8VabyMvT/uuH8YikUjU/qfybbgcgQ8O3gAAjAjQ/d6i/cEeAIDjEa7wc8tDJy/d7yNq6CgtM83JRqbzdqpbUk5+pcto83nIz1ffTmXrFBQULy+Xy83iM1cbvL79PACgQCbHulGd1Obly4qTd4VCiby8vDLfLwUFBRrfC13fn3n/3tY4PU8igVyhuUunXK6AQlEco1Sq29D3eXl5kMuLt63pWGQyzWVNJpWWWEYGpbI4Dkm+BHl56o8VUCoLz59MVvwZ17S/gvz8KpdtmUz9fGm7PalUVuJvqdbrVbZceb9JReekPPkS9XOhUBYPUJMnkSDPovwBa7QpkyVfS0u8L1KZFPbW5W7aqN89SrVjzCv1WgJroXq7NxcUFH+myvss6KKyz1uRiuozlcWgUGq+6KhQ6BZ/Rcvq8vmobuXFZYh4pfLi8iiU+PyW/F4rIsnPL/M9qCtdYjanOrAgCFpdWDDb5CguLg4SiQRisRg//vgjYmNj8c033yA/Px9z5szRahsymQzBwcFGjlR7UVFRpg7BLMkUAqwtRZArBXxwsPj9OhQcp/O2FEJxoTfkxZHBvvp10zM3J6/eRv06FdQuAKQUqB9rZZ+hRwk5qr8zMtLN6jNXG5x4GFfmnEoVxT+EBVKpxnMe8ygGwfL0MtMN9f6EhYUhMyNT47yMjAy1GGNjYyvfYIkPbHBwMNLS01SvHz16hGCF+rGkp6VqPJaS+0pNTUVOiYsG4WHhKHBUv7iWL8kv3F9amto2gkXqF1aio6Phmp9S+XFUIC1Nt89WkeTk4v0mxMcjOLjyCyG6bL80iURS4brhEeFAmm3x8nnFlZ6HDx8ixbb8qoWm7ZaeVvJ1YmLxOUtISIBz4/LjNuZ3T8mK3cOHD5EvKX4PHj54AAdx1SqbuoqNzVb9nVbOZ0EXJT9vMTFlP2+laarPVBZDbm6uxulZWVk6xV/RsnGP4xAs1ryfkpSCgJvJeWjhbAvHanrvyovbEOVWpij+/iz5m5CSWrbuEhYWBomDdo0M5dEnZnOpA2vTwGK2yZGXlxcuXbqEunXrQiQSISAgAEqlEp988glmzZoFS8vKC7O1tTV8fX2rIdqKSSQSREVFwcfHB3Z2dpWv8AT5MzgOE3dfwrSeLfW+t2fzDU+MbxePXffU70uKzbJFZ5h2mGtz89aJWDz4ZFiFy3RfuRPTS/RmDQgIqHD5BHEigEcAgHr16lW6PGnrPgDARmxd5pwWyBUAQv6bL0ZAQEBh5e1KvGqZxk0aI6BFA7VtAZrez/vQh6+vL5xj5QAyysxzdnb+L8bCz1+jRo0AVJIglfgCCAgIgEt4AYDCClrjJo0R0NJTLdZ6Lq4ljqV4esl9ubi6wkGWAaCwstTctzmauTioLW9rZ4uAgADUi5YBSFVtIyDAS205b29vBDSt2v2vrtEyAMWVUG0/K+5JIQCSAACenp4ICGhawdIVvdfqCiv8Zd9/Ozv7CstJs2bNEFC/uNuz3ZkEILUweWjZogXcHWzLrFM2ptJxao67fnoogEQAQIMGDVARY3732J2KB1CYELVs2RK2F5KA9P9e+/mhrm3FF50MLRxxAGIAAC5qnwX9lPy8NWpc9Hkrq2x9RvvyVudiMoCyLQ6Ojk5axK9pP2XLV0OvhggIaFLJtoAlpx9g7tFo+NSzx50Pn650ef2Vd360P2/aKGw5KkxYxP/9JgCAW6wCgPpFnRa+vvCup88jUfSL2ZzqwGFh2j06wGyTI6DwB7ak5s2bo6CgAJmZmXBxcal0fZFIBHt7+0qXqy52dnZmFY85GLPtAgBg4akQvbdxKsoFF2KcIVWoj0x/JNQVzwcmVSm+2iYuS1KmDIYmZ2HTlXBM6uqLpq6OyMxXb4avrMza2BRfQba0tGIZN4CSfcJFIosy59SiRLc6C4vi+SWvL9jY2Gh8Lwz1/tjZ2cHKSvNFKktLS1iViKa8UUbLa921t7eHlVXxz5ONjW2ZuK2tNZc1sU3xVUFrKytYWBTHaGdb9ju46PxZWxVXbjWdOxvbsjHoquQxAdq/F2JxcWzWYrHW6+kbr4VF8W+nXKHEz5fUKxS2pc6FpUXxd2/h71z5FSBtymTJ12Jr6xJ/V3zF15jfPRYljtHezr7UazvY21XtSryuSn6myvss6KKyz1tpmuozla1zNVZza5SllaVO8Ve0rFjLz8fcf+8CAKLS86rtN6u8/Rhi/1by4t+Ekr8Z1tZlk/bSn1996LO+OdSBtb1Xz2yfc3TmzBl07dpVrSk7ODgYzs7OWiVG9GQpnRgBgFxpmOItCIBgxreretQpwGvtH6O5i379ltsvPoQFx+6i54ojOB4aX/kKZHS14X5ZQ970q++nT5eb1jkivWbrL4Riyu5LatMqOq+GHiigFnwUahxjfRTy5ZrvOeIQ31Vn7M/JkzaIg9kmR+3bt4eNjQ3mzJmDiIgInDp1CgsXLsQbb7xh6tCIzMonvaPQxycDs/vq99wnyX+tEInZ+Ri09miZn6ngxExI5QrcfJyGw8GPy3xJ8met+pVX/yz5zpjyt0zb+rGxE5In7QfdGP64FVVmmrbn1csxHyP8kuEg1m+wgjvx6fj4wDW91jW0Jyl5ru5PDUdHNCxjlNXlZ/Tv3VMTmW23OgcHB2zYsAHz58/HCy+8gDp16mDMmDFMjqjamfuPooudcUdJarXwADo0csH12MJ7JQ5M6o/hgY00Lqvrj1xKTj723o3Bs0GN4OHI+/GK1ITKQkUVZEGoOQ8Hrgn5U3VfWTfUOfl6YDgAwN8tFz+c89F5/Vd+O2OYQGo5Q7xf1fFxrRnfCKTJh/uvmjqEamW2yREAtGjRAr/++qupwyB64hUlRgCw6Uo4unm7Y+eNKIwM0pwkVaTkUJrDfz6OqzGpWHXWGTc/HmmweGu6kpWdiiot9tYKWFtW7xDC2jJmq02QRw7cbEOgULaDpUX1/IyZSyuUucShiwCPykcP0yQx2/RD/5orJhpUnsq+ImreN0j1M9tudURkngQAozeexHt7L6PrssNq8yq7wh2bkQufeXsw6teTAICrMYWjg92JzzBCpDVXyR+v8s6pvbUCi4Y8xOSO1yCV5/+3bPWqrhaNkufDUqTEhz2j0cD+Lu4+Pm2ymPRhzrGVVDIh17UFUN/krZ6dDB0bZsJSVP4zkqhmqymtyURm3XJEtZNMocTZyCR8f+yu0feVLrFCPQN0O9Pl995SJKCXdzpiMm0RkV47R247HVE4CmBitnbPWyny0YFriM3MK/yXod/VZCrU2ycdttaFFcnI5FtoXLe1iSMqnyGrRGLL4g/jo5R7aNt4QLnL1sYrpOZQwTTGef1ucCisLAT8+cBNNc2ckska2GCnt+punXySzq05MJ9PlflickTVzvbTraYOQSeWFiK09nRG6WcFlGeQbypebFX4XI439gZW60h33w4MNfo+Sv+QlRyBqLJ7ZdJyi5/q/vlfNw0ZlkncikvD7D9v4J2efhhRzn1Y+qiJXadKEqBeiS/vaKp6mDXh3qySDBFvdZQNUxQ/K4vCnQ73q9qDdo3lSkzxwzRr+ueTnmwsvZVjtzqqNuEp2UjLK6h8QTPT2rOexgEIenmnY1afCHg55WPWU61U03t6Fz/LwdKier+GGjhKq3V/ALDq3APV36fCEitctuRF781XI4wVUrXpvuwvHAmJw7MbThh0u2rd6jTk1qXrZuaYJBirAqnLVnmF1DgqOq/m0LJFVWOs97C8rbLIkLlhckTV4npsKlou2Af3z3+vlv1lzh+DWU+1MsyXbjm1sdc7xMHXVYKvnwrH10Pb4s4ntWNAgcrOWen58ZnFN01nFcigr7jMPMgU+t9vsOj4PbSYvxeXH1XflecCuQnvjzC/fEildBGqaqVI34+xGZ+iGo3ntWYpkOXhXOguhCfd0Gp5dqur2czxYllNw+SIqsXcI7eqbV+vtPeBg401vhnWHp5Oxff85Mv1K+4NHR7jTuzJCpfZfH42QuM2oqVbHbXpowKTsGDQQ3g55ZvN11XJEaCCEzN1Xr/kD5mTjRyudtkltp2P1Fz9Wgcbf70bT63+R691AWDmn9cRkZqD/qv034a5MGZl4c/7sbgQlVzl7VQUorbd6qrOXD5VxsUr61SRyirDlyIOIjTxKs483FnuMtXR4sdyTDUFkyOqFoaojGkS9+VoHH1nEKLmjMJXQ9tiUEtPLH2us2p+y/ol/vZ6T6991LXVLoFIz0vAjvEN4e3soJo2pEUqPBxkeK/bI732bQyNvtqNYw/jAQCTf7+g93YsRQKWDnuA8W1vIdA9RzV92t7Lem/znAHKSXlPYa9JSlZ2dKlPlGzHyszX3MXymQ0n0GvFEcRn5ekXXC2j7VXy2pSGCYISR0Ie4+fL4UbYtuHOFCvT5dPl3MRnhBkvEKJaiAMykFF9f+wuZh/WrilfH/Ud7VD/v4eHzhnUpsz8No37w07sAFeHRnB39MTGim+JqTKxpQB7sSUySz2ew9VehrhsG+PuXEtKQcCzv5xAzndjkVOg/0h+jjbF6470T8b95MKk8ERYAgDgbnw63v7jEl7r0hxvdmsBQLurkyWfg2Qo4SnZUAgCWro7GXS71e3Tg9eQkC3BqlFdNc6/GF+cpC49FYwx7ZuWu63bcRlqLavGVNV38258Brr7uJeZrqkabs4VanMYfe3ovY1Izn6M9w55Il1ibepwqDqYvtgR1ShMjsjgotNyMOLn4whs4Ixdt6JNGoulhRX8PbubNAYAsBABbRvkVL5gNZHIFCiQK4xSkUzMzsfT64/hxuNUJOcU4EJ0cnFypMX6Hl/8jk1je2FYgBcA/ZKlc5FJaObqAE8ne8Rl5qHlgn0AgLDZz6Gpq6NO2zIFTRffr8akYvHJ+wCANp71NCYGMkXx1FxpxYmvIfqla33PUBUL2sw/r+P747oP/c97GdTlFmQiNj0EAPBKG2D1pSaVrqPpneN5rd2q++1lcao6fiYNi93qyODe+uMi7idmGj0xWjiig1G3X9vZz9iGW3HplS9YijaV6n8exCElJx/NXfJga6VAdFphYqhNJTktT4qRPx/HyJ+PY/LvF+D11S4cD43XKcY+K/9Go692QxAEHLwfq5r+h4mTdW1p6ppU8l6xsJTsMvMB9UqGKYcbLr3v8mLRJcR0iXFHYjTnyoWhWpwEobjjpb01H7ZKxTSVsIP3YvDKb6cRkar5+4bMkzm0UNd0TI7I4B6lG/7hnh/0CUDIzGfVpr3fJ8Dg+6mqnIJ0ZEr0v2/GQlS9NTQHsRyTO8Wgj0+awbc9pEUqZveNxOy+kXhp82kAunV5Ohz8GBsuhSExOx+D1h41eHzmTH0ob/7Q1Rb6ttaVXK9q3xDalSVzyRNNXfSVSgWuRR3BwwT976M0B9pUljW958/9chK/34zGMwZ6VIEhK+1VufiTmZcMS1HtvThQ2feMOV8IMhdMjsjgDP25m9y9BX54piPq2hb3j5/1VCtYWZpf8b33+HSV1h/awrjDUIsgoItXJrydJRBbKjG9RzS6Ns7Ca+3jYW2h/49FS7eyN/cXPQjXy6kAV0s8QNGUYjPyMHH7ObSYvxc/X6zaA3MvRuufBCdkSfDzxdByn/t14F5smWmV/NzpHYu+atsPrKkr4tVNZKQyU5uS+ZD4C7gTexLnw/bA2Vb/xxTUdPqMamquQhOvYu/1xZjew3wGSSLzw3uOyOBCU7KqtP6jL17AX8GP8dYfFwEAPvUcIBKJ4OFohx+e6Yg78RmYPbC1Xttu5dUXdx+fqlJ8xvRCUJJRt79wyEO42Gu+F+Wrp8Lx2b++lV7b23cnRuP0OtZy5Mqq7yvl18th2H83Biue74LG9epoXKZ0Bb7kA2vf+uMi3vjvXih9XI7WP5HtvfIIIlJzsO16JI6/O7jM/Albz1a4fnlXBpMl2g+wYQ7JTS2qRxsVu8mYRmJWcTdcJxs5MvJr7wAWT0oJOxe6CwAQ4GH4Hi5UezA5IoOJSM3GsxtOVLnS5V7HBu29XFSvS45SNb1vYJW2bWnxZBf58hIjAKjvIEWjuvl6b9vGSolcWdG2NLeI3E/I0Hv7x0Pjsfb8Q3w2qDXaNnTBGzsLhyFPyy3AMQ0JBgDIlEq8u+tSudtMyJKggZOd3jHpKyK18B6sU+GVD58oKvV/eUp3M6lNV/BLEoTyXlBl9CkSuq5jynvdDK2mfIR0OeXGGA1UWzXlfBI92TVFMpjcAhlazN9XpW2cmjIEjZztIbayRMfGrtj4Sk/IlUr0aV7fMEFSpQRBBBjgvidri7LbOBGWgJgM/Z+tU3Tf0b67MZAuGq+afi4qGY2/3qVxHadZ2yvc5vv7rmDnq330jkmTZaeDIVMoEZKUiasxqTgyeaDBE7CfL4ZhsK9BN1ljCAa+xq1taTdEpZ8tQDVXyYp9daV/SmXhniwsypYb/RMcAdXVTnQrLg2fHLiGyd1bYnRb72rZJ5EhMDkigwhOqlpXug/6BKBXMw+1aRM6NavSNjWxEJnffUo1h4BGTgVIyhVDqtD9PA5c869BolAoBbWR2wAgOUdzS5VcWXE1RptRmEISM3E+KhljOzSFrbUllIKy3HJ0LjIJH+6/qjbt4wNXsWV873K3fysuDW0bupQ7Xxua6u014Qq+PiGWvFcmT1r2e6dktU+AALmy9t54XRmFUglLi5JltfjsVOUqviGGgdeG6ZNJ0+0/X6ZAxyWHIBKJcHX6cNhaW1bbvg313dFn5d/IKZDjWGgCFIsnGH1/VMjQp/N8pHG7+5sj1hTJILILdLtZdVzHpng46zm83aMlfnyuE+YPb2+kyNT5NzT9M4/MmbWlEqJSLUdvdIxFA4cCDG2Riq+eCsfcAeEa121UtzBBcRDL0b9ZxaPfOdnI0MipuAufCALq/XfDs4VIgFUlg0M0/FJzS5Gu4jIllf4wBy08gDd/v4DP/7qJLEkqdlz6Fr+eW4O0vLJDS9/V0G0wPktSZlpJHRb/iZD/bnjOzpfhxU3q98SVV4ktHbYp6hflxlbq9QsbjXOfX4E8DzJF+UN8S2QKnAgz8pOfq4G+icwvlzV/VqligiAg08hDx1dm45VwhCRlITgxE79di6ji1ooLUHlfE8bo8lb6IePsVlfz5Enl6L3yb1OHUe3YckQGoWurQB2xFZq7OWLVC12NFJFmNlb21bq/mmZOv0hEptuqTeveJBPdmxSPVtTAQYoNz99DdoH6lcz3uz/CpL1BWDb8AUqb3TcCi874oGvjTDSum4+BzYuTp9NRzujWOBNiSwEJ2WI0cCyslByPqIeDIe7IKii8CdrFToqMfGsohcJfWLGlEg5iOdIkYr2PNyFbgs//uokP+rjj9uNoPEh2xfhOTXAz+iDq1fFEiwadMMIvGXcSHbDk1H0MbHoDUnkuRMjFohN2ANTPgabffm0qBHtvX0cL58uISG+IPbf1PpxS+61aTcTeWoGPe0UhIVuM9Vcbl5lfXa0HlUnPTSh33tZrkdUYSeX0Hspb0Px3ZU6GJagewKzb/szjvTWVsVvOYM/tR9j8UnGCVN31+nyZXOPfVVf1Z46Zg5oWrznR5XvI2M+XM1dMjqjKtup4Vcutjg2+GNzGSNFQVTWtp92gDI42ijLTSrYGldTcRYIp3WLQun5OmXl9fDJUfxclRgAwoFk6vJ3zMf9UM3RsmIV3u8bgZrwDVlz0hggCvn4qDO51ZFh/xQtDW6TiWpwTDj0oHLzDzV6Kad0f4W6iA46Gu2By51jcSXDEnw/dS+8eP5y4DV+nYADAtmteyMt3hKttCJB8AzFp9/F8YBKeD0zCpL1BkMqLW4E03AagNw/bE8gpkMLDPh1AULnLWVkoMa5tPBKybSr9eausglvZ+s/6J8HbOR/ezvk4+CAf8dm2laxhXlzspLAUKZGtRQeJxSfuYfHJ+9UQlfnQtvjq2rWtKCk3l+H79fH7zcJR6k6FJ6JLI9PEUBsHVCmvLNXGY62dBHg6FiApR/8LkjUFkyPSS06BDP8+jEcPH3e8uu2c1uvd+WQkmrs5wsaq+vpPU/Vp7lL+gAuaEqPKt1eYjLzbtXD48HaehdtwtFHAvU5hN7zJnR8DAJo45+NanBPis8V4vcNjeDkVwMupAE2c89HCVYIWrpIyydGwlsnwqVec8PTyTkdydiZc/8sDErOi1APSZ7SvSlZ6pU08LEXlX50rWXFY+vQD2IsLuxzeSywevjw2PRiNnLS/4GApUkKhyATgVe4yTrbFV6vFlrpdptW64m2Q0dPKxuZqJ8XCoYXPsZp6yB8SWfnfN7fi0nErLl31unvjDEgk5yFXepZZNi4jDJfC96NN4/5o7tFB9+ArsO16JCJTszHzqVal7hPSvzuSKa6uFyXlXX88jA3PV21bpq4zm6IFLTknH48z1b9HKwuj9PKllTyPh+/HYlhAkzKDPBj6XOty7p70lsqaon/TdIxvF487iQ6mDsXomByRXsZuOYM/7z/WaZ3M+WPgYGP650S4OTRCSk7Zh2xS1VnrWInWV3njLHwzMAwAUCAv/qX3cdZ8z4+XY77G50opKhzEoeIaRGVXQJWCEvcfn8XKEcH4J8wVR0Ld1LoYVqYoMQIKk8Eij9MfAhoGg8rIS0RqThwsRQIUQnFsH/eKRmpGMB6lvqoePwQ83TIFaZLKP6cV1WcKx8NSwEIkqLpB6ksEAa3q5yAhxwbJudpdsezbtDjZCfLIwdXHdQEAKbn5aO7mWO56dtYKvNHpMaTSx7gXq55I58uV+OfuzwCAMw9/1zk5EgQBD5PLDiDxOP0hzobuxZoL9jj/yBkudWzwTg+/crejqYhJ5QqcCk9EN2/1mAUIEAQBcqUAax0emm2uV/LDUrPQxlW/dVNzC7D3ziOMDGqE+o6GGT2yjrUcr7aPR1jiBXT06Vvl7SmUSnh9tQsKpYCh/g21Xm/37UeYf/SOxuf/PUjKhExR/GEd9esprH2xB/7X1XjDXSZmS9Bz+RGtl3cUZ+OPK9+hqVsbdGo6zCgx5Unl+OVSGBxMXLSj03Kw4myIwberS1e5x5kSpOYWoEsTt0o/6yXnjm8XD0C/C501DQdkIL3omhgBMIvECACGtJ6MYW3eMXUYtZKFAYYB10bbBhWPMmdjVRyHnXVxQtGhYRYcxYUtIs52mvvxa3sEIq2WFOBkkwOlUNgF8WHCZVyNOgw7ayWeDUiG2LLykdTK++kq3a1RrlTCykKJpvXyVLHtu74UZx7uxJAWhQ+srSOWw9clDy3dCq80Hw/ejCCPHNX71tM7Ay8EJeHNToUtb9oSWyrhYlfc+mUpkqGn13F8MzBMbXCNNg2y4eem+eGLtlaa34+e3hn4oMcjfDc4FPoMomwBwMay8FwdCYlDbEb5D3+0syo+p/GZ6oMZvP3fQ6n1teTkfY3fm//e+wUSaTomdSycd7KSASQ0JaUf7r+KoeuPYfA69Xs/f78ZhSHrjsLry12ITsvRuoucrlfyJTL1sngirOx9YB0bZqGXd7pW2/47JA7P/3ICidnq3XSXndauUhmTFoydl7/Fg4TiZ5w9u+EE3vrjIgas/qfCdc+VMzJX6YFqAOCl1ono5JWFO7F/QaHU/76gsJQspOTkI0MiU12cORISp9M2Pv/rZplpjzPzEPj9gVKDwghYfPKe3rFqY+ah64hM074C3aLeZeQWZODu49MGi0EQlGrvyZy/buD9fVcMtn19RKZmo9m3e7H0VLBO6yVlS7Dm3AMkVDK4j7YGrP4HPZYfwT8P4svMy8qXYtetaKTk5GPj5XDcjk/XsAXtbLgUisFr/0V4as1LpthyREb143Od8NGBaxgeUH73nepmbSmGh5M3bKzqoEDOp2Qb0sutDT8y2Ibn71X4WhdT/uueN2lvEOpYl71nykIkoFV9zV1UOntl4lpMCnzqFb72dZXgYao9JnV4jPYNs3HkznqIhFYQQUBQ/Rw0dCyApUjAyJb3cfqBBJ18nsbF8H1q29Q0eEVL11ykSayRkmeNRk4FeK1dBB6n+0Mur7jrzP7b32Lds4V/J+da49cbnVTzBvumQgRglIaWsg97RmP3PQ8cfugO/xKJS8nk6Iv+EVh7uRGu/NcCAxS3YIggYM0zhT/235xsish0e3g5RMHGqgD1HYAOntm4/NgJzV0keL/7IwDAT1e9EJpqr1bRb+EqwTP+STgQoj6k/9MtU1R/O4gVZc6ZAAGB7jl4ISgRBx+4l7kS+laXWBTIRZh/qhnisyTwnrenvFOo9gyl0pV4TV2XTocnIiwlGxM7N9f4LJrPDt9AYrYEa0d3w6eHrpe7X11M2X0JlhYitYEW1px/CAC4/Kj4Pp++Pml4JiAZm2/kIDXPEe/vu4Jt4zoaJIbS+U3Jbome5YwkWdQ19nH6AzRy8a9w+8N+Olal+I7d3wQAuBC2F34NCgf9uRCdDAAIqeCxEyk5+ehTYmSuyp6p1bREl1xBEJCVL4WjjbVOLW83H6eh45I/YWUhwqUPNLeaaBrxUps9HA4um4yLRIBUUfFFmYry18MhcUhLzEVAQPnLpOaVvbCiVArldt1ztdNttNvKCIISh26tRk5+Op7t8D7sxU5Yc+5hucuHp2RX2KIMFLbOinW4HSAmPRe/34xSm+ar57Mgh64/hltx6VhxJgT3Zz6r1zY0WXTiLoaUaqF8cdNpHH1YNmnSx+TfCy8ojdmai82Dyg7qY86YHJHOUnO1u6Lc3ssFU3r6Y1zHZnC2Nb8b+Ea0exe7ry4ydRhkAuUlWC1cy78y93YX9a6Y0/6r6BdJyIyAFSLws4b7LKJSbiMqRbth6Gb0iQIAxGWL0fC/ASr+vfeLVusWca8jw6e9LqheO9ooNCZGRV4ISsLhh+4Vtsu83SUWV/bWVZsmgkht2PYZvaPw9oFAtVa1ZwOT8ErbeLUucW92Kqy0zT3WXG17zwYk41RkPcgVJSpLJYJ6ubXmkek+6lV4A/173WJKrwKgsCVxUsfH+P5MHZTm75aDT3pH4+9QV/wbVtxnKzgxE7ZWNmjuIsGDFHvIleodLY6EPMbwn46rXv+vqy/ORiRh89VwzBjQCukSKb47dhcA0LGx5r5g+aVaXNo2yEJ9e/VjVCoFzCiVWL39x0WsOhuC17v44tWO9fBa+8c4HuGCx1m2qm6Mr7YvrOBM+28UyVwNwyq3cM1FC9c83EpwhFRhgeRcMWytFLCxlCE86QYauZTfvU9f8RlhlSZH+hGQlPUYbg4NKl3y4L0YRKXlYEpPf8w4dB3xWXn4ZUwPhKZU/tyzIgVy9ffufFQShqw7iRFBjfDHa+V3sRMEJUQlnpM2/+gdAIXPZOu45E+N6yw8cQ8LRuh/j1vJZKeTVxYuxlioKvtnIhLxxo6TsLVKhaWoDhRCYWwfaGhlORORiJe3Fn6v9GkXAH979dFfBUGASCRCHWsJRBDUksu3d5Xf8mqI/gYiCPBwkGL7jUgMaSlG6n9d529E/4OeLUZXuO7T64/h4eznyp0/98hNfPPvHex5vR+ebaVdJb/3yiNaP/R85M/HseHl7vAop7tn0cWHB8lZSMyWYMOlMIxq3QT+9etqXF5fkcm3kCMJBuBcZl6zevo/wD0kWfvPlblgckQ68/ji90qXyV7wCmysLGBhIYKLvU01RKU7R1s9O68TVYOGjtU/hKq2lZRODTNxN2Yffr8hx6ig4q5PRfecOdkUJ0wNHKT/TSubeHb2yiwzbcmwh9hy4XNY4JkyMfVoUnb5xEz14bolkksau12KrZRlun0GuOfg4/8SqyEtUnExpriykZKbj/e7P0JLtzycjKyHLTfVB2h4fdtf8KhTmIScehCH/3Wdgb6r/gYg4GxELL4f2U217EONrRUC2i8+hBm9iqdM6x4DIAZ/3GiKQf5tEZuRi7Y/HNKwLnAnPgMf7r+KepYh6OOjQB+fDEhkFvjroRsSyhtNqtSV+5n/JeJF9979dtMTY1onwNpSwJmHF/9bYUDZyAUl4jJCUc++AezExVfcPR0L0KNJBk5F1kNKnuYY4rKKny1mbaFEoEcupHLtRsisyJAWqTh8ewW86gVWuuxzv5wEAISn5mDFmcKuep0au6KurYAhvim4m+SAx1nqozOWPHXfHbuL+wkZ8OpefKHw5c2nIVUocf3RA1yLEtCuSS9YiCyRU5AGKwsb2IkdcDf2NG7FHEfvli+hiWthnKVbmawslOjeOBMR6XZlYtBWboEMdcrpxv5mp8e4GOOMv4IfoL+vB/qt+gfzngpDQ6cC/B3qit/vFiaXReelyCcHzkIocSfGjccZ8G+ofo+b1cdb8GlfKwxpdgtOVnXx87Xiof42XAorN97yWqpmHrqO0+GJ2DWxb5lkq7Tx7eLRr2k6dt/LgoDiVtWKnoP2Sut45MktsT+48D6pkveiJWRJsOjEPQz1b4hv/i1MYEf9ehLyH8bhZMhWSOX5GBj0Oiwt1KvRKTn5uJ+YqXViBBS28E3dcxm/V5BUFxn160lcjE7B53/drPDBukX+flB+98xLj1JwKjwR++48wjvd3XA+dDve7ASk5FkjLFX9QlLRRbsiS07eh6OtNTo3dkWGRIq+zeub7b2K+mByRForkCtgP2Nbpcvtfb0f7MUsWkQ1idhSCVQyeIKrnRTD/VLQt2k6MnKBiR0ckC1V72rSsWEmXO2StdrnCP+Ucue18zig1Whn16PVH1CYX3AVjTVcUG3gIMWKEYWVnJBke9haKeFTatj6uQOKH0tQdF8WAPRrmo5+TdX73i8YXFzZC3DPxcazM9XizcnNwUutUpCRb4W9t2/gk14J8HfPwz9hrhjsW/Ew17m5u/HqbxcRnW6Jro0U6ONTuO/7SQ4Y4Z+MmExbyBQiuNnLVPezAYX312lqIdzw/D3ciM+HQlnc+tDCtWzlbUK70t1pBNSvk4Y61nLYWSuRkW+FII9c3IpaCqB4v/MHiTH3WHPVgChPNUvFrnv1NR7bHzcjIVFew5x+4arHBmy7+KXaMvMGhuJWvCNuJzgiOU8MhVKEXKmFxm5ZRa2UL7Uq7NL7OF19SPa5f11G5ybusLFU4O0usYjPtsGue/XRuG4+zoXfgtjSGnKlCHmSS3C2fICXWgMvIRFv7Q9ApxLJezMXCaZ0jcGFmLq4EJGB4X7qXcEaOWXBUQzM7huJO7ERuBOrPiCBSCSGIPz3DLfgzejh+wISMsPhKHZQVfxd7aSY3DkWvv+1YL+5LxBKAbC2ELD69B7sv5ePr57uDmd7Z+y5HQ0nGzlc7aVwspEjONkBgqDEb1dOYdnpYAwP6oHgpAyIIMDRRr3V0FEsR3rmZuy5BgxqXh8N/+tCO6RFKo6EumHJicN4PjAbfz5wh4+zBNlSK/g5H0K+3BK2Vi2QLy/8zCsFBSxFSigFEfzdc/E4yxZ+LoWt8t2bZCJNYo39we7/DQZT3neLoPZYhNDEq2jiEojdtxOw9NQdBLrnYu2ZCKwc+RibbjTEzXhHDP/pKIIaOKslTEWfzxeCktTurQtJiENTjxyNF34G+qapzseLG4/h1NThUCjlsLCwwCf79uDwgwKsPX8XzrZKZORbAxCw4vRxOFkWHuNv5+cgPd8Z/fwnoIkzEJ0Wi1EbbyE+2wZdvLKRWWCFXKklhrVMxp8P3ZEhsUKuzApFQ9ZYipSqc/MoLQJnHiSgft2m8K3fCffjbsLdsQkuRKt/Ti9Gp8DOSgGJ3AKhyZm4E5+BZ4IaQxAE1QA40enZ8P56F3ZM6IobsakY0iIV/m652HDNCzlSS1hZCIVlXqrAgNX/oLNXJgLrxcP6v6/yFq55iEq3w3MBSUjKFeN0lAusLNTP4Lx/LqGPTzp+PGGH6Ew7bJ/QHyMCm9SaBEkk1NIxFO/cKfwRbN267Ogt1S0vLw/BwcEICAiAvX3NfAipUinA+pMtFS5zeuoQ9GzqUeEy5mbj2ZmmDoGIiIio1nKxbIaBbcebvA6sbW7Ay/uklcoSo7ufPoMAA/d/rQ621g7Il9W8kVSIiIiIaoI0RUTlC5kRDuVNlXqqgqFPX+3UDNkLXqmRiREAjOr4salDICIiIqrQXw9dIa/86Q9myV7eqfKFzAhbjqhceVI5HGdtL3f+9yM64OP+QdUYkeGJrfS74ZWI9PNvmAvqiBWwsVIiQ2KNq4+dEJdtg3y5BQRBBIUgQnNXR4SnZkGbAYsN8aDZIjaWCkgVFuXe+K1YPAGWH/2mNs3aQgmlANUoX0V+fK4TPth31SBxGUrS1y8hLa8AQ9YdRXS6do8xsBCJoKydve/haGON7ALDDiNd0t1Pn0GrhQc0zjsy+SkM8mtYpjxVlz9e64tRbZqYbP+6cLW30Tg8eHkmdfWtcACI8pQc4KCi81K0nDHOnWLxBLy06RR2336kcf6ue2VHY/RwsEXEnOfhMLP8+po2rkwfhs5LD1dpG+VZN7Bm3XLB5IjKKHruQkX6NPPAh30rHxWIiGqfhGwxGvw3mt7cY80Rm2UDH+d8xGbZlBnyWh95Mjm0e5ILDJYYAUCBQvvnmBSRlXO85pYYAcDrO84hLDlb68QIKHyOVG1lzMQIAF749aRRt18Vj9JzMH7LGVOHoRVdEiOg4pHxagJdBzUw1Gf0+f9GcTSGmnZ9hckRASgcxrJhOQ/vK+nNbi2w6oUusLRgj0xzk5gjRka+FZxt5Tgb7Yx7SQ5wt5eiUd18HA13hbWFgO5NMmBtKeDvUFd0bJiF/3Ws/Cnsu+954IWgJCTmiHE6qh7srBQY0iIV+XILXIipi3uJDmjfMFttNK8/7tZH2wbZaiN+kXYK5CJkF1hBLohUw2ADhQ92da9TXJm7n1QHgR65uBhTF/5uudh5twHe6hyL84/q4kxUPXRtnAmpQoTTUS5wspHD21mCpFwxPB0LcO2xE5JzxfhuRAfMPXIV+XILaJuMlCcqQ/MzOvSh6aGX5qAmXGWvyJ/3yz4UtDI1rVJjTh4kl//A2aHrq/ag26r66MA1k+7fHPVafkT1sOCKbLgUisuPyh9psyr0+Y5JzinAIT0+26XFanjQtaHUtN6AHK2uGlTHaHVyhRIKQYCFSIQCuQJypYD4LAnElhaQK5XYeDkcttaWeJyZp/dVlbDZz6Gpa8VPka6JauKIdTfjHeFeR4qj4S6ISrfDo8yqV0ytLZTlXgU3vMKhTDWxFAn/DW+qOxEEWFoI5bRelL9P9zpSpOVZ671fIiIi0izAxRaXpw/naHVUbMv1KBy5HQf7ezkQLCwgVwpQKAUolErIlEJhYqMUIFcqIVMU/i9XvS7+W64QIFcK/00rni5VKI12da+enRi/v9YHPXw8YGute5eTmsBeXBd50rIPlzRHXi7dcTTUHisuJhh829WXGAEVtVJUJUERIIJcWd765W83Obech2YSERFRlfg42Zg6BJ0wOTKyx5l5eGdvUfN1hilD0dpb3Vti2fOdYW35ZHSdG9FuKo7f34SUnFhTh6LyYvvPcSf4Bhw8XHEtUYyRgZ5wqVN4xWVQIPDxU/loMPcPE0dJVLs0qmuP2Mw8DPZriH/KebL8tN7+8HFxwIf7r8LP3alM16nWns7o1NgV/Xwb4LVt5wAAH/YNRK9mHkjLK0BitgRWFhZIyJZg6algAEC/5vVxMjxRbTt9mnngdIT6A103j+2J1p71cDo8Ee/vuwIAuP3JSFyPTcPE7YX7GuzXEJO6+sLJ1hodG7nifmIG+q1SH3H0gz4BEFtaYOGJe2rTZw9shXyZEmEpWThwLxZrX+wGWytLWFmI8OPpYFyNScWC4e0hCMC3R++gsbM9QpLKdh17sa035j3dDv7f7S8zz8PBFr2beUAiU+BwcGFXoHYN6yHI0xmXo1MQmpKN2QNbYf7RuwCAY+8MwlNr/lXbxsigRjh4T/37uoePO67HpiFfXvhw2oEtPXH0YTxmDAjCq52a42psKuysLeFib4PFJ+/jr+DibkidG7tCJAJux2UgX67AtvG9Mfa/+3HmDm6DuCwJejR1R1MXB5wOT0SXJm44H5WMHTei8DA5Cz193HErPh1bxvVCaq4Uk3aeV4tt/UvdsPD4PYSlZKNtw3r4sF8gXtt2Dh/2DcTW6xFo1cAZkWk5WP1CV7y35zJCU7Lham+Dt3q0QDNXR7Rwc0JGvhR7bj+Ch4MtGjvbY9reK6rt+3s4IaC+MyJTs/FCW2+sO/8Qnw1qjai0HHx/vPA9Xv58Z3z+101k5svQ2NkeQ/wb4sW2PvjzfiyWnwlB72Ye2P+//vgz+DFux6XD08kOnx68hlc7NUdgg7ookCvwaqfmWHz8DurKc5Bl7YBrjzOw7qXuGLP5NG7FpePFtt4Y36kZjofG46kWnph75BZuPE7DlJ5+SMzJR6ZECmc7MXzdHHE+KhmnSpT5C+8/DZlCiYRsCeqIrdC5sRuuxKRg+E/HAQCB9esiqIEzXuvcHFYWIvx2LQLv9PBDhkQKiUyBFzedwtgOTfFW95a4/CgFa84/wDdPt0daXgGOhyXg9S6+EFtaYMi6o6p9NnN1wId9AzF1z2XVtNe7NEfnJm4Y274pXth4EonZ+XixnTd83RxxISoZfwXH4Y+JfQAAidn5+PLILfRq5oHFJ+9jVJsmeLGtN0QiEdLyCvDurkt4t6cfFEoB/Vs0gNjSAnFZeVAoBLg72GLvnUf441Y0XmrnjR+e6YR5/9zGg6RMLHqmE648SoGNlSWi03OQmluAMxFJmDO4DfJlCrRp6AwnG2scuBeLf//f3r1HRVnnfwB/P8MAgwKCggRaul5AxGEG8UIrZJKUoqxarb/W+Al53Dgdj+ZWbmnHUyyd3ZJSVm0PrKTUqd1WTV2zk2ZXMy+5muMVuRiEXAevxGW4zOf3hz+fmAB1FoFher/O4TDz/T7PPN9n+JwPfJjv93nyyvG/kcPQz8MVF2stMA7qj75uWvzPO/uwbEoYKmvq8V3pJbhrXZB5IA8A8O3SeFTU1KOfzg06VxfsOn0Bk37ljwtX6/BlQSWmhwZhi6kYBeYaPH1fKH64XAsvdy0OFlfDx8MVsSMDsdVUjCkjAjDpVwPxo6UZmQfyMOGeAbjH1xPbThRjhJ83RgV4I+Ors4gPuQsPDuhdE+s4ra6LiQgWvr8f+/LL4dXHAzpXLVw0Clw0CrQaBS4aDVxdNNBqFGg1N74r/9+mgdblerurS+t+TZttTpRdRl1TC6zy03WWLtVZMO5uP4z098K+wir4e7oj+lcD8e0P1Rg7eADuHx4AP09erQ24fqfvgwXbkV/Zc4uo+7h5o67xGu4LeQx39Q2+ramYNQ1NCErdAh+dG8rsWKehUa7H4FDfvqhtbIZfXx0CvHQY7ueFqCH+qKqph87VBZ8XVGCknzearVacLL8CL3dXuGgUPBQSBFcXDbx1rmhqsaKfzhUA4KJRUF1rgT7QF3593SEiuFjXCF8PN/R100Ln6gKNcv34rYkAinJ9IaqIOM1dtlu72XlZrQKrCLSt/iFhtQo0muvvR1OLFW7anz65vVxngYerFo0tLXB10aDFKmiyNOCb46cQO84AcXGFi0YDqwh0WhccKanGYJ++8HZ3xZ5zZRja3xNe7lq4a13w8h4Tgv29sfNUCWosTQgP8sXm48VYNiUMb36TiwF93OHp7oqzlZ37dHWknxfyq2vw7P2jkTR+OF7/4jSemhSCsIB+sApgaW6Bl84VrhoNFAVosdq+H3TnOcMNyql7MWbIXo4UM7dbGzh0cWSxWJCamopPPvkEOp0OCxYswIIFC25rX0cpjgDHCgy6uWZrE1qszTBfK0ZTiwW1lquoabiIkktnMWnko6i6VoyKq+dR03DJ7ql4vn0DMTpoEr7J34rBviFobLGg6loRAMDP627MNCxSt2XMkL0YM2QvxgzZizFD9nKkmHGKNUerVq3CqVOn8Pbbb6OsrAzPP/88goKCMG3atJ4eGjkprcYVWo0rBvcfZdMeJbOhKAoG+QbbtIsIGpvrcbL0KwT2G45BvsFobmnEebMJw/wN0Lq0XcsyMqB33QyNiIiI6JfCYYujuro6bNmyBRs2bEBYWBjCwsKQn5+P9957j8URdbuOpkMpigJ31z4YN3S62qZ1cUPwXeO7a2hEREREdIc4bHGUm5uL5uZmREREqG2RkZHIzMyE1WqF5jbusyMiqKvr+fus1NfX23wnuhXGDNmLMUP2YsyQvRgzZC9HipnbXdPssMWR2WyGr68v3Nx+mpbk5+cHi8WCK1euoH///rd8jaamJpw9e7Yrh2mXoqKinh4C9TKMGbIXY4bsxZghezFmyF6OEjOt64qOOGxxVF9f3+YEbjxvbGxsb5c2XF1dMWLEiDs+NnvV19ejqKgIQ4cOhYfHnbuLPDkvxgzZizFD9mLMkL0YM2QvR4qZgoKC29rOYYsjd3f3NkXQjec63e1dflpRlB6/MkZrHh4eDjUecnyMGbIXY4bsxZghezFmyF6OEDO3e5sQh72JREBAAC5fvozm5ma1zWw2Q6fTwdvbuwdHRkREREREzshhi6PQ0FBotVocP35cbTt69Cj0ev1tXYyBiIiIiIjIHg5bZXh4eGD27Nl4+eWXceLECXz66afYuHEj5s+f39NDIyIiIiIiJ+Swa44AYPny5Xj55ZeRlJQET09PLF68GA8++GBPD4uIiIiIiJyQQxdHHh4eeO211/Daa6/19FCIiIiIiMjJOey0OiIiIiIiou7E4oiIiIiIiAgsjoiIiIiIiACwOCIiIiIiIgLA4oiIiIiIiAgAoIiI9PQgusKxY8cgInBzc+vpoUBE0NTUBFdXVyiK0tPDoV6AMUP2YsyQvRgzZC/GDNnLkWKmsbERiqJg7NixN93OoS/l3Rk9/QNoTVEUhyjSqPdgzJC9GDNkL8YM2YsxQ/ZypJhRFOW26gOn/eSIiIiIiIjIHlxzREREREREBBZHREREREREAFgcERERERERAWBxREREREREBIDFEREREREREQAWR0RERERERABYHBEREREREQFgcURERERERASAxVGXs1gsWLFiBcaNG4fo6Ghs3Lixp4dE3Wzv3r0ICQmx+VqyZAkA4MyZM/jtb38Lg8GARx55BKdOnbLZd9euXZg6dSoMBgMWLVqES5cuqX0igtdffx1RUVGYMGECVq1aBavV2q3nRndWY2MjZs6cicOHD6ttJSUlSE5OhtFoRHx8PPbv32+zz4EDBzBz5kwYDAbMnz8fJSUlNv05OTmIiYlBREQEVqxYgfr6erWP+an3ay9mXnnllTY5591331X7O5NXLl++jMWLFyMiIgKxsbH497//3T0nSp1WWVmJJUuWYMKECYiJicFf/vIXWCwWAMwz1L6bxYxT5xmhLvWnP/1JEhIS5NSpU/LJJ59IRESEfPzxxz09LOpGf/vb3yQlJUWqqqrUr6tXr0ptba1MmjRJXn31VSkoKJC0tDT59a9/LbW1tSIiYjKZJDw8XLZv3y5nz56VxMREefLJJ9XXfeutt2Ty5Mly5MgROXjwoERHR0t2dnZPnSZ1UkNDgyxatEiCg4Pl0KFDIiJitVolISFBnn32WSkoKJDMzEwxGAxSWloqIiKlpaViNBrlrbfekry8PHn66adl5syZYrVaRURk9+7dEhkZKZ9//rmYTCaJj4+X1NRU9ZjMT71bezEjIpKcnCxZWVk2Oaeurk5EOp9XUlJSJCkpSc6dOyebN2+WMWPGiMlk6r6Tpv+K1WqVuXPnysKFCyUvL0+OHDkicXFx8uqrrzLPULtuFjMizp1nWBx1odraWtHr9Ta/tN58801JTEzswVFRd3v22WfljTfeaNO+ZcsWiY2NVX/BWK1WiYuLkw8++EBERJYtWybPP/+8un1ZWZmEhITIDz/8ICIikydPVrcVEdmxY4dMmTKlK0+Fukh+fr785je/kYSEBJs/dA8cOCBGo1EtmEVEkpKSZO3atSIikpGRYZNP6urqJCIiQt1/3rx56rYiIkeOHJHw8HCpq6tjfurlOooZEZGYmBj5+uuv292vM3mluLhYgoODpaSkRO1fsWKFzeuRYyooKJDg4GAxm81q24cffijR0dHMM9Sum8WMiHPnGU6r60K5ublobm5GRESE2hYZGQmTycTpT78ghYWFGDp0aJt2k8mEyMhIKIoCAFAUBWPHjsXx48fV/nHjxqnbBwYGIigoCCaTCZWVlSgvL8f48ePV/sjISJSWlqKqqqpLz4fuvG+//RYTJ07Ev/71L5t2k8mE0aNHo0+fPmpbZGRkhzHi4eGBsLAwHD9+HC0tLTh58qRNv9FoRFNTE3Jzc5mfermOYubHH39EZWVluzkH6FxeMZlMCAwMxODBg236v/vuuzt7cnTH+fv7Izs7G35+fjbtP/74I/MMtetmMePseUbbbUf6BTKbzfD19YWbm5va5ufnB4vFgitXrqB///49ODrqDiKC77//Hvv370dWVhZaWlowbdo0LFmyBGazGSNGjLDZfsCAAcjPzwcAVFVVYeDAgW36KyoqYDabAcCm/0YCq6ioaLMfObZ58+a12242mzuMgVv1X7t2DRaLxaZfq9XCx8cHFRUV0Gg0zE+9WEcxU1hYCEVRkJmZiX379sHHxwdPPPEE5syZA6BzeaWjeKusrLxj50Vdw9vbGzExMepzq9WKd999F1FRUcwz1K6bxYyz5xkWR12ovr7eJiEAUJ83Njb2xJCom5WVlalxkJGRgQsXLuCVV15BQ0NDh/FxIzYaGho67G9oaFCft+4DGFvO5FYxcrP+9mKkdb+IMD85ofPnz0NRFAwbNgyJiYk4cuQIVq5cCU9PT8TFxXUqr9wqHqn3SE9Px5kzZ7B161bk5OQwz9AttY6Z06dPO3WeYXHUhdzd3dv8MG881+l0PTEk6maDBg3C4cOH0a9fPyiKgtDQUFitVixbtgwTJkxoNz5uxEZH8ePh4WGTSNzd3dXHwPUpD+Qc3N3dceXKFZu224kRb2/vNnHRut/DwwMtLS3MT05o9uzZmDJlCnx8fAAAo0aNQlFREf75z38iLi6uU3mlo30ZL71Leno63n77baxZswbBwcHMM3RLP4+ZkSNHOnWe4ZqjLhQQEIDLly+jublZbTObzdDpdPD29u7BkVF38vHxUdcVAcDw4cNhsVjg7++P6upqm22rq6vVj5MDAgLa7ff390dAQAAAqB9Pt37s7+/fJedB3a+jGLidGPHx8YG7u7tNf3NzM65cuaLGEPOT81EURf2D5YZhw4apU1I6k1duti/1Dmlpadi0aRPS09Px0EMPAWCeoZtrL2acPc+wOOpCoaGh0Gq16qJGADh69Cj0ej00Gr71vwRff/01Jk6caHPPh7Nnz8LHx0ddYCgiAK6vTzp27BgMBgMAwGAw4OjRo+p+5eXlKC8vh8FgQEBAAIKCgmz6jx49iqCgIK43ciIGgwGnT59WpyEA13/OHcVIfX09zpw5A4PBAI1GA71eb9N//PhxaLVajBo1ivnJSf31r39FcnKyTVtubi6GDRsGoHN5xWg0orS0VF2LcqPfaDR26TnRnbF+/Xq8//77WL16NWbMmKG2M89QRzqKGafPM912XbxfqJUrV8qMGTPEZDLJ3r17ZezYsbJnz56eHhZ1k5qaGomJiZFnnnlGCgsL5csvv5To6Gj5+9//LjU1NRIVFSVpaWmSn58vaWlpMmnSJPVyqseOHZOwsDDZvHmzep+AlJQU9bWzsrIkOjpaDh06JIcOHZLo6GjZuHFjT50q3SGtL8vc3Nws8fHxsnTpUsnLy5OsrCwxGo3q/UdKSkpEr9dLVlaWev+RhIQE9fLwu3btkrFjx8revXvFZDLJjBkzJC0tTT0W85NzaB0zJpNJRo8eLdnZ2VJcXCzvvfeejBkzRo4dOyYinc8rCxYskMTERDl79qxs3rxZ9Ho973PUCxQUFEhoaKisWbPG5r40VVVVzDPUrpvFjLPnGRZHXayurk7++Mc/itFolOjoaNm0aVNPD4m6WV5eniQnJ4vRaJRJkybJunXr1F8qJpNJZs+eLXq9Xh599FE5ffq0zb4ffPCBTJ48WYxGoyxatEguXbqk9jU3N8uf//xnGTdunEycOFHS09PV16Xe6+f3rCkqKpLHH39cxowZIzNmzJBvvvnGZvsvv/xSHnzwQQkPD5ekpCT1PhI3ZGVlyb333iuRkZGyfPlyaWhoUPuYn5zDz2Nm7969kpCQIHq9XqZNm9bmD9HO5JXq6mpJSUkRvV4vsbGx8uGHH3b9CVKnZWVlSXBwcLtfIswz1NatYsaZ84wi8v9zeoiIiIiIiH7BOOGTiIiIiIgILI6IiIiIiIgAsDgiIiIiIiICwOKIiIiIiIgIAIsjIiIiIiIiACyOiIiIiIiIALA4IiIiIiIiAsDiiIiIiIiICACLIyIi6iYhISHYtm0bAKCpqQk5OTndctwvvvgCBQUFAIDDhw8jJCQEFy5c6JZjExFR76KIiPT0IIiIyPmZzWZ4eXlBp9Nh+/bteOGFF3Du3LkuPWZpaSliY2PxzjvvYOLEiWhsbMTVq1fRv39/uLi4dOmxiYio99H29ACIiOiXwd/fX33cXf+X+/lx3NzcbMZBRETUGqfVERFRt7gxrW7btm1Yvny52nb48GEA16e/PfzwwwgPD0dcXBwyMjLQ2Nhos//atWsxZcoUREdHo6ioCGVlZfjDH/6Ae++9F2FhYbjvvvuQnp4Oq9WKCxcu4IEHHgAAzJ8/H+vWrWszra6hoQEZGRl44IEHoNfrMWvWLOzZs0c95rZt2xAXF6d+HzNmDB5++GEcPXpU3ebEiROYN28eIiIiMH78eCxevBhlZWVd/n4SEdGdx+KIiIi6VXx8PFasWAEA2L9/PyIiIrBv3z4sXboUc+fOxa5du/DSSy/h448/xrJly2z2/cc//oG1a9di/fr1GDp0KJ566inU1NRg06ZN2L17NxYsWIDs7Gx8/vnnCAwMxJYtWwAA69atw4IFC9qM5ZlnnsGOHTuwcuVK7Ny5E1OnTsXTTz+NTz/9VN2mvLwc77//PtLT07F9+3Z4eHjghRdegIigpaUFKSkpGD9+PHbu3ImcnByUlZWp50dERL0Lp9UREVG30ul08PLyAvDTVLvMzEzMnTsXjz32GADgnnvuQWpqKpKSknDhwgUMHjwYADBr1izo9XoA1z/1mTVrFqZPn47AwEAAQHJyMjZs2IBz585h6tSp6N+/PwCgX79+6Nu3r804CgsL8dlnnyEzMxP3338/AGDx4sXIzc1FZmYmpk6dCuD6xSNSU1MRGhoKAHjiiSewaNEimM1muLu74/Llyxg4cCAGDRqEu+++GxkZGbh48WJXvX1ERNSFWBwREVGPO3PmDE6cOIGtW7eqbTfWCxUWFqrF0ZAhQ9R+nU6HxMRE7N69GydOnEBxcTHOnTuH6upqWK3WWx7zxsUgIiMjbdrHjx+P1atX27QNHz5cfXyjsGtqasLAgQOxcOFCpKWlYe3atYiKisLkyZMxffp0e06fiIgcBIsjIiLqcVarFQsXLsScOXPa9LW+gIJOp1Mf19XVITExEQ0NDZg2bRrmzJmD8PBwPP74450ai4hAq7X99ejm5tbudgDw3HPPYd68efjqq69w8OBBpKWlITs7Gzt27Gh3PyIiclxcc0RERN1OURSb5yNHjsT333+PIUOGqF8VFRVYtWoVamtr232N/fv34/Tp03jnnXewZMkSxMfHw9PTExcvXlQLl58fp7WQkBAAsLm4AgD85z//wYgRI27rPM6fP4+XXnoJAwYMwO9+9zusXbsW2dnZKCwsRG5u7m29BhEROQ5+ckRERN2uT58+AIBTp05hxIgR+P3vf4+lS5di/fr1mDFjBioqKvDiiy9i8ODBHV56+6677gIA7Ny5Ew899BDKy8uxevVqNDU1qVe5u3GcvLw8jB492mb/4cOHY8qUKUhNTYWiKBgyZAg++ugjfPbZZ8jIyLit8/D19cVHH32EhoYGPPnkk9BoNNi+fTv69euHYcOG/TdvDRER9SAWR0RE1O2ioqJgMBjw2GOPIT09HdOnT8eaNWuQlZWFzMxM+Pj4IDY2Fs8991yHrxEeHo7ly5cjJycHGRkZCAgIQHx8PAIDA3Hy5EkA14uXRx55BKtWrUJxcTHi4uJsXmP16tVYvXo1XnzxRVy7dg3BwcFYt25dm+064uvriw0bNuCNN97A3Llz0dLSAqPRiE2bNsHT0/O/f4OIiKhHKNJdd+IjIiIiIiJyYFxzREREREREBBZHREREREREAFgcERERERERAWBxREREREREBIDFEREREREREQAWR0RERERERABYHBEREREREQFgcURERERERASAxREREREREREAFkdEREREREQAWBwREREREREBAP4PP+RBnOiPhRsAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def plot_losses(losses_gen, losses_dis):\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "\n",
        "    #convert losses into numpy\n",
        "    losses_gen = [i.cpu().detach().numpy() for i in losses_gen]\n",
        "    losses_dis = [i.cpu().detach().numpy() for i in losses_dis]\n",
        "    plt.plot(losses_gen,label=\"Generator\")\n",
        "    plt.plot(losses_dis,label=\"Discriminator\")\n",
        "    plt.xlabel(\"iterations\") #Epochs * Steps\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_losses(losses_gen, losses_dis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Join augmented data + real data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "iQcq2uyUNMod"
      },
      "outputs": [],
      "source": [
        "#define the device\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "sample_number = df_train[df_train['Class'] == 0].shape[0]\n",
        "gan_samples = gan.sample(sample_number)\n",
        "samples_test = np.append(gan_samples, np.ones((sample_number, 1)), axis=1)\n",
        "gan_df = pd.DataFrame(samples_test, columns=df_train.columns)\n",
        "#concat wgan_df with df_train\n",
        "df_concat = pd.concat([df_train, gan_df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgzG8giNNMoe",
        "outputId": "7e1f2f08-3f7c-4355-f34b-002267c09fbe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0    227845\n",
              "0.0    227451\n",
              "Name: Class, dtype: int64"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_concat['Class'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZAh0vE4NMoe"
      },
      "source": [
        "# Model evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOwrQY8nVPyC"
      },
      "source": [
        "### Deep Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import PrecisionRecallDisplay, RocCurveDisplay, average_precision_score\n",
        "from imblearn.metrics import classification_report_imbalanced\n",
        "\n",
        "\n",
        "def common_metrics(y_true, y_pred, y_prob):\n",
        "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred, normalize='true')\n",
        "    RocCurveDisplay.from_predictions(y_true, y_prob)\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
        "    #\n",
        "    fig, ax = plt.subplots()\n",
        "    PrecisionRecallDisplay.from_predictions(y_true, y_prob, ax=ax)\n",
        "    # P / (P + N)\n",
        "    ap_frauds = (len(y_true[y_true == 1]) / len(y_true))\n",
        "    plt.axhline(y=ap_frauds, color='r', linestyle='-')\n",
        "    plt.show()\n",
        "    #\n",
        "    print(classification_report_imbalanced(y_true, y_pred))\n",
        "    print(\"AUPRC:\", auc(recall, precision))\n",
        "    print(\"AP:\", average_precision_score(y_true, y_prob))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxxwX4RmNMoe"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, nr_features):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(nr_features, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.flatten(self.model(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7hMcvVUNMoe"
      },
      "outputs": [],
      "source": [
        "class FraudModel:\n",
        "    def __init__(self, train_df):\n",
        "        self.lr = 0.001\n",
        "        self.batch_size = 64\n",
        "        self.dataset = FraudDataset(train_df, fraud = False)\n",
        "        self.dataloader = DataLoader(self.dataset, self.batch_size, shuffle=True)\n",
        "        self.model = Model(nr_features=self.dataset.features).to(device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "        self.loss = nn.BCELoss()\n",
        "\n",
        "    def train(self, epochs=100):\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "            for _, (x, y) in enumerate(tqdm(self.dataloader)):\n",
        "                x,y = x.to(device), y.to(device)\n",
        "                self.optimizer.zero_grad()\n",
        "                y_pred = self.model(x)\n",
        "                loss = self.loss(y_pred, y)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                epoch_loss += loss.item() / len(x)\n",
        "            print(\"[Epoch %d/%d] loss: %f\" % (epoch+1, epochs, np.mean(epoch_loss)))\n",
        "\n",
        "    def predict(self, x):\n",
        "        with torch.no_grad():\n",
        "            pred = self.model(x)\n",
        "            return pred.cpu().numpy()\n",
        "\n",
        "    def evaluate(self, test_df, confidence=0.50):\n",
        "            input = torch.Tensor(test_df.drop(['Class'], axis=1).values).to(device)\n",
        "            y_true = torch.Tensor(test_df['Class'].values).to(device)\n",
        "            #preds = self.predict(input)\n",
        "            preds = np.array([])\n",
        "            dataloader_test = DataLoader(TensorDataset(input, y_true), batch_size=64, shuffle=False)\n",
        "\n",
        "            for (inp, _) in dataloader_test:\n",
        "                pred = self.predict(inp)\n",
        "                preds = np.concatenate((preds, pred), axis=None)\n",
        "            labels = np.zeros_like(preds)\n",
        "            labels[preds >= confidence] = 1\n",
        "            common_metrics(y_true.cpu().numpy(), labels,preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tiV0tCMHNMoe",
        "outputId": "0130c9df-a1d9-41a5-a4ac-769a641dd23c"
      },
      "outputs": [],
      "source": [
        "fw_model = FraudModel(df_concat)\n",
        "fw_model.train(epochs=300)\n",
        "fw_model.evaluate(df_test, confidence=0.90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsD2LUSt_8Pm"
      },
      "outputs": [],
      "source": [
        "path = \"vgan_model.pt\"\n",
        "torch.save(fw_model.model.state_dict(), path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA46ZOhkNMof"
      },
      "source": [
        "### LSTM-Based Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqE6UVvINMof"
      },
      "outputs": [],
      "source": [
        "#split df_concat into train and test\n",
        "ltsm_train, ltsm_test = train_test_split(df_concat, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXGHXm2URN6t"
      },
      "outputs": [],
      "source": [
        "def create_time_dataset(in_df, seq_size=10):\n",
        "    X, y = [], []\n",
        "    for i in range(len(in_df) - seq_size):\n",
        "        slice = in_df[i:i+seq_size]\n",
        "        X.append(slice.drop(['Class', 'Time'], axis=1).values)\n",
        "        y.append(slice['Class'].values[-1])\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    return torch.Tensor(X).to(device), torch.Tensor(y).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5vGLuqCNMof"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = 256\n",
        "        self.lstm = nn.LSTM(input_size,\n",
        "                            hidden_size=self.hidden_size,\n",
        "                            num_layers=5,\n",
        "                            bidirectional=True,\n",
        "                            batch_first=True)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(self.hidden_size * 2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, hx = self.lstm(x)\n",
        "        out = torch.cat((hx[0][-2, :, :], hx[0][-1, :, :]), dim=1)\n",
        "        return torch.flatten(self.linear(out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxXQ9eFUNMog"
      },
      "outputs": [],
      "source": [
        "class LSTMFraudModel:\n",
        "    def __init__(self, train_df):\n",
        "        self.lr = 0.0001\n",
        "        self.batch_size = 128\n",
        "        X_train, y_train = create_time_dataset(train_df)\n",
        "        self.dataset = TensorDataset(X_train, y_train)\n",
        "        self.dataloader = DataLoader(self.dataset, self.batch_size, shuffle=True)\n",
        "        self.model = LSTMModel(input_size=X_train.shape[2]).to(device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "        self.loss = nn.BCELoss()\n",
        "\n",
        "    def train(self, epochs=100):\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "            for _, (x, y) in enumerate(tqdm(self.dataloader)):\n",
        "                x,y = x.to(device), y.to(device)\n",
        "                self.optimizer.zero_grad()\n",
        "                y_pred = self.model(x)\n",
        "                loss = self.loss(y_pred, y)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                epoch_loss += loss.item() / len(x)\n",
        "            print(\"[Epoch %d/%d] loss: %f\" % (epoch+1, epochs, np.mean(epoch_loss)))\n",
        "\n",
        "    def predict(self, x):\n",
        "        with torch.no_grad():\n",
        "            pred = self.model(x)\n",
        "            return pred.cpu().numpy()\n",
        "\n",
        "    def evaluate(self, test_df, confidence=0.50):\n",
        "        input, y_true = create_time_dataset(test_df)\n",
        "        dataloader = DataLoader(TensorDataset(input, y_true), self.batch_size, shuffle=False)\n",
        "        preds = np.array([])\n",
        "        for (inp, _) in dataloader:\n",
        "            pred = self.predict(inp)\n",
        "            preds = np.concatenate((preds, pred), axis=None)\n",
        "        labels = np.zeros_like(preds)\n",
        "        labels[preds >= confidence] = 1.0\n",
        "        common_metrics(y_true.cpu().numpy(), labels, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwXwPyZDNMog"
      },
      "outputs": [],
      "source": [
        "lstm = LSTMFraudModel(ltsm_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgwtLJdsNMog",
        "outputId": "a4b3284c-bc58-4f54-a048-a098aa54db52"
      },
      "outputs": [],
      "source": [
        "lstm.train(epochs=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "okxsi5Uk-GCh",
        "outputId": "21747c25-6a6b-412f-cab0-e7b0e5858bb0"
      },
      "outputs": [],
      "source": [
        "lstm.evaluate(df_test,confidence=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJp_92UnAwDh"
      },
      "outputs": [],
      "source": [
        "path = \"lstm_model.pth\"\n",
        "torch.save(lstm.model.state_dict(), path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Common Regression Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def common_regression_models(train_df, test_df):\n",
        "    r = setup(data=train_df, target='Class', session_id=42, train_size=0.80)\n",
        "    r.add_metric('apc', 'APC', average_precision_score, target='pred_proba')\n",
        "    best = r.compare_models(sort='AUC')\n",
        "\n",
        "    pred_df = predict_model(best, data=test_df.copy().drop('Class', axis=1), raw_score=True)\n",
        "    l = np.argmax(pred_df[['prediction_score_0', 'prediction_score_1']].values, axis=1)\n",
        "    p = pred_df[['prediction_score_0', 'prediction_score_1']].values.squeeze()\n",
        "    p_bin = p[:, 1]\n",
        "    p_bin[l == 0] = 1.0 - p[l == 0][:, 0]\n",
        "    common_metrics(test_df['Class'].values, l, p_bin)\n",
        "\n",
        "    return best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "#split df_concat into train and test\n",
        "vgan_train_df, vgan_test_df = train_test_split(df_concat, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_a9e6f_row8_col1 {\n",
              "  background-color: lightgreen;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_a9e6f\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_a9e6f_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
              "      <th id=\"T_a9e6f_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_a9e6f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_a9e6f_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
              "      <td id=\"T_a9e6f_row0_col1\" class=\"data row0 col1\" >42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9e6f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_a9e6f_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
              "      <td id=\"T_a9e6f_row1_col1\" class=\"data row1 col1\" >Class</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9e6f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_a9e6f_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
              "      <td id=\"T_a9e6f_row2_col1\" class=\"data row2 col1\" >Binary</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9e6f_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_a9e6f_row3_col0\" class=\"data row3 col0\" >Original data shape</td>\n",
              "      <td id=\"T_a9e6f_row3_col1\" class=\"data row3 col1\" >(364236, 31)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9e6f_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_a9e6f_row4_col0\" class=\"data row4 col0\" >Transformed data shape</td>\n",
              "      <td id=\"T_a9e6f_row4_col1\" class=\"data row4 col1\" >(364236, 31)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9e6f_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_a9e6f_row5_col0\" class=\"data row5 col0\" >Transformed train set shape</td>\n",
              "      <td id=\"T_a9e6f_row5_col1\" class=\"data row5 col1\" >(291388, 31)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9e6f_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "      <td id=\"T_a9e6f_row6_col0\" class=\"data row6 col0\" >Transformed test set shape</td>\n",
              "      <td id=\"T_a9e6f_row6_col1\" class=\"data row6 col1\" >(72848, 31)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9e6f_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "      <td id=\"T_a9e6f_row7_col0\" class=\"data row7 col0\" >Numeric features</td>\n",
              "      <td id=\"T_a9e6f_row7_col1\" class=\"data row7 col1\" >30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9e6f_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "      <td id=\"T_a9e6f_row8_col0\" class=\"data row8 col0\" >Preprocess</td>\n",
              "      <td id=\"T_a9e6f_row8_col1\" class=\"data row8 col1\" >True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9e6f_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "      <td id=\"T_a9e6f_row9_col0\" class=\"data row9 col0\" >Imputation type</td>\n",
              "      <td id=\"T_a9e6f_row9_col1\" class=\"data row9 col1\" >simple</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9e6f_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
              "      <td id=\"T_a9e6f_row10_col0\" class=\"data row10 col0\" >Numeric imputation</td>\n",
              "      <td id=\"T_a9e6f_row10_col1\" class=\"data row10 col1\" >mean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9e6f_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
              "      <td id=\"T_a9e6f_row11_col0\" class=\"data row11 col0\" >Categorical imputation</td>\n",
              "      <td id=\"T_a9e6f_row11_col1\" class=\"data row11 col1\" >mode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9e6f_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
              "      <td id=\"T_a9e6f_row12_col0\" class=\"data row12 col0\" >Fold Generator</td>\n",
              "      <td id=\"T_a9e6f_row12_col1\" class=\"data row12 col1\" >StratifiedKFold</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9e6f_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
              "      <td id=\"T_a9e6f_row13_col0\" class=\"data row13 col0\" >Fold Number</td>\n",
              "      <td id=\"T_a9e6f_row13_col1\" class=\"data row13 col1\" >10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9e6f_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
              "      <td id=\"T_a9e6f_row14_col0\" class=\"data row14 col0\" >CPU Jobs</td>\n",
              "      <td id=\"T_a9e6f_row14_col1\" class=\"data row14 col1\" >-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9e6f_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
              "      <td id=\"T_a9e6f_row15_col0\" class=\"data row15 col0\" >Use GPU</td>\n",
              "      <td id=\"T_a9e6f_row15_col1\" class=\"data row15 col1\" >False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9e6f_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
              "      <td id=\"T_a9e6f_row16_col0\" class=\"data row16 col0\" >Log Experiment</td>\n",
              "      <td id=\"T_a9e6f_row16_col1\" class=\"data row16 col1\" >False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9e6f_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
              "      <td id=\"T_a9e6f_row17_col0\" class=\"data row17 col0\" >Experiment Name</td>\n",
              "      <td id=\"T_a9e6f_row17_col1\" class=\"data row17 col1\" >clf-default-name</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a9e6f_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
              "      <td id=\"T_a9e6f_row18_col0\" class=\"data row18 col0\" >USI</td>\n",
              "      <td id=\"T_a9e6f_row18_col1\" class=\"data row18 col1\" >5952</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fef0efb7a30>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Initiated</th>\n",
              "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
              "      <td>13:40:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Status</th>\n",
              "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
              "      <td>Fitting 10 Folds</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Estimator</th>\n",
              "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
              "      <td>Gradient Boosting Classifier</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                            \n",
              "                                                                            \n",
              "Initiated  . . . . . . . . . . . . . . . . . .                      13:40:36\n",
              "Status     . . . . . . . . . . . . . . . . . .              Fitting 10 Folds\n",
              "Estimator  . . . . . . . . . . . . . . . . . .  Gradient Boosting Classifier"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_ead8d th {\n",
              "  text-align: left;\n",
              "}\n",
              "#T_ead8d_row0_col0, #T_ead8d_row0_col1, #T_ead8d_row0_col2, #T_ead8d_row0_col3, #T_ead8d_row0_col4, #T_ead8d_row0_col5, #T_ead8d_row0_col6, #T_ead8d_row0_col7, #T_ead8d_row0_col8, #T_ead8d_row0_col9, #T_ead8d_row1_col0, #T_ead8d_row1_col1, #T_ead8d_row1_col2, #T_ead8d_row1_col3, #T_ead8d_row1_col4, #T_ead8d_row1_col5, #T_ead8d_row1_col6, #T_ead8d_row1_col7, #T_ead8d_row1_col8, #T_ead8d_row1_col9, #T_ead8d_row2_col0, #T_ead8d_row2_col1, #T_ead8d_row2_col2, #T_ead8d_row2_col3, #T_ead8d_row2_col4, #T_ead8d_row2_col5, #T_ead8d_row2_col6, #T_ead8d_row2_col7, #T_ead8d_row2_col8, #T_ead8d_row2_col9, #T_ead8d_row3_col0, #T_ead8d_row3_col1, #T_ead8d_row3_col2, #T_ead8d_row3_col3, #T_ead8d_row3_col4, #T_ead8d_row3_col5, #T_ead8d_row3_col6, #T_ead8d_row3_col7, #T_ead8d_row3_col8, #T_ead8d_row3_col9, #T_ead8d_row4_col0, #T_ead8d_row4_col1, #T_ead8d_row4_col2, #T_ead8d_row4_col3, #T_ead8d_row4_col4, #T_ead8d_row4_col5, #T_ead8d_row4_col6, #T_ead8d_row4_col7, #T_ead8d_row4_col8, #T_ead8d_row4_col9, #T_ead8d_row5_col0, #T_ead8d_row5_col1, #T_ead8d_row5_col2, #T_ead8d_row5_col3, #T_ead8d_row5_col4, #T_ead8d_row5_col5, #T_ead8d_row5_col6, #T_ead8d_row5_col7, #T_ead8d_row5_col8, #T_ead8d_row5_col9, #T_ead8d_row6_col0, #T_ead8d_row6_col1, #T_ead8d_row6_col2, #T_ead8d_row6_col3, #T_ead8d_row6_col4, #T_ead8d_row6_col5, #T_ead8d_row6_col6, #T_ead8d_row6_col7, #T_ead8d_row6_col8, #T_ead8d_row6_col9, #T_ead8d_row7_col0, #T_ead8d_row7_col1, #T_ead8d_row7_col2, #T_ead8d_row7_col3, #T_ead8d_row7_col4, #T_ead8d_row7_col5, #T_ead8d_row7_col6, #T_ead8d_row7_col7, #T_ead8d_row7_col8, #T_ead8d_row7_col9, #T_ead8d_row8_col0, #T_ead8d_row8_col1, #T_ead8d_row8_col2, #T_ead8d_row8_col3, #T_ead8d_row8_col4, #T_ead8d_row8_col5, #T_ead8d_row8_col6, #T_ead8d_row8_col7, #T_ead8d_row8_col8, #T_ead8d_row8_col9 {\n",
              "  text-align: left;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_ead8d\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_ead8d_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
              "      <th id=\"T_ead8d_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
              "      <th id=\"T_ead8d_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
              "      <th id=\"T_ead8d_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
              "      <th id=\"T_ead8d_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
              "      <th id=\"T_ead8d_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
              "      <th id=\"T_ead8d_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
              "      <th id=\"T_ead8d_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
              "      <th id=\"T_ead8d_level0_col8\" class=\"col_heading level0 col8\" >APC</th>\n",
              "      <th id=\"T_ead8d_level0_col9\" class=\"col_heading level0 col9\" >TT (Sec)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_ead8d_level0_row0\" class=\"row_heading level0 row0\" >lr</th>\n",
              "      <td id=\"T_ead8d_row0_col0\" class=\"data row0 col0\" >Logistic Regression</td>\n",
              "      <td id=\"T_ead8d_row0_col1\" class=\"data row0 col1\" >0.9987</td>\n",
              "      <td id=\"T_ead8d_row0_col2\" class=\"data row0 col2\" >0.9999</td>\n",
              "      <td id=\"T_ead8d_row0_col3\" class=\"data row0 col3\" >0.9977</td>\n",
              "      <td id=\"T_ead8d_row0_col4\" class=\"data row0 col4\" >0.9996</td>\n",
              "      <td id=\"T_ead8d_row0_col5\" class=\"data row0 col5\" >0.9987</td>\n",
              "      <td id=\"T_ead8d_row0_col6\" class=\"data row0 col6\" >0.9973</td>\n",
              "      <td id=\"T_ead8d_row0_col7\" class=\"data row0 col7\" >0.9973</td>\n",
              "      <td id=\"T_ead8d_row0_col8\" class=\"data row0 col8\" >0.9999</td>\n",
              "      <td id=\"T_ead8d_row0_col9\" class=\"data row0 col9\" >14.4810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ead8d_level0_row1\" class=\"row_heading level0 row1\" >rf</th>\n",
              "      <td id=\"T_ead8d_row1_col0\" class=\"data row1 col0\" >Random Forest Classifier</td>\n",
              "      <td id=\"T_ead8d_row1_col1\" class=\"data row1 col1\" >0.9998</td>\n",
              "      <td id=\"T_ead8d_row1_col2\" class=\"data row1 col2\" >0.9999</td>\n",
              "      <td id=\"T_ead8d_row1_col3\" class=\"data row1 col3\" >0.9996</td>\n",
              "      <td id=\"T_ead8d_row1_col4\" class=\"data row1 col4\" >0.9999</td>\n",
              "      <td id=\"T_ead8d_row1_col5\" class=\"data row1 col5\" >0.9998</td>\n",
              "      <td id=\"T_ead8d_row1_col6\" class=\"data row1 col6\" >0.9995</td>\n",
              "      <td id=\"T_ead8d_row1_col7\" class=\"data row1 col7\" >0.9995</td>\n",
              "      <td id=\"T_ead8d_row1_col8\" class=\"data row1 col8\" >0.9999</td>\n",
              "      <td id=\"T_ead8d_row1_col9\" class=\"data row1 col9\" >40.4210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ead8d_level0_row2\" class=\"row_heading level0 row2\" >ada</th>\n",
              "      <td id=\"T_ead8d_row2_col0\" class=\"data row2 col0\" >Ada Boost Classifier</td>\n",
              "      <td id=\"T_ead8d_row2_col1\" class=\"data row2 col1\" >0.9995</td>\n",
              "      <td id=\"T_ead8d_row2_col2\" class=\"data row2 col2\" >0.9999</td>\n",
              "      <td id=\"T_ead8d_row2_col3\" class=\"data row2 col3\" >0.9994</td>\n",
              "      <td id=\"T_ead8d_row2_col4\" class=\"data row2 col4\" >0.9997</td>\n",
              "      <td id=\"T_ead8d_row2_col5\" class=\"data row2 col5\" >0.9995</td>\n",
              "      <td id=\"T_ead8d_row2_col6\" class=\"data row2 col6\" >0.9991</td>\n",
              "      <td id=\"T_ead8d_row2_col7\" class=\"data row2 col7\" >0.9991</td>\n",
              "      <td id=\"T_ead8d_row2_col8\" class=\"data row2 col8\" >1.0000</td>\n",
              "      <td id=\"T_ead8d_row2_col9\" class=\"data row2 col9\" >31.3360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ead8d_level0_row3\" class=\"row_heading level0 row3\" >dt</th>\n",
              "      <td id=\"T_ead8d_row3_col0\" class=\"data row3 col0\" >Decision Tree Classifier</td>\n",
              "      <td id=\"T_ead8d_row3_col1\" class=\"data row3 col1\" >0.9995</td>\n",
              "      <td id=\"T_ead8d_row3_col2\" class=\"data row3 col2\" >0.9995</td>\n",
              "      <td id=\"T_ead8d_row3_col3\" class=\"data row3 col3\" >0.9995</td>\n",
              "      <td id=\"T_ead8d_row3_col4\" class=\"data row3 col4\" >0.9996</td>\n",
              "      <td id=\"T_ead8d_row3_col5\" class=\"data row3 col5\" >0.9995</td>\n",
              "      <td id=\"T_ead8d_row3_col6\" class=\"data row3 col6\" >0.9991</td>\n",
              "      <td id=\"T_ead8d_row3_col7\" class=\"data row3 col7\" >0.9991</td>\n",
              "      <td id=\"T_ead8d_row3_col8\" class=\"data row3 col8\" >0.9993</td>\n",
              "      <td id=\"T_ead8d_row3_col9\" class=\"data row3 col9\" >6.1090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ead8d_level0_row4\" class=\"row_heading level0 row4\" >qda</th>\n",
              "      <td id=\"T_ead8d_row4_col0\" class=\"data row4 col0\" >Quadratic Discriminant Analysis</td>\n",
              "      <td id=\"T_ead8d_row4_col1\" class=\"data row4 col1\" >0.9984</td>\n",
              "      <td id=\"T_ead8d_row4_col2\" class=\"data row4 col2\" >0.9993</td>\n",
              "      <td id=\"T_ead8d_row4_col3\" class=\"data row4 col3\" >0.9969</td>\n",
              "      <td id=\"T_ead8d_row4_col4\" class=\"data row4 col4\" >0.9999</td>\n",
              "      <td id=\"T_ead8d_row4_col5\" class=\"data row4 col5\" >0.9984</td>\n",
              "      <td id=\"T_ead8d_row4_col6\" class=\"data row4 col6\" >0.9968</td>\n",
              "      <td id=\"T_ead8d_row4_col7\" class=\"data row4 col7\" >0.9968</td>\n",
              "      <td id=\"T_ead8d_row4_col8\" class=\"data row4 col8\" >0.9992</td>\n",
              "      <td id=\"T_ead8d_row4_col9\" class=\"data row4 col9\" >0.6850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ead8d_level0_row5\" class=\"row_heading level0 row5\" >knn</th>\n",
              "      <td id=\"T_ead8d_row5_col0\" class=\"data row5 col0\" >K Neighbors Classifier</td>\n",
              "      <td id=\"T_ead8d_row5_col1\" class=\"data row5 col1\" >0.9978</td>\n",
              "      <td id=\"T_ead8d_row5_col2\" class=\"data row5 col2\" >0.9990</td>\n",
              "      <td id=\"T_ead8d_row5_col3\" class=\"data row5 col3\" >0.9968</td>\n",
              "      <td id=\"T_ead8d_row5_col4\" class=\"data row5 col4\" >0.9987</td>\n",
              "      <td id=\"T_ead8d_row5_col5\" class=\"data row5 col5\" >0.9978</td>\n",
              "      <td id=\"T_ead8d_row5_col6\" class=\"data row5 col6\" >0.9956</td>\n",
              "      <td id=\"T_ead8d_row5_col7\" class=\"data row5 col7\" >0.9956</td>\n",
              "      <td id=\"T_ead8d_row5_col8\" class=\"data row5 col8\" >0.9988</td>\n",
              "      <td id=\"T_ead8d_row5_col9\" class=\"data row5 col9\" >37.7820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ead8d_level0_row6\" class=\"row_heading level0 row6\" >nb</th>\n",
              "      <td id=\"T_ead8d_row6_col0\" class=\"data row6 col0\" >Naive Bayes</td>\n",
              "      <td id=\"T_ead8d_row6_col1\" class=\"data row6 col1\" >0.9975</td>\n",
              "      <td id=\"T_ead8d_row6_col2\" class=\"data row6 col2\" >0.9974</td>\n",
              "      <td id=\"T_ead8d_row6_col3\" class=\"data row6 col3\" >0.9971</td>\n",
              "      <td id=\"T_ead8d_row6_col4\" class=\"data row6 col4\" >0.9978</td>\n",
              "      <td id=\"T_ead8d_row6_col5\" class=\"data row6 col5\" >0.9975</td>\n",
              "      <td id=\"T_ead8d_row6_col6\" class=\"data row6 col6\" >0.9950</td>\n",
              "      <td id=\"T_ead8d_row6_col7\" class=\"data row6 col7\" >0.9950</td>\n",
              "      <td id=\"T_ead8d_row6_col8\" class=\"data row6 col8\" >0.9975</td>\n",
              "      <td id=\"T_ead8d_row6_col9\" class=\"data row6 col9\" >0.4410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ead8d_level0_row7\" class=\"row_heading level0 row7\" >svm</th>\n",
              "      <td id=\"T_ead8d_row7_col0\" class=\"data row7 col0\" >SVM - Linear Kernel</td>\n",
              "      <td id=\"T_ead8d_row7_col1\" class=\"data row7 col1\" >0.9986</td>\n",
              "      <td id=\"T_ead8d_row7_col2\" class=\"data row7 col2\" >0.0000</td>\n",
              "      <td id=\"T_ead8d_row7_col3\" class=\"data row7 col3\" >0.9981</td>\n",
              "      <td id=\"T_ead8d_row7_col4\" class=\"data row7 col4\" >0.9990</td>\n",
              "      <td id=\"T_ead8d_row7_col5\" class=\"data row7 col5\" >0.9986</td>\n",
              "      <td id=\"T_ead8d_row7_col6\" class=\"data row7 col6\" >0.9971</td>\n",
              "      <td id=\"T_ead8d_row7_col7\" class=\"data row7 col7\" >0.9971</td>\n",
              "      <td id=\"T_ead8d_row7_col8\" class=\"data row7 col8\" >0.0000</td>\n",
              "      <td id=\"T_ead8d_row7_col9\" class=\"data row7 col9\" >2.2090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ead8d_level0_row8\" class=\"row_heading level0 row8\" >ridge</th>\n",
              "      <td id=\"T_ead8d_row8_col0\" class=\"data row8 col0\" >Ridge Classifier</td>\n",
              "      <td id=\"T_ead8d_row8_col1\" class=\"data row8 col1\" >0.9978</td>\n",
              "      <td id=\"T_ead8d_row8_col2\" class=\"data row8 col2\" >0.0000</td>\n",
              "      <td id=\"T_ead8d_row8_col3\" class=\"data row8 col3\" >0.9974</td>\n",
              "      <td id=\"T_ead8d_row8_col4\" class=\"data row8 col4\" >0.9981</td>\n",
              "      <td id=\"T_ead8d_row8_col5\" class=\"data row8 col5\" >0.9978</td>\n",
              "      <td id=\"T_ead8d_row8_col6\" class=\"data row8 col6\" >0.9955</td>\n",
              "      <td id=\"T_ead8d_row8_col7\" class=\"data row8 col7\" >0.9955</td>\n",
              "      <td id=\"T_ead8d_row8_col8\" class=\"data row8 col8\" >0.0000</td>\n",
              "      <td id=\"T_ead8d_row8_col9\" class=\"data row8 col9\" >0.4380</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fef0e667fd0>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9230a0abc88b4c928025ca68996f393c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing:   0%|          | 0/61 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "best_vgan = common_regression_models(vgan_train_df, df_test)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
