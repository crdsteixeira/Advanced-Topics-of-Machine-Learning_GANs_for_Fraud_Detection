{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Body>   \n",
    "<img src=\"data:image/png;base64,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\" width=\"200\" align=\"right\" >   \n",
    "<h1> <b>Advanced Topics on Machine Learning </b> </h1>\n",
    "<p><b>CÃ¡tia Teixeira</b> (200808037) | <b>Henrique Bastos</b> (202204383) | <b>Ian Karkles</b> (202200596) | <b>Vitor Pereira</b> (202210497)\n",
    "<p>Master in Data Science and Engineering</p>\n",
    "</Body>\n",
    "Faculdade de Engenharia da Universidade do Porto\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation using cGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import SubsetRandomSampler, DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/henriqueribeiro/Downloads/creditcard.csv')\n",
    "\n",
    "#split df into train and test\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (227845, 31)\n",
      "Test shape:  (56962, 31)\n"
     ]
    }
   ],
   "source": [
    "#print the characteristics of the dataframes\n",
    "print('Train shape: ', df_train.shape)\n",
    "print('Test shape: ', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudDataset(Dataset):\n",
    " \n",
    "  def __init__(self, dataframe, fraud = True):\n",
    "    scaler = StandardScaler()\n",
    "    df = dataframe\n",
    "    if fraud:\n",
    "      df = df[df['Class'] == 1]\n",
    " \n",
    "    x = df.iloc[:, 0:-1].values\n",
    "    y = df.iloc[:, -1].values\n",
    "    #y = np.reshape(y, (1,-1))\n",
    "    \n",
    "    # Standardize data\n",
    "    #x = scaler.fit_transform(x)\n",
    "    #y = scaler.fit_transform(y)\n",
    "\n",
    "    self.x_data=torch.tensor(x,dtype=torch.float32)\n",
    "    self.y_data=torch.tensor(y,dtype=torch.float32)\n",
    "\n",
    "    self.features = x.shape[1]\n",
    "  def __len__(self):\n",
    "    return len(self.x_data)\n",
    "   \n",
    "  def __getitem__(self,idx):\n",
    "    return self.x_data[idx], self.y_data[idx]\n",
    "\n",
    "creditData = FraudDataset(df_train, fraud= True)\n",
    "creditDataTest = FraudDataset(df_test, fraud= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(creditData, batch_size=32, drop_last=True)\n",
    "test_dataloader = DataLoader(creditDataTest, batch_size=32, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, nr_features):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "        nn.Linear(latent_dim, 1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(1024, 512),\n",
    "        nn.ReLU(inplace = True),\n",
    "        nn.Dropout(0.2),\n",
    "        \n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(inplace = True),\n",
    "        nn.Dropout(0.2),\n",
    "        \n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(inplace = True),\n",
    "        nn.Dropout(0.2),\n",
    "        \n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(inplace = True),\n",
    "        nn.Dropout(0.2),\n",
    "        \n",
    "        nn.Linear(64, 32),\n",
    "        nn.ReLU(inplace = True),\n",
    "        nn.Dropout(0.2),\n",
    "        \n",
    "        nn.Linear(32, 16),\n",
    "        nn.ReLU(inplace = True),\n",
    "        nn.Dropout(0.2),\n",
    "        \n",
    "        nn.Linear(16, nr_features),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nr_features):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "        nn.Linear(nr_features, 32),\n",
    "        nn.ReLU(inplace = True),\n",
    "        nn.Dropout(0.2),\n",
    "        \n",
    "        nn.Linear(32, 16),\n",
    "        nn.ReLU(inplace = True),\n",
    "        nn.Dropout(0.2),\n",
    "        \n",
    "        nn.Linear(16, 1),\n",
    "        \n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Loss Function + Device + Reset Gradients Funct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, train_df, latent_size=15):\n",
    "        self.lr = 0.0001\n",
    "        self.batch_size = 32\n",
    "        self.n_critic = 5\n",
    "        self.clip_value = 0.01\n",
    "        self.cur_batch_size = 64\n",
    "        self.latent_dim = 15\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.dataset = FraudDataset(train_df, fraud= True)\n",
    "        self.dataloader = DataLoader(self.dataset, self.batch_size, shuffle=True)\n",
    "\n",
    "        self.G = Generator(latent_dim=latent_size, nr_features=self.dataset.features).to(self.device)\n",
    "        self.D = Discriminator(nr_features=self.dataset.features).to(self.device)\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        self.g_optimizer = torch.optim.Adam(self.G.parameters(), self.lr)\n",
    "        self.d_optimizer = torch.optim.Adam(self.D.parameters(), self.lr)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def train_generator(self,real_data):\n",
    "        # Clear generator gradients\n",
    "        self.g_optimizer.zero_grad()\n",
    "\n",
    "        fake_targets = torch.ones(real_data.size(0)).to(self.device)\n",
    "        fake_targets = fake_targets - 0.1\n",
    "\n",
    "        # Random noise from a uniform distribution\n",
    "        latent_space_samples = torch.randn(real_data.size(0), self.latent_dim).to(self.device)\n",
    "        generated_data = self.G(latent_space_samples)  # Fake data generated by the generator\n",
    "        fake_preds = self.D(generated_data).reshape(-1)\n",
    "\n",
    "        g_loss = self.criterion(fake_preds, fake_targets)\n",
    "\n",
    "        # Compute feature matching loss\n",
    "        real_features = self.D(real_data).detach().mean(dim=0)\n",
    "        generated_features = self.D(generated_data).mean(dim=0)\n",
    "        feature_matching_loss = torch.mean(torch.abs(real_features - generated_features))\n",
    "\n",
    "        # Combine generator loss and feature matching loss\n",
    "        lambda_fm = 0.01  # Adjust the weight of feature matching loss as needed\n",
    "        total_g_loss = g_loss + lambda_fm * feature_matching_loss\n",
    "\n",
    "        total_g_loss.backward()\n",
    "        self.g_optimizer.step()\n",
    "\n",
    "        return total_g_loss, generated_data\n",
    "\n",
    "    def train_discriminator(self, real_data, real_labels):\n",
    "    # Reset gradients\n",
    "        \n",
    "        self.d_optimizer.zero_grad()\n",
    "        real_labels = torch.ones(real_data.size(0)).to(self.device) * 0.9\n",
    "        \n",
    "        real_preds = self.D(real_data).reshape(-1)\n",
    "        d_loss_real = self.criterion(real_preds, real_labels)\n",
    "        \n",
    "        fake_labels = torch.zeros(self.cur_batch_size).to(self.device) * 0.1\n",
    "\n",
    "        # random noise from uniform distribution\n",
    "        latent_space_samples = torch.randn(self.cur_batch_size, self.latent_dim).to(self.device)\n",
    "\n",
    "   \n",
    "        generated_data = self.G(latent_space_samples).detach()  # fake data generated by generator\n",
    "        fake_preds = self.D(generated_data).reshape(-1)\n",
    "        d_loss_fake = self.criterion(fake_preds, fake_labels)\n",
    "        #d_loss_fake = self.criterion(fake_preds, fake_labels)\n",
    "        \n",
    "        loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        # Adjust the parameters using backprop\n",
    "        self.d_optimizer.step()\n",
    "        \n",
    "            \n",
    "        return loss\n",
    "\n",
    "    def train(self,real_data, epochs=1000):\n",
    "        losses_gen = []\n",
    "        losses_dis = []\n",
    "        for epoch in range(epochs):  \n",
    "            for i, (real_data, real_labels)  in enumerate(self.dataloader):\n",
    "                # Train discriminator\n",
    "                d_error = self.train_discriminator(real_data, real_labels)\n",
    "                # Clip weights of discriminator\n",
    "                for p in self.D.parameters():\n",
    "                    p.data.clamp_(-self.clip_value, self.clip_value)\n",
    "                # Train generator every n_critic iterations\n",
    "                if i % self.n_critic == 0:\n",
    "                    g_error, _ = self.train_generator(real_data)        \n",
    "                losses_gen.append(g_error)\n",
    "                losses_dis.append(d_error)\n",
    "\n",
    "            print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f} \\n'\n",
    "                .format(epoch, epoch, i+1, len(self.dataloader), d_error.item(), g_error.item()))\n",
    "            \n",
    "        return losses_gen, losses_dis\n",
    "\n",
    "\n",
    "\n",
    "    def sample(self, count):\n",
    "        with torch.no_grad():\n",
    "            z = torch.Tensor(np.random.normal(0, 1, (count, self.latent_size))).to(device)\n",
    "            gen = self.G(z)\n",
    "            return gen.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/0], Step [13/13], d_loss: 1.4665, g_loss: 0.6896 \n",
      "\n",
      "Epoch [1/1], Step [13/13], d_loss: 1.2121, g_loss: 0.6912 \n",
      "\n",
      "Epoch [2/2], Step [13/13], d_loss: 1.1010, g_loss: 0.6921 \n",
      "\n",
      "Epoch [3/3], Step [13/13], d_loss: 1.1430, g_loss: 0.6938 \n",
      "\n",
      "Epoch [4/4], Step [13/13], d_loss: 1.1481, g_loss: 0.6945 \n",
      "\n",
      "Epoch [5/5], Step [13/13], d_loss: 1.1160, g_loss: 0.6953 \n",
      "\n",
      "Epoch [6/6], Step [13/13], d_loss: 1.1277, g_loss: 0.6958 \n",
      "\n",
      "Epoch [7/7], Step [13/13], d_loss: 1.1187, g_loss: 0.6966 \n",
      "\n",
      "Epoch [8/8], Step [13/13], d_loss: 1.1920, g_loss: 0.6974 \n",
      "\n",
      "Epoch [9/9], Step [13/13], d_loss: 1.0963, g_loss: 0.6984 \n",
      "\n",
      "Epoch [10/10], Step [13/13], d_loss: 1.1380, g_loss: 0.6987 \n",
      "\n",
      "Epoch [11/11], Step [13/13], d_loss: 1.0693, g_loss: 0.6994 \n",
      "\n",
      "Epoch [12/12], Step [13/13], d_loss: 1.0972, g_loss: 0.7001 \n",
      "\n",
      "Epoch [13/13], Step [13/13], d_loss: 1.0951, g_loss: 0.7004 \n",
      "\n",
      "Epoch [14/14], Step [13/13], d_loss: 1.1166, g_loss: 0.7004 \n",
      "\n",
      "Epoch [15/15], Step [13/13], d_loss: 1.0947, g_loss: 0.7003 \n",
      "\n",
      "Epoch [16/16], Step [13/13], d_loss: 1.1041, g_loss: 0.7000 \n",
      "\n",
      "Epoch [17/17], Step [13/13], d_loss: 1.1083, g_loss: 0.7009 \n",
      "\n",
      "Epoch [18/18], Step [13/13], d_loss: 1.0925, g_loss: 0.7007 \n",
      "\n",
      "Epoch [19/19], Step [13/13], d_loss: 1.1139, g_loss: 0.7007 \n",
      "\n",
      "Epoch [20/20], Step [13/13], d_loss: 1.1200, g_loss: 0.7006 \n",
      "\n",
      "Epoch [21/21], Step [13/13], d_loss: 1.0627, g_loss: 0.7007 \n",
      "\n",
      "Epoch [22/22], Step [13/13], d_loss: 1.1364, g_loss: 0.7009 \n",
      "\n",
      "Epoch [23/23], Step [13/13], d_loss: 1.1691, g_loss: 0.7008 \n",
      "\n",
      "Epoch [24/24], Step [13/13], d_loss: 1.0994, g_loss: 0.7006 \n",
      "\n",
      "Epoch [25/25], Step [13/13], d_loss: 1.1483, g_loss: 0.7008 \n",
      "\n",
      "Epoch [26/26], Step [13/13], d_loss: 1.1427, g_loss: 0.7003 \n",
      "\n",
      "Epoch [27/27], Step [13/13], d_loss: 1.1783, g_loss: 0.7004 \n",
      "\n",
      "Epoch [28/28], Step [13/13], d_loss: 1.1401, g_loss: 0.7006 \n",
      "\n",
      "Epoch [29/29], Step [13/13], d_loss: 1.0838, g_loss: 0.7006 \n",
      "\n",
      "Epoch [30/30], Step [13/13], d_loss: 1.0931, g_loss: 0.7007 \n",
      "\n",
      "Epoch [31/31], Step [13/13], d_loss: 1.0539, g_loss: 0.7005 \n",
      "\n",
      "Epoch [32/32], Step [13/13], d_loss: 1.0999, g_loss: 0.7009 \n",
      "\n",
      "Epoch [33/33], Step [13/13], d_loss: 1.0958, g_loss: 0.7004 \n",
      "\n",
      "Epoch [34/34], Step [13/13], d_loss: 1.1255, g_loss: 0.7004 \n",
      "\n",
      "Epoch [35/35], Step [13/13], d_loss: 1.1080, g_loss: 0.7011 \n",
      "\n",
      "Epoch [36/36], Step [13/13], d_loss: 1.1157, g_loss: 0.7007 \n",
      "\n",
      "Epoch [37/37], Step [13/13], d_loss: 1.1475, g_loss: 0.7008 \n",
      "\n",
      "Epoch [38/38], Step [13/13], d_loss: 1.0901, g_loss: 0.7006 \n",
      "\n",
      "Epoch [39/39], Step [13/13], d_loss: 1.1028, g_loss: 0.7001 \n",
      "\n",
      "Epoch [40/40], Step [13/13], d_loss: 1.0429, g_loss: 0.7007 \n",
      "\n",
      "Epoch [41/41], Step [13/13], d_loss: 1.0581, g_loss: 0.7010 \n",
      "\n",
      "Epoch [42/42], Step [13/13], d_loss: 1.1544, g_loss: 0.7005 \n",
      "\n",
      "Epoch [43/43], Step [13/13], d_loss: 1.1108, g_loss: 0.7002 \n",
      "\n",
      "Epoch [44/44], Step [13/13], d_loss: 1.1438, g_loss: 0.7006 \n",
      "\n",
      "Epoch [45/45], Step [13/13], d_loss: 1.1084, g_loss: 0.7003 \n",
      "\n",
      "Epoch [46/46], Step [13/13], d_loss: 1.1272, g_loss: 0.7009 \n",
      "\n",
      "Epoch [47/47], Step [13/13], d_loss: 1.1530, g_loss: 0.7005 \n",
      "\n",
      "Epoch [48/48], Step [13/13], d_loss: 1.0811, g_loss: 0.7005 \n",
      "\n",
      "Epoch [49/49], Step [13/13], d_loss: 1.0757, g_loss: 0.7005 \n",
      "\n",
      "Epoch [50/50], Step [13/13], d_loss: 1.0958, g_loss: 0.7007 \n",
      "\n",
      "Epoch [51/51], Step [13/13], d_loss: 1.0949, g_loss: 0.7008 \n",
      "\n",
      "Epoch [52/52], Step [13/13], d_loss: 1.1154, g_loss: 0.7005 \n",
      "\n",
      "Epoch [53/53], Step [13/13], d_loss: 1.1092, g_loss: 0.7002 \n",
      "\n",
      "Epoch [54/54], Step [13/13], d_loss: 1.0559, g_loss: 0.7003 \n",
      "\n",
      "Epoch [55/55], Step [13/13], d_loss: 1.1088, g_loss: 0.7005 \n",
      "\n",
      "Epoch [56/56], Step [13/13], d_loss: 1.1361, g_loss: 0.7007 \n",
      "\n",
      "Epoch [57/57], Step [13/13], d_loss: 1.0889, g_loss: 0.7008 \n",
      "\n",
      "Epoch [58/58], Step [13/13], d_loss: 1.0951, g_loss: 0.7005 \n",
      "\n",
      "Epoch [59/59], Step [13/13], d_loss: 1.0854, g_loss: 0.7004 \n",
      "\n",
      "Epoch [60/60], Step [13/13], d_loss: 1.1192, g_loss: 0.7009 \n",
      "\n",
      "Epoch [61/61], Step [13/13], d_loss: 1.1486, g_loss: 0.7011 \n",
      "\n",
      "Epoch [62/62], Step [13/13], d_loss: 1.1367, g_loss: 0.7003 \n",
      "\n",
      "Epoch [63/63], Step [13/13], d_loss: 1.1835, g_loss: 0.7009 \n",
      "\n",
      "Epoch [64/64], Step [13/13], d_loss: 1.0889, g_loss: 0.7004 \n",
      "\n",
      "Epoch [65/65], Step [13/13], d_loss: 1.0873, g_loss: 0.7010 \n",
      "\n",
      "Epoch [66/66], Step [13/13], d_loss: 1.1106, g_loss: 0.7004 \n",
      "\n",
      "Epoch [67/67], Step [13/13], d_loss: 1.0907, g_loss: 0.7004 \n",
      "\n",
      "Epoch [68/68], Step [13/13], d_loss: 1.1388, g_loss: 0.7009 \n",
      "\n",
      "Epoch [69/69], Step [13/13], d_loss: 1.1247, g_loss: 0.7008 \n",
      "\n",
      "Epoch [70/70], Step [13/13], d_loss: 1.1169, g_loss: 0.7008 \n",
      "\n",
      "Epoch [71/71], Step [13/13], d_loss: 1.1162, g_loss: 0.7004 \n",
      "\n",
      "Epoch [72/72], Step [13/13], d_loss: 1.1116, g_loss: 0.7008 \n",
      "\n",
      "Epoch [73/73], Step [13/13], d_loss: 1.1101, g_loss: 0.7005 \n",
      "\n",
      "Epoch [74/74], Step [13/13], d_loss: 1.0900, g_loss: 0.7002 \n",
      "\n",
      "Epoch [75/75], Step [13/13], d_loss: 1.1130, g_loss: 0.7011 \n",
      "\n",
      "Epoch [76/76], Step [13/13], d_loss: 1.0451, g_loss: 0.7004 \n",
      "\n",
      "Epoch [77/77], Step [13/13], d_loss: 1.0504, g_loss: 0.7001 \n",
      "\n",
      "Epoch [78/78], Step [13/13], d_loss: 1.0564, g_loss: 0.7006 \n",
      "\n",
      "Epoch [79/79], Step [13/13], d_loss: 1.1819, g_loss: 0.7003 \n",
      "\n",
      "Epoch [80/80], Step [13/13], d_loss: 1.1186, g_loss: 0.7004 \n",
      "\n",
      "Epoch [81/81], Step [13/13], d_loss: 1.0982, g_loss: 0.7005 \n",
      "\n",
      "Epoch [82/82], Step [13/13], d_loss: 1.1268, g_loss: 0.7007 \n",
      "\n",
      "Epoch [83/83], Step [13/13], d_loss: 1.0776, g_loss: 0.7007 \n",
      "\n",
      "Epoch [84/84], Step [13/13], d_loss: 1.0840, g_loss: 0.7004 \n",
      "\n",
      "Epoch [85/85], Step [13/13], d_loss: 1.0668, g_loss: 0.7002 \n",
      "\n",
      "Epoch [86/86], Step [13/13], d_loss: 1.1509, g_loss: 0.7008 \n",
      "\n",
      "Epoch [87/87], Step [13/13], d_loss: 1.1054, g_loss: 0.7005 \n",
      "\n",
      "Epoch [88/88], Step [13/13], d_loss: 1.0429, g_loss: 0.7006 \n",
      "\n",
      "Epoch [89/89], Step [13/13], d_loss: 1.1633, g_loss: 0.7008 \n",
      "\n",
      "Epoch [90/90], Step [13/13], d_loss: 1.1175, g_loss: 0.7004 \n",
      "\n",
      "Epoch [91/91], Step [13/13], d_loss: 1.0756, g_loss: 0.7008 \n",
      "\n",
      "Epoch [92/92], Step [13/13], d_loss: 1.0889, g_loss: 0.7000 \n",
      "\n",
      "Epoch [93/93], Step [13/13], d_loss: 1.0657, g_loss: 0.7003 \n",
      "\n",
      "Epoch [94/94], Step [13/13], d_loss: 1.0656, g_loss: 0.7005 \n",
      "\n",
      "Epoch [95/95], Step [13/13], d_loss: 1.1277, g_loss: 0.7007 \n",
      "\n",
      "Epoch [96/96], Step [13/13], d_loss: 1.1770, g_loss: 0.7011 \n",
      "\n",
      "Epoch [97/97], Step [13/13], d_loss: 1.0620, g_loss: 0.7010 \n",
      "\n",
      "Epoch [98/98], Step [13/13], d_loss: 1.0970, g_loss: 0.7009 \n",
      "\n",
      "Epoch [99/99], Step [13/13], d_loss: 1.1883, g_loss: 0.7003 \n",
      "\n",
      "Epoch [100/100], Step [13/13], d_loss: 1.0976, g_loss: 0.7004 \n",
      "\n",
      "Epoch [101/101], Step [13/13], d_loss: 1.1102, g_loss: 0.7005 \n",
      "\n",
      "Epoch [102/102], Step [13/13], d_loss: 1.1081, g_loss: 0.7008 \n",
      "\n",
      "Epoch [103/103], Step [13/13], d_loss: 1.0617, g_loss: 0.7005 \n",
      "\n",
      "Epoch [104/104], Step [13/13], d_loss: 1.0949, g_loss: 0.7008 \n",
      "\n",
      "Epoch [105/105], Step [13/13], d_loss: 1.0746, g_loss: 0.7005 \n",
      "\n",
      "Epoch [106/106], Step [13/13], d_loss: 1.1434, g_loss: 0.7011 \n",
      "\n",
      "Epoch [107/107], Step [13/13], d_loss: 1.1103, g_loss: 0.7008 \n",
      "\n",
      "Epoch [108/108], Step [13/13], d_loss: 1.0637, g_loss: 0.7007 \n",
      "\n",
      "Epoch [109/109], Step [13/13], d_loss: 1.0913, g_loss: 0.7005 \n",
      "\n",
      "Epoch [110/110], Step [13/13], d_loss: 1.1408, g_loss: 0.7000 \n",
      "\n",
      "Epoch [111/111], Step [13/13], d_loss: 1.1226, g_loss: 0.7007 \n",
      "\n",
      "Epoch [112/112], Step [13/13], d_loss: 1.1184, g_loss: 0.7009 \n",
      "\n",
      "Epoch [113/113], Step [13/13], d_loss: 1.0882, g_loss: 0.7009 \n",
      "\n",
      "Epoch [114/114], Step [13/13], d_loss: 1.0831, g_loss: 0.7004 \n",
      "\n",
      "Epoch [115/115], Step [13/13], d_loss: 1.0676, g_loss: 0.7004 \n",
      "\n",
      "Epoch [116/116], Step [13/13], d_loss: 1.0602, g_loss: 0.7006 \n",
      "\n",
      "Epoch [117/117], Step [13/13], d_loss: 1.0746, g_loss: 0.7006 \n",
      "\n",
      "Epoch [118/118], Step [13/13], d_loss: 1.1271, g_loss: 0.7007 \n",
      "\n",
      "Epoch [119/119], Step [13/13], d_loss: 1.1125, g_loss: 0.7004 \n",
      "\n",
      "Epoch [120/120], Step [13/13], d_loss: 1.0966, g_loss: 0.7009 \n",
      "\n",
      "Epoch [121/121], Step [13/13], d_loss: 1.1097, g_loss: 0.7009 \n",
      "\n",
      "Epoch [122/122], Step [13/13], d_loss: 1.1103, g_loss: 0.7003 \n",
      "\n",
      "Epoch [123/123], Step [13/13], d_loss: 1.1567, g_loss: 0.7003 \n",
      "\n",
      "Epoch [124/124], Step [13/13], d_loss: 1.0920, g_loss: 0.7005 \n",
      "\n",
      "Epoch [125/125], Step [13/13], d_loss: 1.0977, g_loss: 0.7002 \n",
      "\n",
      "Epoch [126/126], Step [13/13], d_loss: 1.0835, g_loss: 0.7007 \n",
      "\n",
      "Epoch [127/127], Step [13/13], d_loss: 1.0995, g_loss: 0.7006 \n",
      "\n",
      "Epoch [128/128], Step [13/13], d_loss: 1.1415, g_loss: 0.7007 \n",
      "\n",
      "Epoch [129/129], Step [13/13], d_loss: 1.0857, g_loss: 0.7006 \n",
      "\n",
      "Epoch [130/130], Step [13/13], d_loss: 1.1194, g_loss: 0.7007 \n",
      "\n",
      "Epoch [131/131], Step [13/13], d_loss: 1.1189, g_loss: 0.7007 \n",
      "\n",
      "Epoch [132/132], Step [13/13], d_loss: 1.0859, g_loss: 0.7007 \n",
      "\n",
      "Epoch [133/133], Step [13/13], d_loss: 1.1353, g_loss: 0.7001 \n",
      "\n",
      "Epoch [134/134], Step [13/13], d_loss: 1.1231, g_loss: 0.7000 \n",
      "\n",
      "Epoch [135/135], Step [13/13], d_loss: 1.1411, g_loss: 0.7005 \n",
      "\n",
      "Epoch [136/136], Step [13/13], d_loss: 1.0794, g_loss: 0.7008 \n",
      "\n",
      "Epoch [137/137], Step [13/13], d_loss: 1.1156, g_loss: 0.7010 \n",
      "\n",
      "Epoch [138/138], Step [13/13], d_loss: 1.0631, g_loss: 0.7010 \n",
      "\n",
      "Epoch [139/139], Step [13/13], d_loss: 1.1001, g_loss: 0.7004 \n",
      "\n",
      "Epoch [140/140], Step [13/13], d_loss: 1.0861, g_loss: 0.7005 \n",
      "\n",
      "Epoch [141/141], Step [13/13], d_loss: 1.1243, g_loss: 0.7007 \n",
      "\n",
      "Epoch [142/142], Step [13/13], d_loss: 1.0683, g_loss: 0.7003 \n",
      "\n",
      "Epoch [143/143], Step [13/13], d_loss: 1.0769, g_loss: 0.7009 \n",
      "\n",
      "Epoch [144/144], Step [13/13], d_loss: 1.1119, g_loss: 0.7009 \n",
      "\n",
      "Epoch [145/145], Step [13/13], d_loss: 1.0751, g_loss: 0.7007 \n",
      "\n",
      "Epoch [146/146], Step [13/13], d_loss: 1.0917, g_loss: 0.7006 \n",
      "\n",
      "Epoch [147/147], Step [13/13], d_loss: 1.0706, g_loss: 0.7005 \n",
      "\n",
      "Epoch [148/148], Step [13/13], d_loss: 1.0826, g_loss: 0.7005 \n",
      "\n",
      "Epoch [149/149], Step [13/13], d_loss: 1.0767, g_loss: 0.7000 \n",
      "\n",
      "Epoch [150/150], Step [13/13], d_loss: 1.1036, g_loss: 0.7011 \n",
      "\n",
      "Epoch [151/151], Step [13/13], d_loss: 1.0737, g_loss: 0.7001 \n",
      "\n",
      "Epoch [152/152], Step [13/13], d_loss: 1.0837, g_loss: 0.7006 \n",
      "\n",
      "Epoch [153/153], Step [13/13], d_loss: 1.1424, g_loss: 0.7006 \n",
      "\n",
      "Epoch [154/154], Step [13/13], d_loss: 1.1595, g_loss: 0.7004 \n",
      "\n",
      "Epoch [155/155], Step [13/13], d_loss: 1.1095, g_loss: 0.7006 \n",
      "\n",
      "Epoch [156/156], Step [13/13], d_loss: 1.0894, g_loss: 0.7002 \n",
      "\n",
      "Epoch [157/157], Step [13/13], d_loss: 1.0529, g_loss: 0.7008 \n",
      "\n",
      "Epoch [158/158], Step [13/13], d_loss: 1.1015, g_loss: 0.7005 \n",
      "\n",
      "Epoch [159/159], Step [13/13], d_loss: 1.1398, g_loss: 0.7008 \n",
      "\n",
      "Epoch [160/160], Step [13/13], d_loss: 1.0844, g_loss: 0.7004 \n",
      "\n",
      "Epoch [161/161], Step [13/13], d_loss: 1.0718, g_loss: 0.7010 \n",
      "\n",
      "Epoch [162/162], Step [13/13], d_loss: 1.1182, g_loss: 0.7006 \n",
      "\n",
      "Epoch [163/163], Step [13/13], d_loss: 1.1205, g_loss: 0.7006 \n",
      "\n",
      "Epoch [164/164], Step [13/13], d_loss: 1.1025, g_loss: 0.7004 \n",
      "\n",
      "Epoch [165/165], Step [13/13], d_loss: 1.0488, g_loss: 0.7004 \n",
      "\n",
      "Epoch [166/166], Step [13/13], d_loss: 1.1332, g_loss: 0.7007 \n",
      "\n",
      "Epoch [167/167], Step [13/13], d_loss: 1.0950, g_loss: 0.7006 \n",
      "\n",
      "Epoch [168/168], Step [13/13], d_loss: 1.0643, g_loss: 0.7008 \n",
      "\n",
      "Epoch [169/169], Step [13/13], d_loss: 1.0706, g_loss: 0.7004 \n",
      "\n",
      "Epoch [170/170], Step [13/13], d_loss: 1.1659, g_loss: 0.7005 \n",
      "\n",
      "Epoch [171/171], Step [13/13], d_loss: 1.1047, g_loss: 0.7009 \n",
      "\n",
      "Epoch [172/172], Step [13/13], d_loss: 1.0630, g_loss: 0.7009 \n",
      "\n",
      "Epoch [173/173], Step [13/13], d_loss: 1.1420, g_loss: 0.7011 \n",
      "\n",
      "Epoch [174/174], Step [13/13], d_loss: 1.1257, g_loss: 0.7005 \n",
      "\n",
      "Epoch [175/175], Step [13/13], d_loss: 1.1189, g_loss: 0.7007 \n",
      "\n",
      "Epoch [176/176], Step [13/13], d_loss: 1.1407, g_loss: 0.7002 \n",
      "\n",
      "Epoch [177/177], Step [13/13], d_loss: 1.0590, g_loss: 0.7004 \n",
      "\n",
      "Epoch [178/178], Step [13/13], d_loss: 1.0527, g_loss: 0.7008 \n",
      "\n",
      "Epoch [179/179], Step [13/13], d_loss: 1.1218, g_loss: 0.7008 \n",
      "\n",
      "Epoch [180/180], Step [13/13], d_loss: 1.1238, g_loss: 0.7008 \n",
      "\n",
      "Epoch [181/181], Step [13/13], d_loss: 1.0961, g_loss: 0.7012 \n",
      "\n",
      "Epoch [182/182], Step [13/13], d_loss: 1.1273, g_loss: 0.7009 \n",
      "\n",
      "Epoch [183/183], Step [13/13], d_loss: 1.1247, g_loss: 0.7000 \n",
      "\n",
      "Epoch [184/184], Step [13/13], d_loss: 1.0459, g_loss: 0.7008 \n",
      "\n",
      "Epoch [185/185], Step [13/13], d_loss: 1.0921, g_loss: 0.7010 \n",
      "\n",
      "Epoch [186/186], Step [13/13], d_loss: 1.1705, g_loss: 0.7009 \n",
      "\n",
      "Epoch [187/187], Step [13/13], d_loss: 1.0992, g_loss: 0.7010 \n",
      "\n",
      "Epoch [188/188], Step [13/13], d_loss: 1.0674, g_loss: 0.7009 \n",
      "\n",
      "Epoch [189/189], Step [13/13], d_loss: 1.1218, g_loss: 0.7010 \n",
      "\n",
      "Epoch [190/190], Step [13/13], d_loss: 1.0882, g_loss: 0.7007 \n",
      "\n",
      "Epoch [191/191], Step [13/13], d_loss: 1.0956, g_loss: 0.7002 \n",
      "\n",
      "Epoch [192/192], Step [13/13], d_loss: 1.1008, g_loss: 0.7007 \n",
      "\n",
      "Epoch [193/193], Step [13/13], d_loss: 1.1125, g_loss: 0.7010 \n",
      "\n",
      "Epoch [194/194], Step [13/13], d_loss: 1.0650, g_loss: 0.7008 \n",
      "\n",
      "Epoch [195/195], Step [13/13], d_loss: 1.1455, g_loss: 0.7006 \n",
      "\n",
      "Epoch [196/196], Step [13/13], d_loss: 1.0551, g_loss: 0.7004 \n",
      "\n",
      "Epoch [197/197], Step [13/13], d_loss: 1.0780, g_loss: 0.7011 \n",
      "\n",
      "Epoch [198/198], Step [13/13], d_loss: 1.1237, g_loss: 0.7005 \n",
      "\n",
      "Epoch [199/199], Step [13/13], d_loss: 1.1145, g_loss: 0.7006 \n",
      "\n",
      "Epoch [200/200], Step [13/13], d_loss: 1.1074, g_loss: 0.7007 \n",
      "\n",
      "Epoch [201/201], Step [13/13], d_loss: 1.0523, g_loss: 0.7012 \n",
      "\n",
      "Epoch [202/202], Step [13/13], d_loss: 1.0482, g_loss: 0.7009 \n",
      "\n",
      "Epoch [203/203], Step [13/13], d_loss: 1.1506, g_loss: 0.7003 \n",
      "\n",
      "Epoch [204/204], Step [13/13], d_loss: 1.0886, g_loss: 0.7006 \n",
      "\n",
      "Epoch [205/205], Step [13/13], d_loss: 1.0659, g_loss: 0.7005 \n",
      "\n",
      "Epoch [206/206], Step [13/13], d_loss: 1.1205, g_loss: 0.7009 \n",
      "\n",
      "Epoch [207/207], Step [13/13], d_loss: 1.0869, g_loss: 0.7006 \n",
      "\n",
      "Epoch [208/208], Step [13/13], d_loss: 1.0770, g_loss: 0.7009 \n",
      "\n",
      "Epoch [209/209], Step [13/13], d_loss: 1.1381, g_loss: 0.7007 \n",
      "\n",
      "Epoch [210/210], Step [13/13], d_loss: 1.1108, g_loss: 0.7006 \n",
      "\n",
      "Epoch [211/211], Step [13/13], d_loss: 1.0601, g_loss: 0.7006 \n",
      "\n",
      "Epoch [212/212], Step [13/13], d_loss: 1.1251, g_loss: 0.7006 \n",
      "\n",
      "Epoch [213/213], Step [13/13], d_loss: 1.1231, g_loss: 0.7006 \n",
      "\n",
      "Epoch [214/214], Step [13/13], d_loss: 1.0982, g_loss: 0.7006 \n",
      "\n",
      "Epoch [215/215], Step [13/13], d_loss: 1.1166, g_loss: 0.7005 \n",
      "\n",
      "Epoch [216/216], Step [13/13], d_loss: 1.0629, g_loss: 0.7009 \n",
      "\n",
      "Epoch [217/217], Step [13/13], d_loss: 1.0745, g_loss: 0.7003 \n",
      "\n",
      "Epoch [218/218], Step [13/13], d_loss: 1.0769, g_loss: 0.7006 \n",
      "\n",
      "Epoch [219/219], Step [13/13], d_loss: 1.0954, g_loss: 0.7009 \n",
      "\n",
      "Epoch [220/220], Step [13/13], d_loss: 1.0999, g_loss: 0.7009 \n",
      "\n",
      "Epoch [221/221], Step [13/13], d_loss: 1.0619, g_loss: 0.7004 \n",
      "\n",
      "Epoch [222/222], Step [13/13], d_loss: 1.0928, g_loss: 0.7005 \n",
      "\n",
      "Epoch [223/223], Step [13/13], d_loss: 1.0884, g_loss: 0.7006 \n",
      "\n",
      "Epoch [224/224], Step [13/13], d_loss: 1.0935, g_loss: 0.7008 \n",
      "\n",
      "Epoch [225/225], Step [13/13], d_loss: 1.0691, g_loss: 0.7004 \n",
      "\n",
      "Epoch [226/226], Step [13/13], d_loss: 1.0900, g_loss: 0.7005 \n",
      "\n",
      "Epoch [227/227], Step [13/13], d_loss: 1.1475, g_loss: 0.7009 \n",
      "\n",
      "Epoch [228/228], Step [13/13], d_loss: 1.1266, g_loss: 0.7006 \n",
      "\n",
      "Epoch [229/229], Step [13/13], d_loss: 1.0301, g_loss: 0.7009 \n",
      "\n",
      "Epoch [230/230], Step [13/13], d_loss: 1.0754, g_loss: 0.7008 \n",
      "\n",
      "Epoch [231/231], Step [13/13], d_loss: 1.1108, g_loss: 0.7006 \n",
      "\n",
      "Epoch [232/232], Step [13/13], d_loss: 1.0945, g_loss: 0.7008 \n",
      "\n",
      "Epoch [233/233], Step [13/13], d_loss: 1.0934, g_loss: 0.7010 \n",
      "\n",
      "Epoch [234/234], Step [13/13], d_loss: 1.1005, g_loss: 0.7003 \n",
      "\n",
      "Epoch [235/235], Step [13/13], d_loss: 1.0666, g_loss: 0.7006 \n",
      "\n",
      "Epoch [236/236], Step [13/13], d_loss: 1.0964, g_loss: 0.7007 \n",
      "\n",
      "Epoch [237/237], Step [13/13], d_loss: 1.1070, g_loss: 0.7003 \n",
      "\n",
      "Epoch [238/238], Step [13/13], d_loss: 1.0496, g_loss: 0.7008 \n",
      "\n",
      "Epoch [239/239], Step [13/13], d_loss: 1.1020, g_loss: 0.7005 \n",
      "\n",
      "Epoch [240/240], Step [13/13], d_loss: 1.0922, g_loss: 0.7005 \n",
      "\n",
      "Epoch [241/241], Step [13/13], d_loss: 1.0656, g_loss: 0.7005 \n",
      "\n",
      "Epoch [242/242], Step [13/13], d_loss: 1.0917, g_loss: 0.7004 \n",
      "\n",
      "Epoch [243/243], Step [13/13], d_loss: 1.0751, g_loss: 0.7007 \n",
      "\n",
      "Epoch [244/244], Step [13/13], d_loss: 1.1467, g_loss: 0.7006 \n",
      "\n",
      "Epoch [245/245], Step [13/13], d_loss: 1.0640, g_loss: 0.7001 \n",
      "\n",
      "Epoch [246/246], Step [13/13], d_loss: 1.1031, g_loss: 0.7009 \n",
      "\n",
      "Epoch [247/247], Step [13/13], d_loss: 1.0841, g_loss: 0.7006 \n",
      "\n",
      "Epoch [248/248], Step [13/13], d_loss: 1.0670, g_loss: 0.7010 \n",
      "\n",
      "Epoch [249/249], Step [13/13], d_loss: 1.1433, g_loss: 0.7003 \n",
      "\n",
      "Epoch [250/250], Step [13/13], d_loss: 1.0739, g_loss: 0.7009 \n",
      "\n",
      "Epoch [251/251], Step [13/13], d_loss: 1.0837, g_loss: 0.7002 \n",
      "\n",
      "Epoch [252/252], Step [13/13], d_loss: 1.0874, g_loss: 0.7004 \n",
      "\n",
      "Epoch [253/253], Step [13/13], d_loss: 1.0979, g_loss: 0.7006 \n",
      "\n",
      "Epoch [254/254], Step [13/13], d_loss: 1.0617, g_loss: 0.7009 \n",
      "\n",
      "Epoch [255/255], Step [13/13], d_loss: 1.1021, g_loss: 0.7003 \n",
      "\n",
      "Epoch [256/256], Step [13/13], d_loss: 1.1058, g_loss: 0.7009 \n",
      "\n",
      "Epoch [257/257], Step [13/13], d_loss: 1.1133, g_loss: 0.7005 \n",
      "\n",
      "Epoch [258/258], Step [13/13], d_loss: 1.1305, g_loss: 0.7002 \n",
      "\n",
      "Epoch [259/259], Step [13/13], d_loss: 1.1540, g_loss: 0.7004 \n",
      "\n",
      "Epoch [260/260], Step [13/13], d_loss: 1.0836, g_loss: 0.7006 \n",
      "\n",
      "Epoch [261/261], Step [13/13], d_loss: 1.0717, g_loss: 0.7004 \n",
      "\n",
      "Epoch [262/262], Step [13/13], d_loss: 1.0794, g_loss: 0.7006 \n",
      "\n",
      "Epoch [263/263], Step [13/13], d_loss: 1.1045, g_loss: 0.7007 \n",
      "\n",
      "Epoch [264/264], Step [13/13], d_loss: 1.1272, g_loss: 0.7005 \n",
      "\n",
      "Epoch [265/265], Step [13/13], d_loss: 1.1153, g_loss: 0.7004 \n",
      "\n",
      "Epoch [266/266], Step [13/13], d_loss: 1.1269, g_loss: 0.7009 \n",
      "\n",
      "Epoch [267/267], Step [13/13], d_loss: 1.0744, g_loss: 0.7010 \n",
      "\n",
      "Epoch [268/268], Step [13/13], d_loss: 1.0680, g_loss: 0.7008 \n",
      "\n",
      "Epoch [269/269], Step [13/13], d_loss: 1.0830, g_loss: 0.7004 \n",
      "\n",
      "Epoch [270/270], Step [13/13], d_loss: 1.0716, g_loss: 0.7005 \n",
      "\n",
      "Epoch [271/271], Step [13/13], d_loss: 1.1301, g_loss: 0.7003 \n",
      "\n",
      "Epoch [272/272], Step [13/13], d_loss: 1.0800, g_loss: 0.7008 \n",
      "\n",
      "Epoch [273/273], Step [13/13], d_loss: 1.0901, g_loss: 0.7005 \n",
      "\n",
      "Epoch [274/274], Step [13/13], d_loss: 1.1302, g_loss: 0.7001 \n",
      "\n",
      "Epoch [275/275], Step [13/13], d_loss: 1.0910, g_loss: 0.7001 \n",
      "\n",
      "Epoch [276/276], Step [13/13], d_loss: 1.1380, g_loss: 0.7009 \n",
      "\n",
      "Epoch [277/277], Step [13/13], d_loss: 1.1054, g_loss: 0.7010 \n",
      "\n",
      "Epoch [278/278], Step [13/13], d_loss: 1.0866, g_loss: 0.7006 \n",
      "\n",
      "Epoch [279/279], Step [13/13], d_loss: 1.0703, g_loss: 0.7004 \n",
      "\n",
      "Epoch [280/280], Step [13/13], d_loss: 1.1438, g_loss: 0.7006 \n",
      "\n",
      "Epoch [281/281], Step [13/13], d_loss: 1.1225, g_loss: 0.7006 \n",
      "\n",
      "Epoch [282/282], Step [13/13], d_loss: 1.1374, g_loss: 0.7008 \n",
      "\n",
      "Epoch [283/283], Step [13/13], d_loss: 1.1377, g_loss: 0.7010 \n",
      "\n",
      "Epoch [284/284], Step [13/13], d_loss: 1.0725, g_loss: 0.7006 \n",
      "\n",
      "Epoch [285/285], Step [13/13], d_loss: 1.1172, g_loss: 0.7004 \n",
      "\n",
      "Epoch [286/286], Step [13/13], d_loss: 1.0807, g_loss: 0.7004 \n",
      "\n",
      "Epoch [287/287], Step [13/13], d_loss: 1.0898, g_loss: 0.7005 \n",
      "\n",
      "Epoch [288/288], Step [13/13], d_loss: 1.0372, g_loss: 0.7000 \n",
      "\n",
      "Epoch [289/289], Step [13/13], d_loss: 1.1474, g_loss: 0.7007 \n",
      "\n",
      "Epoch [290/290], Step [13/13], d_loss: 1.0831, g_loss: 0.7006 \n",
      "\n",
      "Epoch [291/291], Step [13/13], d_loss: 1.1470, g_loss: 0.7007 \n",
      "\n",
      "Epoch [292/292], Step [13/13], d_loss: 1.1129, g_loss: 0.7006 \n",
      "\n",
      "Epoch [293/293], Step [13/13], d_loss: 1.0769, g_loss: 0.7007 \n",
      "\n",
      "Epoch [294/294], Step [13/13], d_loss: 1.1071, g_loss: 0.7007 \n",
      "\n",
      "Epoch [295/295], Step [13/13], d_loss: 1.0948, g_loss: 0.7008 \n",
      "\n",
      "Epoch [296/296], Step [13/13], d_loss: 1.1094, g_loss: 0.7004 \n",
      "\n",
      "Epoch [297/297], Step [13/13], d_loss: 1.0926, g_loss: 0.7004 \n",
      "\n",
      "Epoch [298/298], Step [13/13], d_loss: 1.0837, g_loss: 0.7007 \n",
      "\n",
      "Epoch [299/299], Step [13/13], d_loss: 1.1014, g_loss: 0.7007 \n",
      "\n",
      "Epoch [300/300], Step [13/13], d_loss: 1.0997, g_loss: 0.7005 \n",
      "\n",
      "Epoch [301/301], Step [13/13], d_loss: 1.0405, g_loss: 0.7002 \n",
      "\n",
      "Epoch [302/302], Step [13/13], d_loss: 1.1009, g_loss: 0.7008 \n",
      "\n",
      "Epoch [303/303], Step [13/13], d_loss: 1.0922, g_loss: 0.7008 \n",
      "\n",
      "Epoch [304/304], Step [13/13], d_loss: 1.1244, g_loss: 0.7005 \n",
      "\n",
      "Epoch [305/305], Step [13/13], d_loss: 1.0938, g_loss: 0.7004 \n",
      "\n",
      "Epoch [306/306], Step [13/13], d_loss: 1.1540, g_loss: 0.7007 \n",
      "\n",
      "Epoch [307/307], Step [13/13], d_loss: 1.0688, g_loss: 0.7007 \n",
      "\n",
      "Epoch [308/308], Step [13/13], d_loss: 1.0584, g_loss: 0.7008 \n",
      "\n",
      "Epoch [309/309], Step [13/13], d_loss: 1.1417, g_loss: 0.7008 \n",
      "\n",
      "Epoch [310/310], Step [13/13], d_loss: 1.1158, g_loss: 0.7003 \n",
      "\n",
      "Epoch [311/311], Step [13/13], d_loss: 1.1381, g_loss: 0.7006 \n",
      "\n",
      "Epoch [312/312], Step [13/13], d_loss: 1.0490, g_loss: 0.7004 \n",
      "\n",
      "Epoch [313/313], Step [13/13], d_loss: 1.1566, g_loss: 0.7007 \n",
      "\n",
      "Epoch [314/314], Step [13/13], d_loss: 1.0850, g_loss: 0.7006 \n",
      "\n",
      "Epoch [315/315], Step [13/13], d_loss: 1.1758, g_loss: 0.7006 \n",
      "\n",
      "Epoch [316/316], Step [13/13], d_loss: 1.1081, g_loss: 0.7009 \n",
      "\n",
      "Epoch [317/317], Step [13/13], d_loss: 1.1326, g_loss: 0.7003 \n",
      "\n",
      "Epoch [318/318], Step [13/13], d_loss: 1.1061, g_loss: 0.7012 \n",
      "\n",
      "Epoch [319/319], Step [13/13], d_loss: 1.0836, g_loss: 0.7006 \n",
      "\n",
      "Epoch [320/320], Step [13/13], d_loss: 1.0762, g_loss: 0.7002 \n",
      "\n",
      "Epoch [321/321], Step [13/13], d_loss: 1.1018, g_loss: 0.7005 \n",
      "\n",
      "Epoch [322/322], Step [13/13], d_loss: 1.0571, g_loss: 0.7005 \n",
      "\n",
      "Epoch [323/323], Step [13/13], d_loss: 1.0625, g_loss: 0.7003 \n",
      "\n",
      "Epoch [324/324], Step [13/13], d_loss: 1.0875, g_loss: 0.7006 \n",
      "\n",
      "Epoch [325/325], Step [13/13], d_loss: 1.1415, g_loss: 0.7008 \n",
      "\n",
      "Epoch [326/326], Step [13/13], d_loss: 1.1656, g_loss: 0.7003 \n",
      "\n",
      "Epoch [327/327], Step [13/13], d_loss: 1.0944, g_loss: 0.7008 \n",
      "\n",
      "Epoch [328/328], Step [13/13], d_loss: 1.0987, g_loss: 0.7005 \n",
      "\n",
      "Epoch [329/329], Step [13/13], d_loss: 1.1236, g_loss: 0.7005 \n",
      "\n",
      "Epoch [330/330], Step [13/13], d_loss: 1.0585, g_loss: 0.7009 \n",
      "\n",
      "Epoch [331/331], Step [13/13], d_loss: 1.0518, g_loss: 0.7005 \n",
      "\n",
      "Epoch [332/332], Step [13/13], d_loss: 1.0912, g_loss: 0.7006 \n",
      "\n",
      "Epoch [333/333], Step [13/13], d_loss: 1.1271, g_loss: 0.7006 \n",
      "\n",
      "Epoch [334/334], Step [13/13], d_loss: 1.0846, g_loss: 0.7001 \n",
      "\n",
      "Epoch [335/335], Step [13/13], d_loss: 1.0844, g_loss: 0.7009 \n",
      "\n",
      "Epoch [336/336], Step [13/13], d_loss: 1.1143, g_loss: 0.7006 \n",
      "\n",
      "Epoch [337/337], Step [13/13], d_loss: 1.0913, g_loss: 0.7008 \n",
      "\n",
      "Epoch [338/338], Step [13/13], d_loss: 1.0636, g_loss: 0.7006 \n",
      "\n",
      "Epoch [339/339], Step [13/13], d_loss: 1.1343, g_loss: 0.7004 \n",
      "\n",
      "Epoch [340/340], Step [13/13], d_loss: 1.0806, g_loss: 0.7010 \n",
      "\n",
      "Epoch [341/341], Step [13/13], d_loss: 1.0859, g_loss: 0.7009 \n",
      "\n",
      "Epoch [342/342], Step [13/13], d_loss: 1.1358, g_loss: 0.7001 \n",
      "\n",
      "Epoch [343/343], Step [13/13], d_loss: 1.1215, g_loss: 0.7003 \n",
      "\n",
      "Epoch [344/344], Step [13/13], d_loss: 1.1198, g_loss: 0.7006 \n",
      "\n",
      "Epoch [345/345], Step [13/13], d_loss: 1.1141, g_loss: 0.7010 \n",
      "\n",
      "Epoch [346/346], Step [13/13], d_loss: 1.1218, g_loss: 0.7008 \n",
      "\n",
      "Epoch [347/347], Step [13/13], d_loss: 1.1324, g_loss: 0.7006 \n",
      "\n",
      "Epoch [348/348], Step [13/13], d_loss: 1.0942, g_loss: 0.7007 \n",
      "\n",
      "Epoch [349/349], Step [13/13], d_loss: 1.1228, g_loss: 0.7003 \n",
      "\n",
      "Epoch [350/350], Step [13/13], d_loss: 1.0816, g_loss: 0.7004 \n",
      "\n",
      "Epoch [351/351], Step [13/13], d_loss: 1.0744, g_loss: 0.7005 \n",
      "\n",
      "Epoch [352/352], Step [13/13], d_loss: 1.1405, g_loss: 0.7006 \n",
      "\n",
      "Epoch [353/353], Step [13/13], d_loss: 1.1304, g_loss: 0.7008 \n",
      "\n",
      "Epoch [354/354], Step [13/13], d_loss: 1.1176, g_loss: 0.7007 \n",
      "\n",
      "Epoch [355/355], Step [13/13], d_loss: 1.0958, g_loss: 0.7006 \n",
      "\n",
      "Epoch [356/356], Step [13/13], d_loss: 1.0912, g_loss: 0.7006 \n",
      "\n",
      "Epoch [357/357], Step [13/13], d_loss: 1.0946, g_loss: 0.7005 \n",
      "\n",
      "Epoch [358/358], Step [13/13], d_loss: 1.1031, g_loss: 0.7008 \n",
      "\n",
      "Epoch [359/359], Step [13/13], d_loss: 1.1318, g_loss: 0.7003 \n",
      "\n",
      "Epoch [360/360], Step [13/13], d_loss: 1.1283, g_loss: 0.7008 \n",
      "\n",
      "Epoch [361/361], Step [13/13], d_loss: 1.0811, g_loss: 0.7007 \n",
      "\n",
      "Epoch [362/362], Step [13/13], d_loss: 1.1019, g_loss: 0.7009 \n",
      "\n",
      "Epoch [363/363], Step [13/13], d_loss: 1.0958, g_loss: 0.7005 \n",
      "\n",
      "Epoch [364/364], Step [13/13], d_loss: 1.0910, g_loss: 0.7009 \n",
      "\n",
      "Epoch [365/365], Step [13/13], d_loss: 1.0513, g_loss: 0.7009 \n",
      "\n",
      "Epoch [366/366], Step [13/13], d_loss: 1.0857, g_loss: 0.7005 \n",
      "\n",
      "Epoch [367/367], Step [13/13], d_loss: 1.1612, g_loss: 0.7005 \n",
      "\n",
      "Epoch [368/368], Step [13/13], d_loss: 1.0992, g_loss: 0.7006 \n",
      "\n",
      "Epoch [369/369], Step [13/13], d_loss: 1.1094, g_loss: 0.7004 \n",
      "\n",
      "Epoch [370/370], Step [13/13], d_loss: 1.1334, g_loss: 0.7007 \n",
      "\n",
      "Epoch [371/371], Step [13/13], d_loss: 1.1284, g_loss: 0.7009 \n",
      "\n",
      "Epoch [372/372], Step [13/13], d_loss: 1.1321, g_loss: 0.7007 \n",
      "\n",
      "Epoch [373/373], Step [13/13], d_loss: 1.1953, g_loss: 0.7008 \n",
      "\n",
      "Epoch [374/374], Step [13/13], d_loss: 1.0593, g_loss: 0.7005 \n",
      "\n",
      "Epoch [375/375], Step [13/13], d_loss: 1.1482, g_loss: 0.7006 \n",
      "\n",
      "Epoch [376/376], Step [13/13], d_loss: 1.0980, g_loss: 0.7007 \n",
      "\n",
      "Epoch [377/377], Step [13/13], d_loss: 1.1297, g_loss: 0.7007 \n",
      "\n",
      "Epoch [378/378], Step [13/13], d_loss: 1.1136, g_loss: 0.7007 \n",
      "\n",
      "Epoch [379/379], Step [13/13], d_loss: 1.0915, g_loss: 0.7009 \n",
      "\n",
      "Epoch [380/380], Step [13/13], d_loss: 1.0641, g_loss: 0.7004 \n",
      "\n",
      "Epoch [381/381], Step [13/13], d_loss: 1.0920, g_loss: 0.7011 \n",
      "\n",
      "Epoch [382/382], Step [13/13], d_loss: 1.0819, g_loss: 0.7007 \n",
      "\n",
      "Epoch [383/383], Step [13/13], d_loss: 1.0970, g_loss: 0.7004 \n",
      "\n",
      "Epoch [384/384], Step [13/13], d_loss: 1.0760, g_loss: 0.7007 \n",
      "\n",
      "Epoch [385/385], Step [13/13], d_loss: 1.1251, g_loss: 0.7003 \n",
      "\n",
      "Epoch [386/386], Step [13/13], d_loss: 1.1683, g_loss: 0.7011 \n",
      "\n",
      "Epoch [387/387], Step [13/13], d_loss: 1.0888, g_loss: 0.7003 \n",
      "\n",
      "Epoch [388/388], Step [13/13], d_loss: 1.1117, g_loss: 0.7001 \n",
      "\n",
      "Epoch [389/389], Step [13/13], d_loss: 1.0566, g_loss: 0.7009 \n",
      "\n",
      "Epoch [390/390], Step [13/13], d_loss: 1.1257, g_loss: 0.7009 \n",
      "\n",
      "Epoch [391/391], Step [13/13], d_loss: 1.0566, g_loss: 0.7006 \n",
      "\n",
      "Epoch [392/392], Step [13/13], d_loss: 1.1111, g_loss: 0.7002 \n",
      "\n",
      "Epoch [393/393], Step [13/13], d_loss: 1.0866, g_loss: 0.7004 \n",
      "\n",
      "Epoch [394/394], Step [13/13], d_loss: 1.1294, g_loss: 0.7008 \n",
      "\n",
      "Epoch [395/395], Step [13/13], d_loss: 1.0860, g_loss: 0.7008 \n",
      "\n",
      "Epoch [396/396], Step [13/13], d_loss: 1.1263, g_loss: 0.7010 \n",
      "\n",
      "Epoch [397/397], Step [13/13], d_loss: 1.0924, g_loss: 0.7006 \n",
      "\n",
      "Epoch [398/398], Step [13/13], d_loss: 1.0789, g_loss: 0.7004 \n",
      "\n",
      "Epoch [399/399], Step [13/13], d_loss: 1.0525, g_loss: 0.7003 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "losses_gen, losses_dis = gan.train(df_train, epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_losses(losses_gen, losses_dis):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "\n",
    "    #convert losses into numpy\n",
    "    losses_gen = [i.detach().numpy() for i in losses_gen]\n",
    "    losses_dis = [i.detach().numpy() for i in losses_dis]\n",
    "    plt.plot(losses_gen,label=\"Generator\")\n",
    "    plt.plot(losses_dis,label=\"Discriminator\")\n",
    "    plt.xlabel(\"iterations\") #Epochs * Steps\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_losses(losses_gen, losses_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sample_number = df_train[df_train['Class'] == 0].shape[0]\n",
    "gan_samples = gan.sample(sample_number)\n",
    "samples_test = np.append(gan_samples, np.ones((sample_number, 1)), axis=1)\n",
    "gan_df = pd.DataFrame(samples_test, columns=df_train.columns)\n",
    "#concat wgan_df with df_train\n",
    "df_concat = pd.concat([df_train, gan_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>223361</th>\n",
       "      <td>143352.0</td>\n",
       "      <td>1.955041</td>\n",
       "      <td>-0.380783</td>\n",
       "      <td>-0.315013</td>\n",
       "      <td>0.330155</td>\n",
       "      <td>-0.509374</td>\n",
       "      <td>-0.086197</td>\n",
       "      <td>-0.627978</td>\n",
       "      <td>0.035994</td>\n",
       "      <td>1.054560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.238197</td>\n",
       "      <td>0.968305</td>\n",
       "      <td>0.053208</td>\n",
       "      <td>-0.278602</td>\n",
       "      <td>-0.044999</td>\n",
       "      <td>-0.216780</td>\n",
       "      <td>0.045168</td>\n",
       "      <td>-0.047145</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165061</th>\n",
       "      <td>117173.0</td>\n",
       "      <td>-0.400975</td>\n",
       "      <td>-0.626943</td>\n",
       "      <td>1.555339</td>\n",
       "      <td>-2.017772</td>\n",
       "      <td>-0.107769</td>\n",
       "      <td>0.168310</td>\n",
       "      <td>0.017959</td>\n",
       "      <td>-0.401619</td>\n",
       "      <td>0.040378</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.153485</td>\n",
       "      <td>0.421703</td>\n",
       "      <td>0.113442</td>\n",
       "      <td>-1.004095</td>\n",
       "      <td>-1.176695</td>\n",
       "      <td>0.361924</td>\n",
       "      <td>-0.370469</td>\n",
       "      <td>-0.144792</td>\n",
       "      <td>45.90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238186</th>\n",
       "      <td>149565.0</td>\n",
       "      <td>0.072509</td>\n",
       "      <td>0.820566</td>\n",
       "      <td>-0.561351</td>\n",
       "      <td>-0.709897</td>\n",
       "      <td>1.080399</td>\n",
       "      <td>-0.359429</td>\n",
       "      <td>0.787858</td>\n",
       "      <td>0.117276</td>\n",
       "      <td>-0.131275</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.314638</td>\n",
       "      <td>-0.872959</td>\n",
       "      <td>0.083391</td>\n",
       "      <td>0.148178</td>\n",
       "      <td>-0.431459</td>\n",
       "      <td>0.119690</td>\n",
       "      <td>0.206395</td>\n",
       "      <td>0.070288</td>\n",
       "      <td>11.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150562</th>\n",
       "      <td>93670.0</td>\n",
       "      <td>-0.535045</td>\n",
       "      <td>1.014587</td>\n",
       "      <td>1.750679</td>\n",
       "      <td>2.769390</td>\n",
       "      <td>0.500089</td>\n",
       "      <td>1.002270</td>\n",
       "      <td>0.847902</td>\n",
       "      <td>-0.081323</td>\n",
       "      <td>0.371579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063525</td>\n",
       "      <td>0.443431</td>\n",
       "      <td>-0.072754</td>\n",
       "      <td>0.448192</td>\n",
       "      <td>-0.655203</td>\n",
       "      <td>-0.181038</td>\n",
       "      <td>-0.093013</td>\n",
       "      <td>-0.064931</td>\n",
       "      <td>117.44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138452</th>\n",
       "      <td>82655.0</td>\n",
       "      <td>-4.026938</td>\n",
       "      <td>1.897371</td>\n",
       "      <td>-0.429786</td>\n",
       "      <td>-0.029571</td>\n",
       "      <td>-0.855751</td>\n",
       "      <td>-0.480406</td>\n",
       "      <td>-0.435632</td>\n",
       "      <td>1.313760</td>\n",
       "      <td>0.536044</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.480691</td>\n",
       "      <td>-0.230369</td>\n",
       "      <td>0.250717</td>\n",
       "      <td>0.066399</td>\n",
       "      <td>0.470787</td>\n",
       "      <td>0.245335</td>\n",
       "      <td>0.286904</td>\n",
       "      <td>-0.322672</td>\n",
       "      <td>25.76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "223361  143352.0  1.955041 -0.380783 -0.315013  0.330155 -0.509374 -0.086197   \n",
       "165061  117173.0 -0.400975 -0.626943  1.555339 -2.017772 -0.107769  0.168310   \n",
       "238186  149565.0  0.072509  0.820566 -0.561351 -0.709897  1.080399 -0.359429   \n",
       "150562   93670.0 -0.535045  1.014587  1.750679  2.769390  0.500089  1.002270   \n",
       "138452   82655.0 -4.026938  1.897371 -0.429786 -0.029571 -0.855751 -0.480406   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23  \\\n",
       "223361 -0.627978  0.035994  1.054560  ...  0.238197  0.968305  0.053208   \n",
       "165061  0.017959 -0.401619  0.040378  ... -0.153485  0.421703  0.113442   \n",
       "238186  0.787858  0.117276 -0.131275  ... -0.314638 -0.872959  0.083391   \n",
       "150562  0.847902 -0.081323  0.371579  ...  0.063525  0.443431 -0.072754   \n",
       "138452 -0.435632  1.313760  0.536044  ... -0.480691 -0.230369  0.250717   \n",
       "\n",
       "             V24       V25       V26       V27       V28  Amount  Class  \n",
       "223361 -0.278602 -0.044999 -0.216780  0.045168 -0.047145    9.99      0  \n",
       "165061 -1.004095 -1.176695  0.361924 -0.370469 -0.144792   45.90      0  \n",
       "238186  0.148178 -0.431459  0.119690  0.206395  0.070288   11.99      0  \n",
       "150562  0.448192 -0.655203 -0.181038 -0.093013 -0.064931  117.44      0  \n",
       "138452  0.066399  0.470787  0.245335  0.286904 -0.322672   25.76      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.549887</td>\n",
       "      <td>-0.069882</td>\n",
       "      <td>-0.053532</td>\n",
       "      <td>-0.112922</td>\n",
       "      <td>-0.072707</td>\n",
       "      <td>-0.078552</td>\n",
       "      <td>-0.105243</td>\n",
       "      <td>0.058112</td>\n",
       "      <td>-0.110790</td>\n",
       "      <td>-0.059250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120357</td>\n",
       "      <td>0.023052</td>\n",
       "      <td>-0.110802</td>\n",
       "      <td>0.012167</td>\n",
       "      <td>0.028561</td>\n",
       "      <td>0.094315</td>\n",
       "      <td>-0.027522</td>\n",
       "      <td>0.088507</td>\n",
       "      <td>0.245263</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.336103</td>\n",
       "      <td>-0.088221</td>\n",
       "      <td>0.208878</td>\n",
       "      <td>0.068210</td>\n",
       "      <td>-0.156123</td>\n",
       "      <td>0.361882</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>-0.009272</td>\n",
       "      <td>-0.010666</td>\n",
       "      <td>0.081618</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117753</td>\n",
       "      <td>-0.192170</td>\n",
       "      <td>0.053687</td>\n",
       "      <td>0.028958</td>\n",
       "      <td>0.028291</td>\n",
       "      <td>-0.041588</td>\n",
       "      <td>-0.017712</td>\n",
       "      <td>0.253542</td>\n",
       "      <td>0.186685</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.373212</td>\n",
       "      <td>-0.141593</td>\n",
       "      <td>0.019915</td>\n",
       "      <td>-0.028703</td>\n",
       "      <td>-0.111270</td>\n",
       "      <td>0.080322</td>\n",
       "      <td>0.078172</td>\n",
       "      <td>0.091972</td>\n",
       "      <td>0.049368</td>\n",
       "      <td>0.046966</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.169584</td>\n",
       "      <td>0.022320</td>\n",
       "      <td>-0.087265</td>\n",
       "      <td>-0.014356</td>\n",
       "      <td>0.126500</td>\n",
       "      <td>0.104683</td>\n",
       "      <td>0.026121</td>\n",
       "      <td>-0.004408</td>\n",
       "      <td>0.140403</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.511248</td>\n",
       "      <td>-0.206231</td>\n",
       "      <td>0.030732</td>\n",
       "      <td>-0.092887</td>\n",
       "      <td>-0.095452</td>\n",
       "      <td>0.058851</td>\n",
       "      <td>0.099392</td>\n",
       "      <td>-0.084789</td>\n",
       "      <td>-0.011920</td>\n",
       "      <td>0.318085</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062850</td>\n",
       "      <td>-0.008544</td>\n",
       "      <td>-0.051438</td>\n",
       "      <td>-0.140793</td>\n",
       "      <td>0.127849</td>\n",
       "      <td>-0.044202</td>\n",
       "      <td>0.087641</td>\n",
       "      <td>0.070798</td>\n",
       "      <td>0.229650</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.445341</td>\n",
       "      <td>0.090884</td>\n",
       "      <td>0.020296</td>\n",
       "      <td>-0.018501</td>\n",
       "      <td>-0.045372</td>\n",
       "      <td>-0.015069</td>\n",
       "      <td>0.007962</td>\n",
       "      <td>-0.070261</td>\n",
       "      <td>0.071984</td>\n",
       "      <td>-0.038460</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050441</td>\n",
       "      <td>-0.034665</td>\n",
       "      <td>-0.005326</td>\n",
       "      <td>-0.056571</td>\n",
       "      <td>0.053817</td>\n",
       "      <td>-0.015069</td>\n",
       "      <td>0.005978</td>\n",
       "      <td>0.046644</td>\n",
       "      <td>0.055707</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227446</th>\n",
       "      <td>0.279291</td>\n",
       "      <td>-0.044120</td>\n",
       "      <td>0.070437</td>\n",
       "      <td>0.082388</td>\n",
       "      <td>0.166315</td>\n",
       "      <td>0.073097</td>\n",
       "      <td>-0.021063</td>\n",
       "      <td>-0.142272</td>\n",
       "      <td>-0.059467</td>\n",
       "      <td>0.129837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185740</td>\n",
       "      <td>-0.008043</td>\n",
       "      <td>0.152775</td>\n",
       "      <td>0.045698</td>\n",
       "      <td>-0.174620</td>\n",
       "      <td>-0.171711</td>\n",
       "      <td>-0.029142</td>\n",
       "      <td>0.015654</td>\n",
       "      <td>0.241650</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227447</th>\n",
       "      <td>0.503334</td>\n",
       "      <td>-0.096129</td>\n",
       "      <td>0.060529</td>\n",
       "      <td>-0.126463</td>\n",
       "      <td>-0.172743</td>\n",
       "      <td>0.122610</td>\n",
       "      <td>0.037656</td>\n",
       "      <td>0.107913</td>\n",
       "      <td>-0.038546</td>\n",
       "      <td>0.080477</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208206</td>\n",
       "      <td>-0.014641</td>\n",
       "      <td>-0.161423</td>\n",
       "      <td>-0.201274</td>\n",
       "      <td>0.243915</td>\n",
       "      <td>0.170093</td>\n",
       "      <td>0.010409</td>\n",
       "      <td>-0.026757</td>\n",
       "      <td>0.204647</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227448</th>\n",
       "      <td>0.489984</td>\n",
       "      <td>-0.126907</td>\n",
       "      <td>-0.096363</td>\n",
       "      <td>-0.095462</td>\n",
       "      <td>-0.094490</td>\n",
       "      <td>0.072779</td>\n",
       "      <td>-0.123324</td>\n",
       "      <td>0.109525</td>\n",
       "      <td>-0.123035</td>\n",
       "      <td>0.117451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083739</td>\n",
       "      <td>-0.078492</td>\n",
       "      <td>-0.110003</td>\n",
       "      <td>-0.020489</td>\n",
       "      <td>0.107794</td>\n",
       "      <td>0.089668</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.137236</td>\n",
       "      <td>0.185058</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227449</th>\n",
       "      <td>0.426574</td>\n",
       "      <td>-0.303419</td>\n",
       "      <td>0.068501</td>\n",
       "      <td>-0.119662</td>\n",
       "      <td>-0.157397</td>\n",
       "      <td>0.128763</td>\n",
       "      <td>0.160709</td>\n",
       "      <td>0.103662</td>\n",
       "      <td>-0.034709</td>\n",
       "      <td>0.314764</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.169760</td>\n",
       "      <td>0.048567</td>\n",
       "      <td>-0.132345</td>\n",
       "      <td>-0.091997</td>\n",
       "      <td>0.165992</td>\n",
       "      <td>0.129724</td>\n",
       "      <td>0.084031</td>\n",
       "      <td>-0.067558</td>\n",
       "      <td>0.227514</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227450</th>\n",
       "      <td>0.332085</td>\n",
       "      <td>-0.116702</td>\n",
       "      <td>0.085717</td>\n",
       "      <td>0.021954</td>\n",
       "      <td>-0.104582</td>\n",
       "      <td>0.158680</td>\n",
       "      <td>0.090083</td>\n",
       "      <td>0.119250</td>\n",
       "      <td>0.004034</td>\n",
       "      <td>0.027020</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.199652</td>\n",
       "      <td>-0.062520</td>\n",
       "      <td>-0.054085</td>\n",
       "      <td>-0.007825</td>\n",
       "      <td>0.102198</td>\n",
       "      <td>0.102833</td>\n",
       "      <td>0.038211</td>\n",
       "      <td>0.013235</td>\n",
       "      <td>0.134367</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>227451 rows Ã 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0       0.549887 -0.069882 -0.053532 -0.112922 -0.072707 -0.078552 -0.105243   \n",
       "1       0.336103 -0.088221  0.208878  0.068210 -0.156123  0.361882 -0.000093   \n",
       "2       0.373212 -0.141593  0.019915 -0.028703 -0.111270  0.080322  0.078172   \n",
       "3       0.511248 -0.206231  0.030732 -0.092887 -0.095452  0.058851  0.099392   \n",
       "4       0.445341  0.090884  0.020296 -0.018501 -0.045372 -0.015069  0.007962   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "227446  0.279291 -0.044120  0.070437  0.082388  0.166315  0.073097 -0.021063   \n",
       "227447  0.503334 -0.096129  0.060529 -0.126463 -0.172743  0.122610  0.037656   \n",
       "227448  0.489984 -0.126907 -0.096363 -0.095462 -0.094490  0.072779 -0.123324   \n",
       "227449  0.426574 -0.303419  0.068501 -0.119662 -0.157397  0.128763  0.160709   \n",
       "227450  0.332085 -0.116702  0.085717  0.021954 -0.104582  0.158680  0.090083   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23  \\\n",
       "0       0.058112 -0.110790 -0.059250  ...  0.120357  0.023052 -0.110802   \n",
       "1      -0.009272 -0.010666  0.081618  ... -0.117753 -0.192170  0.053687   \n",
       "2       0.091972  0.049368  0.046966  ... -0.169584  0.022320 -0.087265   \n",
       "3      -0.084789 -0.011920  0.318085  ... -0.062850 -0.008544 -0.051438   \n",
       "4      -0.070261  0.071984 -0.038460  ... -0.050441 -0.034665 -0.005326   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "227446 -0.142272 -0.059467  0.129837  ...  0.185740 -0.008043  0.152775   \n",
       "227447  0.107913 -0.038546  0.080477  ... -0.208206 -0.014641 -0.161423   \n",
       "227448  0.109525 -0.123035  0.117451  ...  0.083739 -0.078492 -0.110003   \n",
       "227449  0.103662 -0.034709  0.314764  ... -0.169760  0.048567 -0.132345   \n",
       "227450  0.119250  0.004034  0.027020  ... -0.199652 -0.062520 -0.054085   \n",
       "\n",
       "             V24       V25       V26       V27       V28    Amount  Class  \n",
       "0       0.012167  0.028561  0.094315 -0.027522  0.088507  0.245263    1.0  \n",
       "1       0.028958  0.028291 -0.041588 -0.017712  0.253542  0.186685    1.0  \n",
       "2      -0.014356  0.126500  0.104683  0.026121 -0.004408  0.140403    1.0  \n",
       "3      -0.140793  0.127849 -0.044202  0.087641  0.070798  0.229650    1.0  \n",
       "4      -0.056571  0.053817 -0.015069  0.005978  0.046644  0.055707    1.0  \n",
       "...          ...       ...       ...       ...       ...       ...    ...  \n",
       "227446  0.045698 -0.174620 -0.171711 -0.029142  0.015654  0.241650    1.0  \n",
       "227447 -0.201274  0.243915  0.170093  0.010409 -0.026757  0.204647    1.0  \n",
       "227448 -0.020489  0.107794  0.089668  0.029800  0.137236  0.185058    1.0  \n",
       "227449 -0.091997  0.165992  0.129724  0.084031 -0.067558  0.227514    1.0  \n",
       "227450 -0.007825  0.102198  0.102833  0.038211  0.013235  0.134367    1.0  \n",
       "\n",
       "[227451 rows x 31 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "1.0    227845\n",
       "0.0    227451\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, nr_features):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(nr_features, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.flatten(self.model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudModel:\n",
    "    def __init__(self, train_df):\n",
    "        self.lr = 0.001\n",
    "        self.batch_size = 64\n",
    "        self.dataset = FraudDataset(train_df, fraud = False)\n",
    "        self.dataloader = DataLoader(self.dataset, self.batch_size, shuffle=True)\n",
    "        self.model = Model(nr_features=self.dataset.features).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.loss = nn.BCELoss()\n",
    "\n",
    "    def train(self, epochs=100):\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for _, (x, y) in enumerate(self.dataloader):\n",
    "                self.optimizer.zero_grad()\n",
    "                y_pred = self.model(x)\n",
    "                loss = self.loss(y_pred, y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item() / len(x)\n",
    "            print(\"[Epoch %d/%d] loss: %f\" % (epoch, epochs, np.mean(epoch_loss)))\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            pred = self.model(x)\n",
    "            return pred.cpu().numpy()\n",
    "        \n",
    "    def evaluate(self, test_df, confidence=0.50):\n",
    "        input = torch.Tensor(test_df.drop(['Class'], axis=1).values).to(device)\n",
    "        preds = self.predict(input)\n",
    "        labels = np.zeros_like(preds)\n",
    "        labels[preds >= confidence] = 1\n",
    "        ConfusionMatrixDisplay.from_predictions(test_df['Class'].values, labels, normalize='true')\n",
    "        print(classification_report(test_df['Class'].values, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw_model = FraudModel(df_concat)\n",
    "fw_model.train(epochs=100)\n",
    "fw_model.evaluate(df_test, confidence=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
